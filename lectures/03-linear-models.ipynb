{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93e71d01",
   "metadata": {},
   "source": [
    "# CSE 25 – Introduction to Artificial Intelligence\n",
    "## Worksheet 7: From Fitting to Predicting\n",
    "\n",
    "**Context (from last class):**  \n",
    "Last time, we fit a line to data by choosing parameters that reduced error.\n",
    "Our focus was on *fitting* the model to the data we were given.\n",
    "\n",
    "In this worksheet, we take the next step: using a *trained model* to make predictions and evaluate how well it *generalizes*.\n",
    "\n",
    "We will then extend these ideas to a new kind of task: **classification**.\n",
    "\n",
    "\n",
    "**Today’s guiding questions:**\n",
    "\n",
    "- What can we do *after* we fit a model?\n",
    "- How do we know if a model works well on new data?\n",
    "- How can the same linear structure be used to make decisions between classes?\n",
    "\n",
    "**Learning Objectives:**\n",
    "\n",
    "By the end of today’s class, you will be able to:\n",
    "\n",
    "- Use a fitted linear model to make predictions on new, unseen data  \n",
    "- Explain why we separate data into **training** and **testing** sets  \n",
    "- Distinguish between **regression** and **classification** problems  \n",
    "- Explain how a perceptron uses a weighted sum and bias to make decisions  \n",
    "- Describe how the perceptron updates its parameters when it makes a mistake  \n",
    "\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "Create a copy of this notebook and complete it during class.  \n",
    "Work through the cells below **in order**.\n",
    "\n",
    "You may discuss with your neighbors, but make sure you understand\n",
    "what each step is doing and why.\n",
    "\n",
    "\n",
    "**Submission**\n",
    "\n",
    "When finished, download the notebook as a PDF and upload it to Gradescope under  \n",
    "`In-Class – Week 4 Thursday`.\n",
    "\n",
    "To download as a PDF on DataHub:  \n",
    "`File -> Save and Export Notebook As -> PDF`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a43bff",
   "metadata": {},
   "source": [
    "### From last time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451f445e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions_v2(input_values, w, b):\n",
    "    '''\n",
    "    input_values: list of input values\n",
    "    w: weight (slope)\n",
    "    b: bias (intercept)\n",
    "\n",
    "    Complete the function that calculates the predicted output values using the weight and bias. \n",
    "    Return a list of predicted values.\n",
    "    '''\n",
    "    # Initialize an empty list to store predicted values\n",
    "    predicted_values = []\n",
    "\n",
    "    # Calculate predicted values using the formula: predicted_value = input_value * w + b\n",
    "    # Append each predicted value to the predicted_values list\n",
    "    for input_value in input_values:\n",
    "        predicted_values.append(input_value * w + b)\n",
    "    # Return the list of predicted values\n",
    "    return predicted_values\n",
    "\n",
    "\n",
    "def get_mean_squared_error(actual_values, predicted_values):\n",
    "    '''\n",
    "    actual_values: list of actual output values\n",
    "    predicted_values: list of predicted output values\n",
    "\n",
    "    '''\n",
    "    squared_error_list = []\n",
    "   \n",
    "    # Get pointwise squared errors\n",
    "    for idx, actual_value in enumerate(actual_values):\n",
    "        squared_error = (actual_value - predicted_values[idx])**2\n",
    "        squared_error_list.append((squared_error))\n",
    "    \n",
    "    # Calculate total error\n",
    "    total_error = sum(squared_error_list)\n",
    "    \n",
    "    # Calculate MSE by dividing total error by number of data points\n",
    "    mse = total_error / len(actual_values)\n",
    "    \n",
    "    # Return the MSE\n",
    "    return mse\n",
    "\n",
    "def get_gradients_w_b(error_function, input_values, actual_values, w, b, epsilon=1e-6):\n",
    "    \"\"\"\n",
    "    Approximates the partial derivatives of the error with respect to w and b using finite differences.\n",
    "    Returns: (dE/dw, dE/db)\n",
    "    \"\"\"\n",
    "    # Get predictions at (w, b)\n",
    "    predicted = get_predictions_v2(input_values, w, b)\n",
    "    # Get error at (w, b)\n",
    "    error = error_function(actual_values, predicted)\n",
    "\n",
    "    # Get predictions at (w + epsilon, b)\n",
    "    predicted_w = get_predictions_v2(input_values, w + epsilon, b)\n",
    "    # Get error at (w + epsilon, b)\n",
    "    error_w = error_function(actual_values, predicted_w)\n",
    "    # Partial derivative w.r.t. w\n",
    "    grad_w = (error_w - error) / epsilon\n",
    "    \n",
    "    ##  Partial derivative w.r.t. b\n",
    "    # Get predictions at (w, b + epsilon)\n",
    "    predicted_b = get_predictions_v2(input_values, w, b + epsilon)\n",
    "    # Get error at (w, b + epsilon)\n",
    "    error_b = error_function(actual_values, predicted_b)\n",
    "    # Partial derivative w.r.t. b\n",
    "    grad_b = (error_b - error) / epsilon\n",
    "    \n",
    "    return grad_w, grad_b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e1426b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data for Fahrenheit and Celsius\n",
    "fahrenheit = [32, 50, 68, 86, 104] # x-axis values (INPUT DATA)\n",
    "celsius = [0, 10, 20, 30, 40] # y-axis values (OUTPUT DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c55f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Descent for Fahrenheit to Celsius conversion using both w and b\n",
    "\n",
    "# Initialize parameters\n",
    "w_fc = 1\n",
    "b_fc = -60\n",
    "\n",
    "# Set the learning rate\n",
    "learning_rate_fc = 0.0001\n",
    "\n",
    "# Set the number of gradient descent steps\n",
    "num_steps_fc = 250000\n",
    "\n",
    "grad_weights_fc = []\n",
    "grad_biases_fc = []\n",
    "grad_errors_fc = []\n",
    "\n",
    "for step in range(num_steps_fc):\n",
    "    predicted_fc = get_predictions_v2(fahrenheit, w_fc, b_fc)\n",
    "    error_fc = get_mean_squared_error(celsius, predicted_fc)\n",
    "    grad_weights_fc.append(w_fc)\n",
    "    grad_biases_fc.append(b_fc)\n",
    "    grad_errors_fc.append(error_fc)\n",
    "    grad_w_fc, grad_b_fc = get_gradients_w_b(get_mean_squared_error, fahrenheit, celsius, w_fc, b_fc)\n",
    "    w_fc = w_fc - learning_rate_fc * grad_w_fc\n",
    "    b_fc = b_fc - learning_rate_fc * grad_b_fc\n",
    "    if step % 25000 == 0:\n",
    "        print(f\"Step {step}: w = {w_fc:.4f}, b = {b_fc:.4f}, error = {error_fc:.4f}\")\n",
    "\n",
    "print(f\"Best weight (w) found: {w_fc:.4f}\")\n",
    "print(f\"Best bias (b) found: {b_fc:.4f}\")\n",
    "print(f\"Mean squared error at best parameters: {grad_errors_fc[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd59cf2",
   "metadata": {},
   "source": [
    "Okay, now we have the updated parameters - What do we do with it? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d32f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data points and the fitted line\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Original data\n",
    "plt.scatter(fahrenheit, celsius, label='Data points')\n",
    "\n",
    "x_line = [min(fahrenheit), max(fahrenheit)]\n",
    "y_line = [w_fc * x + b_fc for x in x_line]\n",
    "plt.plot(x_line, y_line, color='red', linestyle='--',  label='Fitted line')\n",
    "\n",
    "plt.xlabel('Fahrenheit')\n",
    "plt.ylabel('Celsius')\n",
    "plt.title('Fahrenheit to Celsius Fit')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae42c72",
   "metadata": {},
   "source": [
    "### What do we do with the model?\n",
    "\n",
    "Now that we’ve fit a line, we can use it to make predictions for new, unseen data.\n",
    "\n",
    "For example, if you have a temperature in Fahrenheit, you can use the model to estimate the temperature in Celsius."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877ad0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict Celsius for new Fahrenheit values using the learned model\n",
    "new_fahrenheit = [40, 77, 100]\n",
    "# Get predictions for the new Fahrenheit values\n",
    "predicted_celsius = get_predictions_v2(new_fahrenheit, w_fc, b_fc)\n",
    "\n",
    "for f, c in zip(new_fahrenheit, predicted_celsius):\n",
    "    print(f\"Fahrenheit: {f} -> Predicted Celsius: {c:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9161c6",
   "metadata": {},
   "source": [
    "### Train/Test Split\n",
    "\n",
    "But, how do we know if our model is correct?\n",
    "\n",
    "It's not enough to just fit a line and make predictions - we want to know if our model is actually making good predictions on unseen data.\n",
    "\n",
    "In practice, we split our data into two parts:\n",
    "\n",
    "- **Training data:** Used to fit (train) the model.\n",
    "- **Testing data:** Used to evaluate how well the model predicts on new, unseen data.\n",
    "\n",
    "This way, we can check if our model *generalizes* well, not just *memorizes* the training points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c276ab2-b8bf-455b-90c9-090135d28edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a bigger dataset for demonstration\n",
    "\n",
    "fahrenheit_all = [32, 40, 50, 59, 68, 77, 86, 95, 104, 113] # Input Values\n",
    "celsius_all = [0, 4, 10, 15, 20, 25, 30, 35, 40, 45] # Target/Output Values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606e69c9",
   "metadata": {},
   "source": [
    "To evaluate how well our model generalizes to new data, we use a **train/test split**:\n",
    "\n",
    "1. **Randomize the data:**\n",
    "    We shuffle the dataset so that the order does not affect which points end up in the training or testing sets. This helps ensure that both sets are representative of the overall data.\n",
    "\n",
    "2. **Split into training and testing sets:**  \n",
    "    After shuffling, we divide the data into two parts:\n",
    "    - The **training set** is used to fit (train) the model.  \n",
    "    - The **testing set** is used to evaluate the model's performance on unseen data.\n",
    "\n",
    "3. **Apply the split to both inputs and outputs:**  \n",
    "    We split both the input values (e.g., `fahrenheit_all`) and the corresponding output values (e.g., `celsius_all`) in the same way, so each input still matches its correct output.\n",
    "\n",
    "This process helps us measure how well our model is likely to perform on new, real-world data, not just the data it was trained on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62477d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print lengths and ranges for verification\n",
    "print(len(fahrenheit_all))\n",
    "print(len(celsius_all))\n",
    "\n",
    "# Print the index ranges\n",
    "print(list(range(len(fahrenheit_all))))\n",
    "print(list(range(len(celsius_all))))\n",
    "\n",
    "indices = list(range(len(fahrenheit_all)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d7a40e-4305-4fe8-a0db-08344ef372cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Shuffle the data indices - we will use random.shuffle() for this.\n",
    "\n",
    "indices = list(range(len(fahrenheit_all)))\n",
    "random.seed(42)  # we use the seed for reproducibility\n",
    "random.shuffle(indices)\n",
    "\n",
    "\n",
    "# Shuffle the data according to the shuffled indices\n",
    "fahrenheit_shuffled = [fahrenheit_all[i] for i in indices]\n",
    "celsius_shuffled = [celsius_all[i] for i in indices]\n",
    "\n",
    "\n",
    "print('Original Data:')\n",
    "print(fahrenheit_all)\n",
    "print(celsius_all)\n",
    "print('-'*10)\n",
    "print('Shuffled Data:')\n",
    "print(fahrenheit_shuffled)\n",
    "print(celsius_shuffled)\n",
    "print('-'*10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8437f01c",
   "metadata": {},
   "source": [
    "Q. Why did we shuffle the indices first, and not the lists directly?\n",
    "\n",
    "`YOUR ANSWER HERE`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d09f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually split into training and testing sets\n",
    "# Here, we will use 70% of the data for training and 30% for testing.\n",
    "\n",
    "train_size = int(0.7 * len(fahrenheit_all))\n",
    "\n",
    "# We can use python list slicing to split the data\n",
    "fahrenheit_train = fahrenheit_shuffled[:train_size]     # first train_size points for training\n",
    "celsius_train = celsius_shuffled[:train_size]           # first train_size points for training\n",
    "fahrenheit_test = fahrenheit_shuffled[train_size:]      # remaining points for testing\n",
    "celsius_test = celsius_shuffled[train_size:]            # remaining points for testing\n",
    "\n",
    "\n",
    "print(\"Training data (Fahrenheit):\", fahrenheit_train)\n",
    "print(\"Training data (Celsius):\", celsius_train)\n",
    "print('-'*10)\n",
    "\n",
    "print(\"Testing data (Fahrenheit):\", fahrenheit_test)\n",
    "print(\"Testing data (Celsius):\", celsius_test)\n",
    "print('-'*10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e2580b",
   "metadata": {},
   "source": [
    "#### Train the model on the training set\n",
    "\n",
    "Now, that we have our train/test split, we can **train** a model on our `training set` using the gradient descent from before and **evaluate** our model using the `test set`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e7c7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Descent for Fahrenheit to Celsius conversion using both w and b\n",
    "\n",
    "# Initialize parameters\n",
    "w_fc = 1\n",
    "b_fc = -60\n",
    "\n",
    "# Set the learning rate\n",
    "learning_rate_fc = 0.0001\n",
    "\n",
    "# Set the number of gradient descent steps\n",
    "num_steps_fc = 250000\n",
    "\n",
    "for step in range(num_steps_fc):\n",
    "    predicted_fc = get_predictions_v2(fahrenheit_train, w_fc, b_fc)\n",
    "    error_fc = get_mean_squared_error(celsius_train, predicted_fc)\n",
    "    grad_w_fc, grad_b_fc = get_gradients_w_b(get_mean_squared_error, fahrenheit_train, celsius_train, w_fc, b_fc)\n",
    "    \n",
    "    w_fc = w_fc - learning_rate_fc * grad_w_fc\n",
    "    b_fc = b_fc - learning_rate_fc * grad_b_fc\n",
    "    \n",
    "    if step % 25000 == 0:\n",
    "        print(f\"Step {step}: w = {w_fc:.4f}, b = {b_fc:.4f}, error = {error_fc:.4f}\")\n",
    "\n",
    "print(f\"Best weight (w) found: {w_fc:.4f}\")\n",
    "print(f\"Best bias (b) found: {b_fc:.4f}\")\n",
    "print(f\"Mean squared error at best parameters for the Train Set: {error_fc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09071168",
   "metadata": {},
   "source": [
    "#### Evaluate the model on the test set.\n",
    "\n",
    "Next, we can *evaluate* on the `test set` using our learned parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9764c255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions on the test set: fahrenheit_test\n",
    "predicted_test = get_predictions_v2(fahrenheit_test, w_fc, b_fc)\n",
    "\n",
    "# Print the predicted values upto 2 decimal places\n",
    "print(\"Predicted Values:\", [round(val, 2) for val in predicted_test])\n",
    "\n",
    "# Print the actual values\n",
    "print(\"Actual Values:\", celsius_test)\n",
    "\n",
    "# Using the predicted values, calculate the mean squared error on the test set\n",
    "test_error = get_mean_squared_error(celsius_test, predicted_test)\n",
    "\n",
    "# Print the test error\n",
    "print(f\"Mean Squared Error on Test Set: {test_error:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096f2acf",
   "metadata": {},
   "source": [
    "Why would the Test Set error more Train Set error?\n",
    "\n",
    "`YOUR ANSWER HERE`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e36330d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Regression vs. Classification\n",
    "\n",
    "So far, we used our model to **predict a (continuous) number** (Celsius temperature). This type of prediction is called **regression**.\n",
    "\n",
    "But what if we wanted to predict a **category** instead of a number? That is called **classification**.\n",
    "\n",
    "**Regression:** Predicting a `continuous value` (for example: someone's weight, the length of a fish, or the amount of rainfall tomorrow).\n",
    "\n",
    "**Classification:** Predicting a discrete `label` or `class` (for example: whether a review is positive or negative, the species of a plant, or the type of fruit in a photo)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fd0cc3",
   "metadata": {},
   "source": [
    "##### Examples: Is this Regression or Classification?\n",
    "\n",
    "Answer the following questions and then dicsuss your answers:\n",
    "\n",
    "1. Predicting the price of a house based on its features.\n",
    "\n",
    "`YOUR ANSWER HERE`\n",
    "\n",
    "2. Predicting whether an email is spam or not.\n",
    "\n",
    "`YOUR ANSWER HERE`\n",
    "\n",
    "3. Predicting if a tumor is benign or malignant.\n",
    "\n",
    "`YOUR ANSWER HERE`\n",
    "\n",
    "4. Predicting the number of hours a student will study next week.\n",
    "\n",
    "`YOUR ANSWER HERE`\n",
    "\n",
    "5. Predicting the temperature in San Diego tomorrow.\n",
    "\n",
    "`YOUR ANSWER HERE`\n",
    "\n",
    "6. Predicting which digit (0-9) is shown in a handwritten image.\n",
    "\n",
    "`YOUR ANSWER HERE`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb7830a",
   "metadata": {},
   "source": [
    "Why is the the last option ___________ given that we are predicting a number?\n",
    "\n",
    "`YOUR ANSWER HERE`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850bb4ae",
   "metadata": {},
   "source": [
    "<!-- Even though the last example (\"Predicting which digit (0-9) is shown in a handwritten image\") involves predicting a number, it is still a classification problem. This is because you are choosing one `label` from a fixed set of `classes` (the digits 0 through 9), not predicting a continuous value. The output is a class label, not a real-valued number. \n",
    "\n",
    "In classification, the \"number\" is just a category name (like 0, 1, ..., 9), not a quantity that can take on any value. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3116d57",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Linear Regression: From a Line to Multiple Variables\n",
    "\n",
    "Linear regression is a method for modeling the relationship between input variables (features) and an output variable (target) by fitting a straight line.\n",
    "\n",
    "#### The Equation of a Line (One Variable)\n",
    "\n",
    "For a single input variable $x$, the equation of a line is:\n",
    "\n",
    "$$\n",
    "y = w \\cdot x + b\n",
    "$$\n",
    "\n",
    "- $y$ = predicted value (output)\n",
    "- $x$ = input value (feature)\n",
    "- $w$ = weight (slope of the line)\n",
    "- $b$ = bias (intercept)\n",
    "\n",
    "This is the basic form of **simple linear regression**. We have already seen this!\n",
    "\n",
    "#### Generalizing to Multiple Input Variables\n",
    "\n",
    "When there are multiple input variables (features), we use **multiple linear regression**. The equation becomes:\n",
    "\n",
    "$$\n",
    "y = w_1 x_1 + w_2 x_2 + \\ldots + w_n x_n + b\n",
    "$$\n",
    "\n",
    "So, the prediction is a **weighted sum** of all the input features plus a bias:\n",
    "\n",
    "$$\n",
    "y = \\sum_{i=1}^n w_i x_i + b\n",
    "$$\n",
    "\n",
    "The model learns the best weights ($w_1, w_2, ..., w_n$) and bias ($b$) to fit the data.\n",
    "\n",
    "**Take away:**  \n",
    "- With one $x$, linear regression fits a line.  \n",
    "- With many $x$'s, it fits a *hyperplane* in higher dimensions.  \n",
    "- The principle is the same: predict $y$ as a weighted sum of the inputs plus a bias."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d10192e",
   "metadata": {},
   "source": [
    "## Introducing the Perceptron\n",
    "\n",
    "Now that we've seen how to fit a line to predict numbers (regression), let's look at a simple model for classification: the **perceptron**.\n",
    "\n",
    "A perceptron is a type of *linear* classifier. It takes a set of input features (numbers), computes a *weighted sum*, and makes a decision about which class the input belongs to.\n",
    "\n",
    "- If the data is linearly separable, the perceptron can find a line (or hyperplane) that separates the classes.\n",
    "- The perceptron updates its weights based on mistakes it makes during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0981be78",
   "metadata": {},
   "source": [
    "### What does the perceptron model look like?\n",
    "\n",
    "The perceptron is actually very similar to the linear model we used for regression.\n",
    "\n",
    "- In regression, we used the equation:  \n",
    "  $$\n",
    "  y = \\sum_{i=1}^n w_i x_i + b\n",
    "  $$\n",
    "\n",
    "- In the perceptron (for classification), we use:  \n",
    "  $$\n",
    "  score = \\sum_{i=1}^n w_i x_i + b\n",
    "  $$\n",
    "\n",
    "    But instead of predicting a number, the perceptron uses the score to decide the class:\n",
    "\n",
    "    - If the score is greater than or equal to 0, predict one class (e.g., +1).\n",
    "    - If the score is less than 0, predict the other class (e.g., -1).\n",
    "\n",
    "  So, the perceptron is just a linear model with a decision rule on top! \n",
    "\n",
    "Let's look at a simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c428f80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Toy perceptron example\n",
    "# Each input is [x1, x2], label is +1 if x2 > x1, else -1\n",
    "\n",
    "X_toy = [\n",
    "    [1.5, 4],\n",
    "    [1, 2],   \n",
    "    [2, 1],   \n",
    "    [3, 5],\n",
    "    [4, 2],   \n",
    "    [0, 0],\n",
    "    [1.5, -0.5] \n",
    "]\n",
    "y_toy = [1, 1, -1, 1, -1, -1, -1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef91be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate points by class for coloring\n",
    "def plot_points(X_points, y_labels):\n",
    "    X_pos = [x for x, y in zip(X_points, y_labels) if y == 1]\n",
    "    X_neg = [x for x, y in zip(X_points, y_labels) if y == -1]\n",
    "\n",
    "    plt.scatter([x[0] for x in X_pos], [x[1] for x in X_pos], color='blue', marker='x', label='Class +1')\n",
    "    plt.scatter([x[0] for x in X_neg], [x[1] for x in X_neg], color='red', marker='o',  label='Class -1')\n",
    "\n",
    "    plt.xlabel('x1')\n",
    "    plt.ylabel('x2')\n",
    "    plt.title('Data')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "plot_points(X_toy, y_toy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af76695b",
   "metadata": {},
   "source": [
    "Complete the `perceptron_predict(x, w, b)` code below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d23b7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete the perceptron prediction function below\n",
    "def perceptron_predict(x, w, b):\n",
    "    '''\n",
    "    x: 2D input list [x1, x2]\n",
    "    w: 2D weight list [w1, w2]\n",
    "    b: bias term\n",
    "    '''\n",
    "    # You can assume x is a list of two elements [x1, x2]\n",
    "    # Score = w_1*x_1 + w_2*x_2 + b\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    # Return +1 if score >= 0 else -1\n",
    "    # YOUR CODE HERE\n",
    "    return None  # Replace this with actual return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d7b7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test cases for perceptron_predict with 2D input x\n",
    "\n",
    "# Case 1: score > 0\n",
    "assert perceptron_predict([2, 3], [1, 1], -4) == 1  # 2*1 + 3*1 - 4 = 1 >= 0\n",
    "\n",
    "# Case 2: score == 0\n",
    "assert perceptron_predict([1, 1], [2, -1], -1) == 1  # 1*2 + 1*(-1) - 1 = 0\n",
    "\n",
    "# Case 3: score < 0\n",
    "assert perceptron_predict([0, 1], [1, 2], -3) == -1  # 0*1 + 1*2 - 3 = -1 < 0\n",
    "\n",
    "# Case 4: negative weights\n",
    "assert perceptron_predict([2, 2], [-1, -1], 3) == -1  # 2*-1 + 2*-1 + 3 = -2 -2 + 3 = -1 < 0 -> -1\n",
    "\n",
    "# Case 5: bias only\n",
    "assert perceptron_predict([0, 0], [0, 0], 2) == 1    # 0 + 0 + 2 = 2 >= 0\n",
    "\n",
    "print(\"All perceptron_predict test cases passed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e35f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = [1, 1]\n",
    "b = 1\n",
    "\n",
    "for x, y_true in zip(X_toy, y_toy):\n",
    "    y_pred = perceptron_predict(x, w, b)\n",
    "    print(f\"Input: {x}, Actual label: {y_true}, Predicted: {y_pred}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee399f76",
   "metadata": {},
   "source": [
    "You don't need to understand the code in the next cell. Run it to see the plot of the current decision boundary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c6a2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Plot the toy data and the decision boundary using current w and b\n",
    "def plot_perceptron_decision_boundary(X_points, y_labels, w, b, current_points=None):\n",
    "    X_pos = [x for x, y in zip(X_points, y_labels) if y == 1]\n",
    "    X_neg = [x for x, y in zip(X_points, y_labels) if y == -1]\n",
    "    # Highlight current points if provided\n",
    "    if current_points is not None:\n",
    "        for pt in current_points:\n",
    "            plt.scatter(pt[0], pt[1], color='gold', edgecolor='black', s=120, marker='*', label='Current point')\n",
    "    plt.scatter([x[0] for x in X_pos], [x[1] for x in X_pos], color='blue', marker='x', label='Class +1')\n",
    "    plt.scatter([x[0] for x in X_neg], [x[1] for x in X_neg], color='red', marker='o', label='Class -1')\n",
    "\n",
    "    x1_vals = np.linspace(-2, 5, 100)\n",
    "    # Robust handling for zero weights\n",
    "    if w[0] == 0 and w[1] == 0:\n",
    "        plt.text(0.5, 0.5, \"No decision boundary\\n(w1=0, w2=0)\",\n",
    "                 fontsize=14, color='red', ha='center', va='center', transform=plt.gca().transAxes)\n",
    "    elif w[1] != 0:\n",
    "        x2_vals = [-(w[0]/w[1])*x1 - b/w[1] for x1 in x1_vals]\n",
    "        plt.plot(x1_vals, x2_vals, color='green', linestyle='--', label='Decision boundary')\n",
    "    elif w[0] != 0:\n",
    "        plt.axvline(x=-b/w[0], color='green', linestyle='--', label='Decision boundary')\n",
    "\n",
    "    plt.xlabel('x1')\n",
    "    plt.ylabel('x2')\n",
    "    plt.title('Perceptron Decision Boundary')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "plot_perceptron_decision_boundary(X_toy, y_toy, w, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bea9d1",
   "metadata": {},
   "source": [
    "You don't need to understand the code in the next cell. Run it and you should be able to control the decision boundary by changing the weights and bias values (similar to what we did for the line)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9831fc20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact, FloatSlider\n",
    "\n",
    "def interactive_perceptron_plot(w1=1.0, w2=1.0, b=1.0):\n",
    "    w = [w1, w2]\n",
    "    # Plot data points\n",
    "    X_pos = [x for x, y in zip(X_toy, y_toy) if y == 1]\n",
    "    X_neg = [x for x, y in zip(X_toy, y_toy) if y == -1]\n",
    "\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    plt.scatter([x[0] for x in X_pos], [x[1] for x in X_pos], color='blue', marker='x', label='Class +1')\n",
    "    plt.scatter([x[0] for x in X_neg], [x[1] for x in X_neg], color='red', marker='o', label='Class -1')\n",
    "\n",
    "    # Plot decision boundary: w1*x1 + w2*x2 + b = 0\n",
    "    x1_vals = np.linspace(-2, 5, 100)\n",
    "    if w1 == 0 and w2 == 0:\n",
    "        plt.text(0.5, 0.5, \"No decision boundary\\n(w1=0, w2=0)\", \n",
    "                 fontsize=14, color='red', ha='center', va='center', transform=plt.gca().transAxes)\n",
    "    elif w2 != 0:\n",
    "        x2_vals = [-(w1/w2)*x1 - b/w2 for x1 in x1_vals]\n",
    "        plt.plot(x1_vals, x2_vals, color='green', linestyle='--', label='Decision boundary')\n",
    "    elif w1 != 0:\n",
    "        plt.axvline(x=-b/w1, color='green', linestyle='--', label='Decision boundary')\n",
    "\n",
    "    # Show predictions for each point\n",
    "    for x, y_true in zip(X_toy, y_toy):\n",
    "        score = w[0] * x[0] + w[1] * x[1] + b\n",
    "        y_pred = 1 if score >= 0 else -1\n",
    "        plt.text(x[0]+0.1, x[1], f'Pred: {y_pred}', fontsize=9, color='black')\n",
    "\n",
    "    plt.xlabel('x1')\n",
    "    plt.ylabel('x2')\n",
    "    plt.title(f'Perceptron: w1={w1:.2f}, w2={w2:.2f}, b={b:.2f}')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "interact(\n",
    "    interactive_perceptron_plot,\n",
    "    w1=FloatSlider(value=1, min=-5, max=5, step=0.1, description='w1'),\n",
    "    w2=FloatSlider(value=1, min=-5, max=5, step=0.1, description='w2'),\n",
    "    b=FloatSlider(value=1, min=-10, max=10, step=0.1, description='b')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8e477c",
   "metadata": {},
   "source": [
    "### How does the perceptron learn?\n",
    "\n",
    "The perceptron learns by updating its weights and bias whenever it makes a mistake on a training example.\n",
    "\n",
    "- For each training example, it computes the score: \n",
    "    $$\n",
    "      score = \\sum_{i=1}^n w_i x_i + b\n",
    "    $$\n",
    "- It predicts the class based on the sign of the score.\n",
    "- If the prediction is wrong, it *updates* the weights and bias to reduce future mistakes:\n",
    "\n",
    "  **Update rule:**\n",
    "\n",
    "  - For each feature $i$:\n",
    "    - $w_i = w_i + y \\cdot x_i$\n",
    "  - Bias:\n",
    "    - $b = b + y$\n",
    "\n",
    "  Here, $y$ is the true label (+1 or -1), and $x_i$ is the value of feature $i$.\n",
    "\n",
    "This update nudges the model to be more likely to predict the correct class next time for similar examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99461a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perceptron learning: one iteration over the data\n",
    "\n",
    "w_learn = [1, 1]  # start with initial weights\n",
    "b_learn = 1       # start with initial bias\n",
    "\n",
    "for x, y_true in zip(X_toy, y_toy):\n",
    "    score = w_learn[0] * x[0] + w_learn[1] * x[1] + b_learn\n",
    "    y_pred = 1 if score >= 0 else -1\n",
    "    if y_pred != y_true:\n",
    "        # Update weights\n",
    "        for i in range(len(w_learn)):\n",
    "            w_learn[i] += y_true * x[i]\n",
    "        # Update bias\n",
    "        b_learn += y_true\n",
    "    print('Old Score:', score)\n",
    "    print('New Score:', w_learn[0] * x[0] + w_learn[1] * x[1] + b_learn)\n",
    "    print(f\"x={x}, y_true={y_true}, y_pred={y_pred}, w={w_learn}, b={b_learn}\")\n",
    "    plot_perceptron_decision_boundary(X_toy, y_toy, w_learn, b_learn, current_points=[x])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b139f7",
   "metadata": {},
   "source": [
    "Now, let's do the same thing, with the same data, just ordered differently. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10221218",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_toy_ordered = [\n",
    "[1.5, 4],\n",
    "[1, 2],\n",
    "[3, 5],\n",
    "[2, 1],\n",
    "[4, 2],\n",
    "[0, 0],\n",
    "[1.5, -0.5]\n",
    "]\n",
    "\n",
    "y_toy_ordered = [1, 1, 1, -1, -1, -1, -1]\n",
    "\n",
    "plot_points(X_toy_ordered, y_toy_ordered)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509db350",
   "metadata": {},
   "source": [
    "Now, let's run the same code from earlier, but this time use `X_toy_ordered` and `y_toy_ordered` instead of `X_toy` and `y_toy_ordered`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8f4caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perceptron learning: one epoch over the data\n",
    "w_learn = [1, 1]  # start with initial weights\n",
    "b_learn = 1       # start with initial bias\n",
    "\n",
    "for x, y_true in zip(X_toy_ordered, y_toy_ordered):\n",
    "    score = w_learn[0]*x[0] + w_learn[1]*x[1] + b_learn\n",
    "    y_pred = 1 if score >= 0 else -1\n",
    "    if y_pred != y_true:\n",
    "        # Update weights\n",
    "        for i in range(len(w_learn)):\n",
    "            w_learn[i] += y_true * x[i]\n",
    "        # Update bias\n",
    "        b_learn += y_true\n",
    "    \n",
    "    print('Old Score:', score)\n",
    "    print('New Score:', w_learn[0] * x[0] + w_learn[1] * x[1] + b_learn)\n",
    "    print(f\"x={x}, y_true={y_true}, y_pred={y_pred}, w={w_learn}, b={b_learn}\")\n",
    "    plot_perceptron_decision_boundary(X_toy_ordered, y_toy_ordered, w_learn, b_learn, current_points=[x])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7e37cc",
   "metadata": {},
   "source": [
    "### What happened?\n",
    "Discuss:\n",
    "- What happened? Why did the model not learn the correct decision boundary?\n",
    "\n",
    "`YOUR ANSWER HERE`\n",
    "\n",
    "- What can we do to make it learn the decision boundary?\n",
    "\n",
    "`YOUR ANSWER HERE`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5163052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's shuffle the data and do one epoch of perceptron learning\n",
    "\n",
    "import random\n",
    "\n",
    "random.seed(42) # for reproducibility\n",
    "\n",
    "# This is another way to shuffle data so that X and y stay aligned\n",
    "# You make a combination of X and y, shuffle that, then unzip back to X and y\n",
    "\n",
    "# For zip, refer: https://docs.python.org/3/library/functions.html#zip \n",
    "combined = list(zip(X_toy_ordered, y_toy_ordered)) \n",
    "\n",
    "# Shuffle the combined data\n",
    "random.shuffle(combined) # shuffle the combined data\n",
    "\n",
    "# One thing we can do is shuffle the data...\n",
    "# Unzip back to X and y with *combined\n",
    "X_shuffled, y_shuffled = zip(*combined)\n",
    "\n",
    "\n",
    "w_learn = [1, 1]  # reset weights\n",
    "b_learn = 1       # reset bias  \n",
    "\n",
    "for x, y_true in zip(X_shuffled, y_shuffled):\n",
    "    y_pred = perceptron_predict(x, w_learn, b_learn)\n",
    "    if y_pred != y_true:\n",
    "        # Update weights\n",
    "        for i in range(len(w_learn)):\n",
    "            w_learn[i] += y_true * x[i]\n",
    "        # Update bias\n",
    "        b_learn += y_true\n",
    "    print('Old Score:', score)\n",
    "    print('New Score:', w_learn[0] * x[0] + w_learn[1] * x[1] + b_learn)\n",
    "    print(f\"x={x}, y_true={y_true}, y_pred={y_pred}, w={w_learn}, b={b_learn}\")\n",
    "    plot_perceptron_decision_boundary(X_shuffled, y_shuffled, w_learn, b_learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98246779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also run multiple epochs and shuffle the data at each epoch\n",
    "# Number of epochs\n",
    "num_epochs = 2\n",
    "\n",
    "w_learn = [1, 1]  # reset weights\n",
    "b_learn = 1       # reset bias  \n",
    "random.seed(42) # for reproducibility\n",
    "\n",
    "plot_perceptron_decision_boundary(X_toy_ordered, y_toy_ordered, w_learn, b_learn)\n",
    "\n",
    "# We can run multiple epochs\n",
    "for epoch in range(num_epochs):\n",
    "    combined = list(zip(X_toy_ordered, y_toy_ordered))\n",
    "    random.shuffle(combined)\n",
    "    X_shuffled, y_shuffled = zip(*combined)\n",
    "\n",
    "    for x, y_true in zip(X_shuffled, y_shuffled):\n",
    "        score = w_learn[0]*x[0] + w_learn[1]*x[1] + b_learn\n",
    "        y_pred = 1 if score >= 0 else -1\n",
    "        if y_pred != y_true:\n",
    "            # Update weights\n",
    "            for i in range(len(w_learn)):\n",
    "                w_learn[i] += y_true * x[i]\n",
    "            # Update bias\n",
    "            b_learn += y_true\n",
    "        print(f'Epoch: {epoch+1}')\n",
    "        print(f\"x={x}, y_true={y_true}, y_pred={y_pred}, w={w_learn}, b={b_learn}\")\n",
    "        plot_perceptron_decision_boundary(X_shuffled, y_shuffled, w_learn, b_learn)\n",
    "    print(f\"Epoch:{epoch+1} w={w_learn}, b={b_learn}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02204cdf",
   "metadata": {},
   "source": [
    "##### Shuffling and Multiple Epochs in Perceptron Training\n",
    "\n",
    "In practice, training a perceptron involves more than just a single pass over the data:\n",
    "\n",
    "- **Shuffling the Data:**  \n",
    "    In each epoch (or iteration), we randomly shuffle the order of the training examples. This prevents the perceptron from getting stuck in patterns caused by the order of the data and helps it learn a better decision boundary.\n",
    "\n",
    "- **Multiple Epochs:**  \n",
    "    Instead of updating the weights just once for each example, we repeat the process for several epochs. In each epoch, the perceptron sees all the training examples (in a new random order), updating its weights whenever it makes a mistake.  \n",
    "    Running multiple epochs gives the perceptron more chances to correct its mistakes and converge to a solution that separates the classes (if possible).\n",
    "\n",
    "This approach improves learning and helps the perceptron find a good decision boundary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff33279",
   "metadata": {},
   "source": [
    "### What if we wanted to work with pictures?\n",
    "\n",
    "So far, we have been working with simple numbers, which made it easy to plug them directly into our models. But what if our data consists of images instead of numbers?\n",
    "\n",
    "To use images in machine learning models, we need to convert them into a numerical format. This usually means *representing* each image as a list (or array) of numbers—one for each pixel. For example, a grayscale image can be turned into a grid of pixel brightness values, where each value shows how dark or light that pixel is. Once we have this numerical representation, we can use the same modeling techniques as before.\n",
    "\n",
    "**What is a pixel?**\n",
    "\n",
    "A *pixel* (short for \"picture element\") is the smallest unit of a digital image. Each pixel represents a single point in the image and has a value that describes its color or brightness. In grayscale images, a pixel's value typically indicates how light or dark it is. In color images, each pixel usually has three values (red, green, and blue) that together define its color. By arranging many pixels in a grid, we can form complex images that computers can process and analyze.\n",
    "\n",
    "**Example:**\n",
    "- A grayscale image can be represented as a grid of numbers, where each number is the brightness of a pixel (e.g., 0 for black, 255 for white).\n",
    "- For the *digits dataset* (see below), each image is 8x8 pixels, so we have 64 numbers per image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e1b73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Grid for digit 0 (circle)\n",
    "zero_grid = [\n",
    "    [0,0,1,1,1,1,0,0],\n",
    "    [0,1,1,0,0,1,1,0],\n",
    "    [1,1,0,0,0,0,1,1],\n",
    "    [1,1,0,0,0,0,1,1],\n",
    "    [1,1,0,0,0,0,1,1],\n",
    "    [1,1,0,0,0,0,1,1],\n",
    "    [0,1,1,0,0,1,1,0],\n",
    "    [0,0,1,1,1,1,0,0]\n",
    "]\n",
    "\n",
    "plt.figure(figsize=(2, 2))\n",
    "plt.imshow(zero_grid, cmap='gray_r')\n",
    "plt.title(\"Digit 0\")\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# Grid for digit 1 (vertical bar)\n",
    "one_grid = [\n",
    "    [0,0,0,1,1,0,0,0],\n",
    "    [0,0,1,1,1,0,0,0],\n",
    "    [0,1,0,1,1,0,0,0],\n",
    "    [0,0,0,1,1,0,0,0],\n",
    "    [0,0,0,1,1,0,0,0],\n",
    "    [0,0,0,1,1,0,0,0],\n",
    "    [0,0,0,1,1,0,0,0],\n",
    "    [0,1,1,1,1,1,0,0]\n",
    "]\n",
    "\n",
    "plt.figure(figsize=(2, 2))\n",
    "plt.imshow(one_grid, cmap='gray_r')\n",
    "plt.title(\"Digit 1\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36814df3",
   "metadata": {},
   "source": [
    "Create your own digits (or other shapes) using the grid below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1767e363",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_grid = [\n",
    "    [0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0]\n",
    "]\n",
    "\n",
    "plt.figure(figsize=(2, 2))\n",
    "plt.imshow(create_grid, cmap='gray_r')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5495e6a4",
   "metadata": {},
   "source": [
    "Let's load real-world data:\n",
    "\n",
    "- Each 8x8 matrix in the digits dataset represents a grayscale image of a handwritten digit.\n",
    "- Each value in the matrix is a pixel intensity:\n",
    "    - 0 means the pixel is completely white (background).\n",
    "    - Higher values (up to 16) mean the pixel is darker (more ink).\n",
    "- So, the matrix is a grid of pixel brightness values, where each number shows how dark that pixel is.\n",
    "- When you plot the matrix as an image, you see the shape of the digit written by hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7189740",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "\n",
    "# Load the digits dataset\n",
    "digits = load_digits()\n",
    "\n",
    "# Print the first 5 images as arrays and their labels\n",
    "for i in range(5):\n",
    "    \n",
    "    print(f\"Image {i} (label: {digits.target[i]}):\")\n",
    "    print(digits.images[i])\n",
    "    \n",
    "    # Show each image in a separate figure with its label\n",
    "    plt.figure(figsize=(2, 2))\n",
    "    plt.imshow(digits.images[i], cmap='gray_r')\n",
    "    plt.title(f\"Label: {digits.target[i]}\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pyactivate)",
   "language": "python",
   "name": "pyactivate"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
