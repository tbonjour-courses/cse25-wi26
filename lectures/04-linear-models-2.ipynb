{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d56799e",
   "metadata": {},
   "source": [
    "## From score to confidence\n",
    "\n",
    "Last time, we looked at the perceptron that uses the **sign** of a **score** to make a decision.\n",
    "\n",
    "- For each training example, it computes the score: \n",
    "    $$\n",
    "      z = \\sum_{i=1}^n w_i x_i + b\n",
    "    $$\n",
    "- It predicts the class based on the sign of the score:\n",
    "    - if $z > 0$, predict class 1\n",
    "    - if $z < 0$, predict class -1\n",
    "\n",
    "This makes a **hard decision**. Let's plot it and see.\n",
    "\n",
    "#### Binary and Multi-Class Classification\n",
    "This is a **binary classification** task because there are only two possible classes: class 1 (positive) and class -1 (negative). The perceptron must decide between these two options for each example. If we have more than two classes, it is called a **multi-class classification** problem. \n",
    "\n",
    "For now, we will keep our focus on the binary case. You will do a multi-class classification task in the `PA`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da16685",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "z_vals = np.linspace(-10, 10, 400)\n",
    "step = (z_vals > 0).astype(int)\n",
    "\n",
    "plt.figure(figsize=(7, 4))\n",
    "plt.axvline(x=0, color='black', linestyle='-', linewidth=2, alpha=0.7, label=\"z=0\")\n",
    "\n",
    "plt.plot(z_vals, step, label=\"Perceptron step(z)\")\n",
    "plt.xlabel(\"Score ($z$)\")\n",
    "plt.ylabel(\"Prediction\")\n",
    "plt.title(\"Perceptron Step Function\")\n",
    "plt.grid(True)\n",
    "plt.ylim(-0.1, 1.1)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009d96e3",
   "metadata": {},
   "source": [
    "### Learning\n",
    "\n",
    "The perceptron learns by updating its weights and bias whenever it makes a **mistake** on a training example. \n",
    "\n",
    "\n",
    "  **Update rule:**\n",
    "\n",
    "  - For each feature $i$:\n",
    "    - $w_i = w_i + y \\cdot x_i$\n",
    "  - Bias:\n",
    "    - $b = b + y$\n",
    "\n",
    "  Here, $y$ is the true label (+1 or -1), and $x_i$ is the value of feature $i$.\n",
    "\n",
    "  This update nudges the model to be more likely to predict the correct class next time for similar examples.\n",
    "\n",
    "\n",
    "\n",
    "### Perceptron Loss\n",
    "\n",
    "The perceptron update can be understood as reducing the **perceptron loss**.\n",
    "\n",
    "The perceptron loss measures how badly a point is misclassified by a linear model.\n",
    "\n",
    "The perceptron loss is given by:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(y, z) = \\max(0, -y z)\n",
    "$$\n",
    "\n",
    "This means:\n",
    "\n",
    "- If $y z > 0$, the prediction is correct and the loss is 0.\n",
    "- If $y z \\le 0$, the point is misclassified and the loss is $-y z$.\n",
    "\n",
    "The perceptron only updates when the loss is nonzero.\n",
    "\n",
    "\n",
    "#### Why the Update Reduces Loss\n",
    "\n",
    "For a misclassified point, $y z \\le 0$.\n",
    "\n",
    "- The weight update changes $z$ in the direction of $y$. The weight update increases $z$ when $y = 1$ and decreases $z$ when $y = -1$.\n",
    "- The bias update does the same.\n",
    "\n",
    "Together, these updates **increase $y z$**.\n",
    "\n",
    "Since the loss is:\n",
    "$$\n",
    "\\max(0, -y z)\n",
    "$$\n",
    "\n",
    "increasing $y z$ directly reduces the perceptron loss for that example and eventually drives it to zero.\n",
    "\n",
    "Let's plot the perceptron loss next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d01a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Range of scores\n",
    "z = np.linspace(-5, 5, 400)\n",
    "\n",
    "# Perceptron loss\n",
    "loss_y_pos = np.maximum(0, -z)   # y = +1\n",
    "loss_y_neg = np.maximum(0, z)    # y = -1\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(z, loss_y_pos, label=\"y = +1\")\n",
    "\n",
    "# Uncomment the next line to see the loss function for when y = -1\n",
    "\n",
    "# plt.plot(z, loss_y_neg, label=\"y = -1\")\n",
    "plt.xlabel(\"Score z\")\n",
    "plt.ylabel(\"Perceptron loss\")\n",
    "plt.title(\"Perceptron Loss\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ea6987",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact, FloatSlider\n",
    "import numpy as np  # Used only for fast grid rendering\n",
    "import math\n",
    "\n",
    "# Data using standard Python lists (labels in {-1, +1})\n",
    "X_toy = [[1.5, 4.0], [1.0, 2.0], [2.0, 1.0], [3.0, 5.0], [4.0, 2.0], [0.0, 0.0], [1.5, -0.5]]\n",
    "y_toy = [1, 1, -1, 1, -1, -1, -1]\n",
    "\n",
    "def combined_perceptron_plot(w1=1.0, w2=1.0, b=-2.0):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # LEFT: 1D Step View (sign)\n",
    "    z_axis = [z * 0.1 for z in range(-100, 101)]\n",
    "    ax1.plot(z_axis, [1 if z >= 0 else -1 for z in z_axis],\n",
    "             color='green', drawstyle='steps-mid', label=\"sign(z)\")\n",
    "    ax1.axvline(x=0, color='black', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    total_loss = 0  # Initialize loss counter\n",
    "    \n",
    "    for i in range(len(X_toy)):\n",
    "        x1, x2 = X_toy[i]\n",
    "        y = y_toy[i]\n",
    "        z_score = w1 * x1 + w2 * x2 + b\n",
    "        pred = 1 if z_score >= 0 else -1\n",
    "        \n",
    "        loss = max(0.0, -y * z_score)  # Perceptron loss\n",
    "        total_loss += loss\n",
    "        \n",
    "        color, m = ('red', 'x') if y == 1 else ('blue', 'o')\n",
    "        if y == 1:\n",
    "            ax1.scatter(z_score, pred, color=color, marker=m, s=100, linewidths=2, zorder=5)\n",
    "        else:\n",
    "            ax1.scatter(z_score, pred, color=color, marker=m, s=80, edgecolors='k', zorder=5)\n",
    "            \n",
    "    # Update title with the calculated loss\n",
    "    ax1.set_title(f\"Perceptron: Score vs Prediction\\nTotal Perceptron Loss: {total_loss:.2f}\")\n",
    "    ax1.set_xlabel(\"Score (z)\")\n",
    "    ax1.set_ylim(-1.1, 1.1)\n",
    "    ax1.set_yticks([-1, 1])\n",
    "\n",
    "    # RIGHT: 2D Feature Space (Speedy Rendering)\n",
    "    xx, yy = np.meshgrid(np.linspace(-1, 5, 100), np.linspace(-1, 6, 100))\n",
    "    zz = w1 * xx + w2 * yy + b\n",
    "\n",
    "    # Binary background: everything <0 is blue, everything >0 is red\n",
    "    ax2.contourf(xx, yy, zz, levels=[-1e9, 0, 1e9], colors=['blue', 'red'], alpha=0.2)\n",
    "\n",
    "    if w2 != 0:\n",
    "        x_line = np.array([-1, 5])\n",
    "        y_line = -(w1 * x_line + b) / w2\n",
    "        ax2.plot(x_line, y_line, \"k-\", linewidth=2)\n",
    "\n",
    "    for i in range(len(X_toy)):\n",
    "        x1, x2 = X_toy[i]\n",
    "        y = y_toy[i]\n",
    "        if y == 1:\n",
    "            ax2.scatter(x1, x2, color='red', marker='x', s=100, linewidths=2)\n",
    "        else:\n",
    "            ax2.scatter(x1, x2, color='blue', marker='o', s=80, edgecolors='k')\n",
    "\n",
    "    ax2.set_xlim(-1, 5)\n",
    "    ax2.set_ylim(-1, 6)\n",
    "    ax2.set_title(\"2D View: Hard Decision Regions\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "interact(combined_perceptron_plot,\n",
    "         w1=(-5.0, 5.0, 0.1),\n",
    "         w2=(-5.0, 5.0, 0.1),\n",
    "         b=(-10.0, 10.0, 0.1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0fe4ce",
   "metadata": {},
   "source": [
    "Sometimes we don’t want a *hard* yes/no.\n",
    "\n",
    "Instead, we might want to say things like:\n",
    "- \"I'm very confident this is class 1\"\n",
    "- \"I'm somewhat confident this is class 1\"\n",
    "- \"I'm unsure\"\n",
    "- \"I'm somewhat confident this is class 0\"\n",
    "- \"I'm confident this is class 0\"\n",
    "\n",
    "To do this, we can instead use a function that:\n",
    "- takes any number (positive or negative) and outputs a value between 0 and 1\n",
    "\n",
    "\n",
    "Q: Why a value between 0 and 1?\n",
    "\n",
    "`YOUR ANSWER HERE`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec19bfd",
   "metadata": {},
   "source": [
    "One common function used for this is called the **sigmoid** function.\n",
    "\n",
    "It takes any number, (or in our case, the score $z$) and outputs a number between 0 and 1:\n",
    "\n",
    "$$\n",
    "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "We will treat this output as a **confidence**.\n",
    "\n",
    "**Note**: *We interpret this value as a confidence, but mathematically it behaves like a probability.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32825f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete the sigmoid function below.\n",
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    z: a numeric value.\n",
    "    Return sigmoid(z) = 1 / (1 + exp(-z)).\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    # You can use math.exp() to compute the exponential of a number.\n",
    "    \n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7201eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use assertions for 4 key test cases\n",
    "assert abs(sigmoid(0) - 0.5) < 1e-7, \"sigmoid(0) failed\"\n",
    "assert abs(sigmoid(1) - 0.7310586) < 1e-7, \"sigmoid(1) failed\"\n",
    "assert abs(sigmoid(-1) - 0.2689414) < 1e-7, \"sigmoid(-1) failed\"\n",
    "assert abs(sigmoid(-10) - 0.0000454) < 1e-7, \"sigmoid(-10) failed\"\n",
    "assert abs(sigmoid(10) - 0.9999546) < 1e-7, \"sigmoid(10) failed\"\n",
    "\n",
    "print(\"All key sigmoid test cases passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71c15ad",
   "metadata": {},
   "source": [
    "Q. What happens if we try to compute `sigmoid(-800)` with the sigmoid implementation above? Why?\n",
    "\n",
    "`YOUR ANSWER HERE`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d09456",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sigmoid(-800)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e0d816",
   "metadata": {},
   "source": [
    "The sigmoid function can be written in two equivalent forms:\n",
    "\n",
    "$$\n",
    "\\sigma(z) = \\frac{1}{1 + e^{-z}} = \\frac{e^{z}}{1 + e^{z}}\n",
    "$$\n",
    "\n",
    "Both formulas produce the same output for any value of $z$. The second form is especially useful for large negative $z$, as it avoids numerical overflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6afbc862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete the sigmoid function below.\n",
    "def stable_sigmoid(z):\n",
    "    \"\"\"\n",
    "    This implementation avoids overflow issues by handling large positive and negative values of z separately.\n",
    "\n",
    "    z: a numeric value.\n",
    "\n",
    "    if z >= 0: use the 'standard' formula: 1/(1 + exp(-z))\n",
    "    if z < 0: use the alternative formula to avoid overflow: exp(z) / (1 + exp(z))\n",
    "    \"\"\"\n",
    "    if z >= 0:\n",
    "        return 1 / (1 + math.exp(-z))\n",
    "    else:\n",
    "        ez = math.exp(z)\n",
    "        return ez / (1 + ez)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50bf25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "stable_sigmoid(-800)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80652089",
   "metadata": {},
   "source": [
    "Now, let's plot the sigmoid function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3c482a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7, 4))\n",
    "sigmoid_values = [stable_sigmoid(z) for z in z_vals]\n",
    "plt.plot(z_vals, sigmoid_values, label=\"Sigmoid(z)\")\n",
    "plt.axvline(x=0, color='black', linestyle='-', linewidth=2, alpha=0.7, label=\"z=0\")\n",
    "plt.xlabel(\"z\")\n",
    "plt.ylabel(\"sigmoid(z)\")\n",
    "plt.title(\"Sigmoid Function\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2acd6bfa",
   "metadata": {},
   "source": [
    "Now we can combine the steps and make a prediciton:\n",
    "\n",
    "1. Compute a score: \n",
    " $$\n",
    "      z = \\sum_{i=1}^n w_i x_i + b\n",
    "$$\n",
    "2. Turn the score into a confidence value $p$: $$p = \\sigma(z)$$\n",
    "\n",
    "We have not changed how the score is computed -  we only changed what we do *after* the score.\n",
    "\n",
    "- To make a prediction:\n",
    "    - If $p > 0.5$, predict class 1\n",
    "    - If $p < 0.5$, predict class 0\n",
    "\n",
    "This means the model outputs a **confidence** (between 0 and 1), and we use a threshold (usually 0.5) to decide the final class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c92efbc",
   "metadata": {},
   "source": [
    "## From confidence to loss\n",
    "\n",
    "A **loss function** tells us how bad the model’s prediction was.\n",
    "\n",
    "- Small loss -> the model did well\n",
    "- Large loss -> the model did poorly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5528b66e",
   "metadata": {},
   "source": [
    "***Note**: We will switch the classes of $y$ from $-1$ and $1$ to $0$ and $1$ for mathematical convenience. This change does not make a difference to the underlying logic or results.*\n",
    "\n",
    "For binary classification, we want a loss function such that:\n",
    "\n",
    "- If the true label is 1:\n",
    "  - high confidence (close to 1) → small loss\n",
    "  - low confidence (close to 0) → large loss\n",
    "\n",
    "- If the true label is 0:\n",
    "  - low confidence (close to 0) → small loss\n",
    "  - high confidence (close to 1) → large loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3733a5fb",
   "metadata": {},
   "source": [
    "One commonly used loss function that has exactly this behavior\n",
    "is called **binary cross-entropy**.\n",
    "\n",
    "For today, you do NOT need to know where this formula comes from.\n",
    "You only need to know that:\n",
    "\n",
    "- it behaves the way we want\n",
    "- it is smooth\n",
    "- it works well with sigmoid\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23258c47",
   "metadata": {},
   "source": [
    "Binary cross-entropy loss for one data point is:\n",
    "- For $y=1$: $L(p, 1) = -\\log(p)$  \n",
    "- For $y=0$: $L(p, 0) = -\\log(1-p)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372c9029",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "p_vals = np.linspace(0.001, 0.999, 500)\n",
    "bce_y1 = [-np.log(p) for p in p_vals]           # y = 1\n",
    "bce_y0 = [-np.log(1 - p) for p in p_vals]       # y = 0\n",
    "plt.figure(figsize=(7, 4))\n",
    "plt.plot(p_vals, bce_y1, label=\"y = 1\")\n",
    "# plt.plot(p_vals, bce_y0, label=\"y = 0\")\n",
    "plt.xlabel(\"$p$\")\n",
    "plt.ylabel(\"Binary cross-entropy loss\")\n",
    "plt.title(\"Binary Cross-Entropy Loss vs Confidence\")\n",
    "plt.legend()\n",
    "plt.axvline(x=0.5, color='gray', linestyle='--', linewidth=2, alpha=0.5)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c45482d",
   "metadata": {},
   "source": [
    "Combining the two cases, we get: \n",
    "$$\n",
    "\\mathcal{L}(p, y) = -\\big(y \\log(p) + (1-y)\\log(1-p)\\big)\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $p$ is the model’s confidence (from sigmoid)\n",
    "- $y$ is the true label (0 or 1)\n",
    "\n",
    "Complete the code in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4649f1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete the binary cross-entropy loss function below.\n",
    "\n",
    "def binary_cross_entropy(p, y):\n",
    "    \"\"\"\n",
    "    p: model confidence [0, 1]\n",
    "    y: true label (0 or 1)\n",
    "    \"\"\"\n",
    "\n",
    "    # Return the binary cross-entropy loss: -(y * log(p) + (1 - y) * log(1 - p))\n",
    "    # You can use math.log() to compute the logarithm of a number.\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d459cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# True label = 1\n",
    "print(binary_cross_entropy(0.9, 1))  # should be small\n",
    "print(binary_cross_entropy(0.1, 1))  # should be large\n",
    "\n",
    "# True label = 0\n",
    "print(binary_cross_entropy(0.1, 0))  # should be small\n",
    "print(binary_cross_entropy(0.9, 0))  # should be large"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e58f7f",
   "metadata": {},
   "source": [
    "Q. For which values of $p$ is this loss undefined or numerically unstable? Why?\n",
    "\n",
    "`YOUR ANSWER HERE`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc882375",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stable_binary_cross_entropy(p, y):\n",
    "    \"\"\"\n",
    "    Clips p to avoid (math) issues, then computes binary cross-entropy loss.\n",
    "    \n",
    "    p: model confidence (between 0 and 1)\n",
    "    y: true label (0 or 1)\n",
    "    \"\"\"\n",
    "    eps = 1e-8\n",
    "\n",
    "    if p < eps:\n",
    "        p = eps\n",
    "    if p > 1 - eps:\n",
    "        p = 1 - eps\n",
    "\n",
    "    return -(y * math.log(p) + (1 - y) * math.log(1 - p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2665dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# should not error out\n",
    "print(stable_binary_cross_entropy(0.0, 1))\n",
    "print(stable_binary_cross_entropy(1.0, 0)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a899b79d",
   "metadata": {},
   "source": [
    "So far we have:\n",
    "\n",
    "1. Compute a score:  \n",
    "   $z = \\sum_{i=1}^n w_i x_i + b$\n",
    "\n",
    "2. Turn the score into a confidence:  \n",
    "   $p = \\sigma(z)$\n",
    "\n",
    "3. Measure how good that confidence was using a loss (given the true label $y$).\n",
    "\n",
    "This gives the *forward* computation:\n",
    "  \n",
    "$x \\xrightarrow{\\,w,b\\,} z \\xrightarrow{\\,\\sigma(.)\\,} p \\xrightarrow{\\,y\\,} L$\n",
    "\n",
    "Each arrow means \"is computed from\" or \"depends on\".\n",
    "\n",
    "This sequence is the **computation graph** for our model.\n",
    "\n",
    "Next, we will ask:\n",
    "\n",
    "- How should we change the parameters $w$ and $b$ so that the loss $L$ becomes smaller?\n",
    "\n",
    "But, first let's visualize how changing the parameters affects the decision boundary and the loss. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6b53c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the toy data again for the next section\n",
    "X_toy = [\n",
    "    [1.5, 4],\n",
    "    [1, 2],\n",
    "    [2, 1],\n",
    "    [3, 5],\n",
    "    [4, 2],\n",
    "    [0, 0],\n",
    "    [1.5, -0.5]\n",
    "]\n",
    "y_toy = [1, 1, 0, 1, 0, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c81fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact, FloatSlider\n",
    "\n",
    "def combined_interactive_plot(w1=1.0, w2=1.0, b=0.0):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # LEFT PLOT: Sigmoid Confidence (1D View)\n",
    "    z_range = np.linspace(-10, 10, 400)\n",
    "    sig_vals = 1 / (1 + np.exp(-z_range))\n",
    "   \n",
    "\n",
    "    ax1.plot(z_range, sig_vals, color='green', label=\"Sigmoid(z)\")\n",
    "    ax1.axvline(x=0, color='black', linestyle='--', alpha=0.5, label=\"Boundary (z=0)\")\n",
    "    \n",
    "    # Plot toy points on the sigmoid curve\n",
    "    total_loss = 0\n",
    "    for x, y in zip(X_toy, y_toy):\n",
    "        z_score = w1 * x[0] + w2 * x[1] + b\n",
    "        p = 1 / (1 + np.exp(-z_score))\n",
    "        \n",
    "        # Calculate loss for annotation\n",
    "        p_clipped = np.clip(p, 1e-8, 1 - 1e-8)\n",
    "        loss = -(y * np.log(p_clipped) + (1 - y) * np.log(1 - p_clipped))\n",
    "        total_loss += loss\n",
    "        \n",
    "        color = 'red' if y == 1 else 'blue'\n",
    "        ax1.scatter(z_score, p, color=color, s=50, edgecolors='k', zorder=5)\n",
    "    \n",
    "    ax1.set_xlabel(\"Score ($z = w_1x_1 + w_2x_2 + b$)\")\n",
    "    ax1.set_ylabel(\"Confidence ($p$)\")\n",
    "    ax1.set_title(f\"1D View: Confidence vs Score\\nTotal CE Loss: {total_loss:.2f}\")\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.legend()\n",
    "\n",
    "    # RIGHT PLOT: Decision Boundary (2D View)\n",
    "    x1_min, x1_max = -1, 5\n",
    "    x2_min, x2_max = -1, 6\n",
    "    xx, yy = np.meshgrid(np.linspace(x1_min, x1_max, 100), np.linspace(x2_min, x2_max, 100))\n",
    "    z_grid = w1*xx + w2*yy + b\n",
    "    p_grid = 1 / (1 + np.exp(-z_grid))\n",
    "    \n",
    "    # Background probability contour\n",
    "    contour = ax2.imshow(p_grid, origin=\"lower\", extent=[x1_min, x1_max, x2_min, x2_max], \n",
    "                         aspect=\"auto\", alpha=0.3, cmap='RdBu_r')\n",
    "    \n",
    "    # Decision boundary line (where z=0)\n",
    "    if w2 != 0:\n",
    "        x1_vals = np.array([x1_min, x1_max])\n",
    "        x2_vals = -(w1 * x1_vals + b) / w2\n",
    "        ax2.plot(x1_vals, x2_vals, \"k--\", label=\"z=0\")\n",
    "    elif w1 != 0:\n",
    "        x0 = -b / w1\n",
    "        ax2.axvline(x=x0, color=\"k\", linestyle=\"--\", label=\"z=0\")\n",
    "    \n",
    "    # Plot the original points\n",
    "    labeled0 = False\n",
    "    labeled1 = False\n",
    "    for x, y in zip(X_toy, y_toy):\n",
    "        if y == 1:\n",
    "            ax2.scatter(x[0], x[1], c='red', marker='x', s=100,\n",
    "                label=\"Class 1\" if not labeled1 else \"\")\n",
    "            labeled1 = True\n",
    "        else:\n",
    "            ax2.scatter(x[0], x[1], c='blue', marker='o', s=100,\n",
    "                label=\"Class 0\" if not labeled0 else \"\")\n",
    "            labeled0 = True\n",
    "\n",
    "    ax2.set_xlim(x1_min, x1_max)\n",
    "    ax2.set_ylim(x2_min, x2_max)\n",
    "    ax2.set_xlabel(\"Feature $x_1$\")\n",
    "    ax2.set_ylabel(\"Feature $x_2$\")\n",
    "    ax2.set_title(\"2D View: Feature Space Boundary\")\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.legend()\n",
    "    \n",
    "\n",
    "interact(\n",
    "    combined_interactive_plot,\n",
    "    w1=FloatSlider(value=1.0, min=-5, max=5, step=0.1),\n",
    "    w2=FloatSlider(value=1.0, min=-5, max=5, step=0.1),\n",
    "    b=FloatSlider(value=-2.0, min=-5, max=5, step=0.1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd103001",
   "metadata": {},
   "source": [
    "## Reducing the loss\n",
    "\n",
    "Earlier in the course, we reduced error by asking:\n",
    "\n",
    "- If we change a parameter slightly, how does the error change?\n",
    "- In which direction should we move the parameter to reduce error?\n",
    "- How sensitive is the error to that parameter?\n",
    "\n",
    "We answered these questions by treating error as a function of the parameters and using **gradients** to understand how changes in the parameters affect the error.\n",
    "\n",
    "[SIDE QUEST](https://docs.google.com/presentation/d/1g1uaurl-GckLb47FskBPzgtsrQJPiAQUhdVML_eOI_8/edit?usp=sharing): Play the slideshow and watch the videos to get some intuition aobut gradients. I have time marked them in the slides, but please feel free to watch the full video.\n",
    "\n",
    "Here, we use the same idea, but with a more precise quantity called a **loss**.\n",
    "\n",
    "The goal is the same:\n",
    "understand how the loss changes with respect to the parameters so that we can update $w$ and $b$ to reduce it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9014c78e",
   "metadata": {},
   "source": [
    "Our model has three parameters:\n",
    "- `w1`: weight for feature x1\n",
    "- `w2`: weight for feature x2\n",
    "- `b`: bias\n",
    "\n",
    "Computation Graph: \n",
    "\n",
    "$x \\xrightarrow{\\,w,b\\,} z \\xrightarrow{\\,\\sigma(.)\\,} p \\xrightarrow{\\,y\\,} L$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320f1ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The forward pass to get the confidence values\n",
    "def forward(X, w1, w2, b):\n",
    "    \"\"\"\n",
    "    Inputs: \n",
    "    X: list of data points, where each point is a list of 2 features [x1, x2]\n",
    "    w1, w2: weights for the two features\n",
    "    b: bias term\n",
    "\n",
    "    Output: A list of p (confidence values between 0 and 1) for each data point\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize an empty list to store confidence values\n",
    "    p_list = []\n",
    "\n",
    "    # Iterate through each data point in X\n",
    "    for i in range(len(X)):\n",
    "        x1 = X[i][0]\n",
    "        x2 = X[i][1]\n",
    "        \n",
    "        # Calculate the linear score z = w1*x1 + w2*x2 + b\n",
    "        z = w1 * x1 + w2 * x2 + b\n",
    "        \n",
    "        # Use the score to get the sigmoid confidence\n",
    "        p = stable_sigmoid(z)\n",
    "\n",
    "        # Append the confidence value to the list\n",
    "        p_list.append(p)\n",
    "        \n",
    "    return p_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690d5a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Loss Function\n",
    "def get_avg_binary_cross_entropy(p_list, y):\n",
    "    \"\"\"\n",
    "    \n",
    "    p_list: list of predicted confidence values (between 0 and 1)\n",
    "    y: list of true labels (0 or 1)\n",
    "\n",
    "    \"\"\"\n",
    "    total_loss = 0.0\n",
    "    for i in range(len(p_list)):\n",
    "        p = p_list[i]\n",
    "        y_true = y[i]\n",
    "        \n",
    "        # Safety: Clip p to avoid log(0) error\n",
    "        loss_i = stable_binary_cross_entropy(p, y_true)\n",
    "        total_loss += loss_i\n",
    "        \n",
    "    return total_loss / len(p_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05deb777",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The gradient calculation using the finite difference method\n",
    "# To see how $(w,b)$ affect $L$, we nudge one parameter at a time and look at how that affects the loss.\n",
    "\n",
    "def get_gradients(X, y, w1, w2, b):\n",
    "    eps = 0.0001\n",
    "    \n",
    "    # STEP 0: Baseline - Where are we now?\n",
    "    # We must Run Forward -> Then Calculate Loss\n",
    "    base_conf = forward(X, w1, w2, b)\n",
    "    base_loss  = get_avg_binary_cross_entropy(base_conf, y)\n",
    "    \n",
    "    # STEP 1: Get gradient for w1\n",
    "    # Nudge w1 -> Re-run Forward -> Re-calculate Loss\n",
    "    w1_conf = forward(X, w1 + eps, w2, b)       # <--- SHOW THIS TO STUDENTS\n",
    "    w1_loss  = get_avg_binary_cross_entropy(w1_conf, y) # <--- AND THIS\n",
    "    grad_w1  = (w1_loss - base_loss) / eps\n",
    "    \n",
    "    # STEP 2: Get gradient for w2\n",
    "    # Nudge w2 -> Re-run Forward -> Re-calculate Loss\n",
    "    w2_conf = forward(X, w1, w2 + eps, b)\n",
    "    w2_loss  = get_avg_binary_cross_entropy(w2_conf, y)\n",
    "    grad_w2  = (w2_loss - base_loss) / eps\n",
    "    \n",
    "    # STEP 3: Get gradient for bias\n",
    "    # Nudge b -> Re-run Forward -> Re-calculate Loss\n",
    "    b_conf = forward(X, w1, w2, b + eps)\n",
    "    b_loss  = get_avg_binary_cross_entropy(b_conf, y)\n",
    "    grad_b  = (b_loss - base_loss) / eps\n",
    "     \n",
    "    return grad_w1, grad_w2, grad_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659340e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Training Loop\n",
    "\n",
    "# Initialize Parameters\n",
    "w1 = 0.0\n",
    "w2 = 0.0\n",
    "b  = 0.0\n",
    "\n",
    "# Initialize hyperparameters\n",
    "learning_rate = 0.1\n",
    "epochs = 500\n",
    "\n",
    "print(f\"Initial Loss: {get_avg_binary_cross_entropy(forward(X_toy, w1, w2, b), y_toy):.4f}\")\n",
    "\n",
    "for i in range(epochs):\n",
    "    \n",
    "    # 1. Calculate Gradients (This runs the model 4 times!)\n",
    "    dw1, dw2, db = get_gradients(X_toy, y_toy, w1, w2, b)\n",
    "    \n",
    "    # 2. Update Weights\n",
    "    w1 = w1 - learning_rate * dw1\n",
    "    w2 = w2 - learning_rate * dw2\n",
    "    b  = b  - learning_rate * db\n",
    "    \n",
    "    if i % 50 == 0:\n",
    "        # Check progress\n",
    "        curr_preds = forward(X_toy, w1, w2, b)\n",
    "        curr_loss = get_avg_binary_cross_entropy(curr_preds, y_toy)\n",
    "        print(f\"Iter {i}: w1={w1:.2f}, w2={w2:.2f}, b={b:.2f} | Loss={curr_loss:.4f}\")\n",
    "\n",
    "print(\"\\nFinal Result:\")\n",
    "print(\"Final loss:\", curr_loss)\n",
    "\n",
    "print(f\"w1: {w1:.4f}, w2: {w2:.4f}, b: {b:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cad0148",
   "metadata": {},
   "source": [
    "We just trained our model by repeating the follwing steps:\n",
    "\n",
    "- Running the model forward to get the confidence values $p$\n",
    "- Computing the loss using $p$ and $y$\n",
    "- Nudging one parameter at a time to see how the loss changes\n",
    "- Updating the parameters using those estimates\n",
    "\n",
    "\n",
    "This works - but it is slow, not optimal. Why? \n",
    "\n",
    "*Hint:*  Look closely at `get_gradients(X, y, w1, w2, b)`. How many times does the model run forward and recompute the loss during **one training step**?\n",
    "\n",
    "\n",
    "`YOUR ANSWER HERE`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pyactivate)",
   "language": "python",
   "name": "pyactivate"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
