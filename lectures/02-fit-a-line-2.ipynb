{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62fa2837",
   "metadata": {},
   "source": [
    "# CSE 25 - Introduction to Artificial Intelligence\n",
    "## Worksheet 6: Learning by Reducing Error\n",
    "\n",
    "**Context (from last class):**  \n",
    "Last time, we fit a line by manually adjusting parameters and measuring error.\n",
    "We saw that different parameter values lead to different errors.\n",
    "\n",
    "Today, we focus on the question:\n",
    "**How can a system automatically change its parameters to reduce error?**\n",
    "\n",
    "**Learning Objectives:**\n",
    "By the end of today’s class, you will be able to:\n",
    "\n",
    "- Interpret error as a function of a model parameter.\n",
    "- Explain how changing a parameter affects error.\n",
    "- Describe brute-force search as a way to find better parameters.\n",
    "- Explain, at a high level, how slope (rate of change) tells us *which direction* to adjust a parameter.\n",
    "- Connect these ideas to the motivation behind gradient descent.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "Create a copy of this notebook and complete it during class.  \n",
    "Work through the cells below **in order**.\n",
    "\n",
    "You may discuss with your neighbors, but make sure you understand each step yourself.\n",
    "\n",
    "**Submission**\n",
    "\n",
    "When finished, download the notebook as a PDF and upload it to Gradescope under  \n",
    "`In-Class – Week 4 Tuesday`.\n",
    "\n",
    "To download as a PDF on DataHub:\n",
    "`File -> Save and Export Notebook As -> PDF`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a43bff",
   "metadata": {},
   "source": [
    "### From last time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da027b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the matplotlib library for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Data for miles and corresponding kilometers\n",
    "# Stored in lists\n",
    "miles = [0, 7, 12, 20, 30] # x-axis values (INPUT DATA)\n",
    "km = [0, 12, 20, 32, 49] # y-axis values (OUTPUT DATA)\n",
    "\n",
    "# Create a scatter plot - using plt.scatter() function\n",
    "plt.scatter(miles, km)\n",
    "\n",
    "# Add labels and title using plt.xlabel(), plt.ylabel(), and plt.title() functions\n",
    "plt.xlabel('Miles')\n",
    "plt.ylabel('KM')\n",
    "plt.title('Miles vs KM')\n",
    "plt.grid()\n",
    "\n",
    "# Show the plot using plt.show() function\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451f445e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pointwise_absolute_error(actual_values, predicted_values):\n",
    "    '''\n",
    "    actual_values: list of actual output values\n",
    "    predicted_values: list of predicted output values\n",
    "\n",
    "    Complete the function that calculates the absolute error between predicted and actual values for each value and returns a list of these absolute errors.\n",
    "    '''\n",
    "     # Initialize an empty list to store errors\n",
    "    errors = []\n",
    "    \n",
    "    # Calculate the absolute error for each point and store it in the list\n",
    "    for actual_value_idx in range(len(actual_values)):\n",
    "        errors.append(abs(actual_values[actual_value_idx] - predicted_values[actual_value_idx]))\n",
    "    \n",
    "    # Return the list of absolute errors\n",
    "    return errors\n",
    "\n",
    "\n",
    "def get_total_error(errors):\n",
    "    '''\n",
    "    errors: list of errors for each data point\n",
    "    Complete the function that calculates the total error by summing the values of the individual errors. Return the total error.\n",
    "    '''\n",
    "    # Use the sum() function to calculate the total error\n",
    "    return sum(errors)\n",
    "\n",
    "def get_predictions_v2(input_values, w, b):\n",
    "    '''\n",
    "    input_values: list of input values\n",
    "    w: weight (slope)\n",
    "    b: bias (intercept)\n",
    "\n",
    "    Complete the function that calculates the predicted output values using the weight and bias. \n",
    "    Return a list of predicted values.\n",
    "    '''\n",
    "    # Initialize an empty list to store predicted values\n",
    "    predicted_values = []\n",
    "\n",
    "    # Calculate predicted values using the formula: predicted_value = input_value * w + b\n",
    "    # Append each predicted value to the predicted_values list\n",
    "    for input_value in input_values:\n",
    "        predicted_values.append(input_value * w + b)\n",
    "    # Return the list of predicted values\n",
    "    return predicted_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c55f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Brute-force search for the best weight (conversion factor) with bias fixed to 0\n",
    "\n",
    "best_weight = None # Initialize best weight\n",
    "lowest_error = float('inf') # Initialize to a very high value\n",
    "\n",
    "weights = [] # To store weights tried\n",
    "total_error_list = [] # To store total errors for each weight\n",
    "\n",
    "w = 1.0\n",
    "b = 0.0\n",
    "step_size = 0.001\n",
    "\n",
    "while w <= 2.0:\n",
    "    predicted = get_predictions_v2(miles, w, b)\n",
    "    errors = get_pointwise_absolute_error(km, predicted)\n",
    "    total_error = get_total_error(errors)\n",
    "    total_error_list.append(total_error) # for the plot\n",
    "    weights.append(w) # for the plot\n",
    "    if total_error < lowest_error:\n",
    "        lowest_error = total_error\n",
    "        best_weight = w\n",
    "    w += step_size\n",
    "\n",
    "print(f'Best weight (conversion factor): {best_weight:.4f}')\n",
    "print(f'Lowest total error: {lowest_error:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49cca735",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "\n",
    "# Changed the plot from scatter to line plot to visualize total error vs weight\n",
    "plt.plot(weights,  total_error_list)\n",
    "\n",
    "plt.xlabel(\"Weight\")\n",
    "plt.ylabel(\"Total Absolute Error\")\n",
    "plt.title(\"Brute-Force Search for Best Weight\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29cc29f",
   "metadata": {},
   "source": [
    "### Problem: Total error depends on dataset size!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275209aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Lets say we are given many more data points - what would happen to the total error?\n",
    "\n",
    "miles_many = [0, 5, 7, 10, 12, 15, 18, 20, 22, 25, 27, 30, 32, 35, 37, 40, 42, 45, 47, 50]\n",
    "km_many = [0, 8, 12, 16, 20, 24, 29, 32, 36, 41, 44, 49, 52, 57, 60, 65, 68, 73, 76, 81]\n",
    "\n",
    "# Use the best_weight and b from previous cells to get the total error on the original points\n",
    "predicted_original = get_predictions_v2(miles, best_weight, b)\n",
    "errors_original = get_pointwise_absolute_error(km, predicted_original)\n",
    "total_error_original = get_total_error(errors_original)\n",
    "\n",
    "# Use best_weight and b from previous cells to get the total error on more data points\n",
    "# YOUR CODE HERE\n",
    "predicted_many = get_predictions_v2(miles_many, best_weight, b)\n",
    "errors_many = get_pointwise_absolute_error(km_many, predicted_many)\n",
    "total_error_many = get_total_error(errors_many)\n",
    "\n",
    "print(f'Total error (original points): {total_error_original:.2f}')\n",
    "print(f\"Total error (many more points): {total_error_many:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1feb5b",
   "metadata": {},
   "source": [
    "#### Why does the total error increase with more data points?\n",
    "\n",
    "Notice that even if our model fits the data just as well, the total error becomes much larger when we have more data points. This is because the total error is simply the sum of all individual errors, so adding more points (even with similar errors per point) will always increase the total.\n",
    "\n",
    "This means that total error depends on the size of the dataset, not just how well the model fits. To fairly compare models or datasets of different sizes, we need a metric that is independent of the number of data points.\n",
    "\n",
    "A common solution is to use the **mean (average) error per point**, called the **Mean Absolute Error (MAE)**. This gives us a normalized measure of error that is easier to compare across datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1423c597",
   "metadata": {},
   "source": [
    "The Mean Absolute Error (MAE) is calculated as:\n",
    "\n",
    "$$\n",
    "\\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} \\left| y_i - \\hat{y}_i \\right|\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $n$ is the number of data points\n",
    "- $y_i$ is the actual value\n",
    "- $\\hat{y}_i$ is the predicted value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e84ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete the function that calculates the Mean Absolute Error (MAE) between predicted and actual values.\n",
    "\n",
    "def get_mean_absolute_error(actual_values, predicted_values):\n",
    "    '''\n",
    "    actual_values: list of actual output values\n",
    "    predicted_values: list of predicted output values\n",
    "\n",
    "    Complete the function that calculates the Mean Absolute Error (MAE) between predicted and actual values.\n",
    "    Return the MAE.\n",
    "    '''\n",
    "    # Initialize a list to store the absolute errors\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    # Loop through each value in actual_values and predicted_values \n",
    "    # to calculate the absolute error and store it in the list\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    # Calculate the mean absolute error (MAE) by dividing the total absolute error by the number of values\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    # Return the MAE\n",
    "    # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a09d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Test Cases ---\n",
    "\n",
    "# Test 1: Simple integers\n",
    "act1 = [10, 20, 30]\n",
    "pre1 = [12, 18, 25]\n",
    "\n",
    "# Calculation: (|10-12| + |20-18| + |30-25|) / 3 = (2 + 2 + 5) / 3 = 3.0\n",
    "print(f\"Test 1 Results: {get_mean_absolute_error(act1, pre1)}\")\n",
    "\n",
    "# Test 2: Perfect match (Error should be 0)\n",
    "act2 = [1.5, 2.5, 3.5]\n",
    "pre2 = [1.5, 2.5, 3.5]\n",
    "print(f\"Test 2 Results: {get_mean_absolute_error(act2, pre2)}\")\n",
    "\n",
    "# Test 3: Large differences\n",
    "act3 = [100, 0]\n",
    "pre3 = [0, 100]\n",
    "# Calculation: (|100-0| + |0-100|) / 2 = (100 + 100) / 2 = 100.0\n",
    "print(f\"Test 3 Results: {get_mean_absolute_error(act3, pre3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14aa332",
   "metadata": {},
   "source": [
    "Let's run our brute force search again, but this time, we will use the MAE intead. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bad68a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Brute-force search for the best weight (conversion factor) with bias fixed to 0\n",
    "# Using Mean Absolute Error as the error metric\n",
    "\n",
    "best_weight = None # Initialize best weight\n",
    "lowest_error = float('inf') # Initialize to a very high value\n",
    "\n",
    "weights = [] # To store weights tried\n",
    "mean_absolute_errors_list = [] # To store mean absolute errors for each weight\n",
    "\n",
    "w = 1.0\n",
    "b = 0.0\n",
    "step_size = 0.001\n",
    "\n",
    "while w <= 2.0:\n",
    "    predicted = get_predictions_v2(miles, w, b)\n",
    "    mean_absolute_error = get_mean_absolute_error(km, predicted) # This line was changed\n",
    "    mean_absolute_errors_list.append(mean_absolute_error) # for the plot\n",
    "    weights.append(w) # for the plot\n",
    "    if mean_absolute_error < lowest_error:\n",
    "        lowest_error = mean_absolute_error\n",
    "        best_weight = w\n",
    "    w += step_size\n",
    "\n",
    "print(f'Best weight (conversion factor): {best_weight:.4f}')\n",
    "print(f'Lowest mean absolute error: {lowest_error:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9bbbef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Use the best_weight and b from previous cells to get the mean absolute error on the original points\n",
    "predicted_original = get_predictions_v2(miles, best_weight, b)\n",
    "mean_absolute_error_original = get_mean_absolute_error(km, predicted_original)\n",
    "\n",
    "# Use the best_weight and b from previous cells to get the mean absolute error on the many points\n",
    "# YOUR CODE HERE\n",
    "predicted_many = get_predictions_v2(miles_many, best_weight, b)\n",
    "mean_absolute_error_many = get_mean_absolute_error(km_many, predicted_many)\n",
    "\n",
    "print(f'Mean absolute error (original points): {mean_absolute_error_original:.2f}')\n",
    "print(f\"Mean absolute error (many more points): {mean_absolute_error_many:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521304d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(weights,  mean_absolute_errors_list)\n",
    "plt.xlabel(\"Weight\")\n",
    "plt.ylabel(\"Mean Absolute Error\")\n",
    "plt.title(\"Brute-Force Search for Best Weight\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935c498a",
   "metadata": {},
   "source": [
    "What changed in the graph? Did the shape of the graph change or remain the same?\n",
    "\n",
    "`YOUR ANSWER HERE`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b71fa21",
   "metadata": {},
   "source": [
    "### Error as a function\n",
    "\n",
    "\n",
    "What is a function? \n",
    "\n",
    "`YOUR ANSWER HERE`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba63ef5a",
   "metadata": {},
   "source": [
    "We can say that the error $e$, is a function of the weight, $w$.\n",
    "\n",
    "As weight changes on the x-axis, the error changes on the y-axis in the plot above. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7786070e",
   "metadata": {},
   "source": [
    "In the last class, we used a brute force approach to find the best parameters for our model (line). What if we can use a better method to find the best parameters. \n",
    "\n",
    "What is our parameter in the model: $$y=wx$$\n",
    "\n",
    "Parameter: $w$\n",
    "\n",
    "Now if we can somehow know how changing $w$ changes the error, we could use that information to update the parameter value, instead of using the brute-force method. And we know that the error is a function of weight! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d95db2",
   "metadata": {},
   "source": [
    "\n",
    "Let's rewrite the error as a function of the weight:\n",
    "$$e = f(w)$$\n",
    "\n",
    "Let's say we change our weight $w$  by adding a **very small** value, say $\\epsilon > 0$, the new error $e^\\prime$ is now:\n",
    "$$e^\\prime = f(w+\\epsilon)$$\n",
    "\n",
    "Now I want to see the ratio of how the error changed, with respect to the change in weight.\n",
    "$$\\frac{e^\\prime - e}{\\epsilon} = \\frac{f(w+\\epsilon) - f(w)}{\\epsilon}$$\n",
    "\n",
    "This ratio is a way to measure how sensitive the error is to small changes in the weight. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28548a1b",
   "metadata": {},
   "source": [
    "### Derivative\n",
    "\n",
    "The equation in the last cell is a **finite difference approximation** of the derivatie.\n",
    "\n",
    "The **limit definition of the derivative** is:\n",
    "\n",
    "$$\n",
    "\\frac{de}{dw} = \\lim_{dw \\to 0} \\frac{f(w+dw) - f(w)}{dw}\n",
    "$$\n",
    "\n",
    "[Video Link for Derivative](https://youtu.be/9vKqVkMQHKk?si=KcqjJqi4GPrGVQqj)\n",
    "\n",
    "This tells us the exact rate at which the error $e$ changes as we make tiny changes to the weight $w$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07af7c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete the function that calculates the derivative of error with respect to weight (w) using limit definition\n",
    "\n",
    "def get_derivative_error_wrt_weight(error_function, input_values, actual_values, w, b, dw=1e-6):\n",
    "    \"\"\"\n",
    "    Calculates the derivative of the error with respect to weight (w) using the limit definition.\n",
    "\n",
    "    error_function: function to compute error\n",
    "    input_values: list of input values\n",
    "    actual_values: list of actual output values\n",
    "    w: current weight\n",
    "    b: current bias\n",
    "    \n",
    "    dw: small value for finite difference approximation\n",
    "\n",
    "    Returns:\n",
    "        derivative (float): Approximate derivative of error with respect to w\n",
    "    \"\"\"\n",
    "    # Get predictions at current w\n",
    "    predicted_w = get_predictions_v2(input_values, w, b)\n",
    "\n",
    "    # Compute error at w\n",
    "    error_w = error_function(actual_values, predicted_w)\n",
    "    \n",
    "    # Get predictions at w + dw\n",
    "    predicted_w_dw = get_predictions_v2(input_values, w + dw, b)\n",
    "    \n",
    "    # Compute error at w + dw\n",
    "    error_w_dw = error_function(actual_values, predicted_w_dw)\n",
    "    \n",
    "    # Approximate derivative using the limit definition above\n",
    "    # YOUR CODE HERE\n",
    "    derivative = \n",
    "    \n",
    "\n",
    "    return derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460e0e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST CASE FOR DERIVATIVE FUNCTION (get_derivative_error_wrt_weight)\n",
    "\n",
    "# Compute the derivative of the error with respect to weight at w=1\n",
    "\n",
    "derivative = get_derivative_error_wrt_weight(\n",
    "    get_mean_absolute_error, miles, km, 1, b\n",
    ")\n",
    "\n",
    "# Should output: w=1.00: -13.8000\n",
    "print(f\"Derivative of error with respect to weight at w={1:.2f}: {derivative:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36bcf4b8",
   "metadata": {},
   "source": [
    "### Mean Squared Error (MSE)\n",
    "\n",
    "While Mean Absolute Error (MAE) is easy to interpret, it is **not a smooth function** - it is non-differentiable at zero, which makes it harder to optimize using gradient based methods.\n",
    "\n",
    "Mean Squared Error (MSE) is another common way to measure error. It is defined as:\n",
    "\n",
    "$$\n",
    "\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $n$ is the number of data points,\n",
    "- $y_i$ is the actual value,\n",
    "- $\\hat{y}_i$ is the predicted value.\n",
    "\n",
    "**Why might we prefer MSE over MAE?**\n",
    "- MSE is a **smooth, differentiable function**, which makes it easier to use with gradient-based optimization methods.\n",
    "- MSE **penalizes larger errors more heavily** (because errors are squared), which can be useful if we want to avoid large mistakes.\n",
    "- The mathematical properties of MSE make it easier to analyze and work with in many machine learning algorithms.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff06aa2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete the function that calculates the Mean Squared Error (MSE) between predicted and actual values.\n",
    "\n",
    "def get_mean_squared_error(actual_values, predicted_values):\n",
    "    '''\n",
    "    actual_values: list of actual output values\n",
    "    predicted_values: list of predicted output values\n",
    "\n",
    "    Complete the function that calculates the Mean Squared Error (MSE) between predicted and actual values.\n",
    "    Return the MSE.\n",
    "    '''\n",
    "    # Initialize list to store squared errors\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    # Loop through each value and accumulate the squared error\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    # Calculate the mean squared error (MSE) by dividing the total squared error by the number of values\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    # Return the MSE\n",
    "    # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2da884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Test Cases for MSE ---\n",
    "\n",
    "# Test 1: Simple integers\n",
    "# Calculation: ((10-12)^2 + (20-18)^2 + (30-25)^2) / 3 \n",
    "# = ((-2)^2 + (2)^2 + (5)^2) / 3 = (4 + 4 + 25) / 3 = 11.0\n",
    "act1 = [10, 20, 30]\n",
    "pre1 = [12, 18, 25]\n",
    "print(f\"Test 1 (Small errors): {get_mean_squared_error(act1, pre1)}\")\n",
    "\n",
    "# Test 2: Perfect match\n",
    "# Calculation: 0^2 + 0^2 = 0\n",
    "act2 = [1.0, 2.0]\n",
    "pre2 = [1.0, 2.0]\n",
    "print(f\"Test 2 (Perfect):      {get_mean_squared_error(act2, pre2)}\")\n",
    "\n",
    "# Test 3: The \"Outlier\" penalty\n",
    "# Notice: One error is 10, the other is 0. \n",
    "# Calculation: (10^2 + 0^2) / 2 = 100 / 2 = 50.0\n",
    "act3 = [10, 0]\n",
    "pre3 = [0, 0]\n",
    "print(f\"Test 3 (One big error): {get_mean_squared_error(act3, pre3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37cac65",
   "metadata": {},
   "source": [
    "Let's run our brute force search again, but this time, we will use the MSE intead. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789fba10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Brute-force search for the best weight (conversion factor) with bias fixed to 0\n",
    "\n",
    "best_weight = None # Initialize best weight\n",
    "lowest_error = float('inf') # Initialize to a very high value\n",
    "\n",
    "weights = [] # To store weights tried\n",
    "mean_squared_errors_list = [] # To store mean squared errors for each weight\n",
    "\n",
    "w = 1.0\n",
    "b = 0.0\n",
    "step_size = 0.001\n",
    "\n",
    "while w <= 2.0:\n",
    "    predicted = get_predictions_v2(miles, w, b)\n",
    "    mean_squared_error = get_mean_squared_error(km, predicted)\n",
    "    mean_squared_errors_list.append(mean_squared_error) # for the plot\n",
    "    weights.append(w) # for the plot\n",
    "    if mean_squared_error < lowest_error:\n",
    "        lowest_error = mean_squared_error\n",
    "        best_weight = w\n",
    "    w += step_size\n",
    "\n",
    "print(f'Best weight (conversion factor): {best_weight:.4f}')\n",
    "print(f'Lowest mean squared error: {lowest_error:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c8b78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(weights,  mean_squared_errors_list)\n",
    "plt.xlabel(\"Weight\")\n",
    "plt.ylabel(\"Mean Squared Error\")\n",
    "plt.title(\"Brute-Force Search for Best Weight\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8fc285",
   "metadata": {},
   "source": [
    "**Note:**  You do **not** need to understand the code in the next cell.  \n",
    "\n",
    "Just run the cell and use the slider to see how changing the weight affects the error and the tangent line at that point on the curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78529eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from ipywidgets import interact, FloatSlider\n",
    "\n",
    "def plot_tangent(weight):\n",
    "    # Find the closest index in weights to the selected weight\n",
    "    idx = np.abs(np.array(weights) - weight).argmin()\n",
    "    x0 = weights[idx]\n",
    "    y0 = mean_squared_errors_list[idx]\n",
    "\n",
    "    # Use the derivative function to get the slope at x0\n",
    "    slope = get_derivative_error_wrt_weight(\n",
    "        get_mean_squared_error, miles, km, x0, 0.0\n",
    "    )\n",
    "\n",
    "    # Tangent line: y = slope * (x - x0) + y0, but keep it short\n",
    "    x_tangent = np.linspace(x0 - 0.05, x0 + 0.05, 20)\n",
    "    y_tangent = slope * (x_tangent - x0) + y0\n",
    "\n",
    "    plt.figure(figsize=(8,5))\n",
    "    plt.plot(weights, mean_squared_errors_list, label=\"Error Curve\")\n",
    "    plt.plot(x_tangent, y_tangent, 'r--', label=f\"Tangent at w={x0:.3f}\")\n",
    "    plt.scatter([x0], [y0], color='red', zorder=5)\n",
    "    plt.xlabel(\"Weight\")\n",
    "    plt.ylabel(\"Mean Squared Error\")\n",
    "    plt.title(\"Interactive Tangent to Error Curve\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "interact(plot_tangent, weight=FloatSlider(min=min(weights), max=max(weights) - 0.01, step=0.01, value=1, description='Weight'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39413b2b",
   "metadata": {},
   "source": [
    "#### What to Notice in the Tangent Line Graph\n",
    "\n",
    "- As you move the weight slider, notice how the red dashed tangent line changes.\n",
    "- The tangent line always touches the error curve at exactly one point (the current weight).\n",
    "\n",
    "- **Pay attention to the angle (slope) of the tangent line:**  \n",
    "  - Where the error curve is steep, the tangent line is also steep (more vertical).\n",
    "  - Where the error curve is flat (near the minimum), the tangent line is almost horizontal.\n",
    "\n",
    "- **The length of the tangent line segment (visually)** appears to change:\n",
    "  - This is because the **vertical distance** covered by the line depends on the slope.\n",
    "  - The x-range is always the same, but a steeper slope makes the line look longer up and down.\n",
    "  \n",
    "- **Try to find the minimum:**  \n",
    "  - At the lowest point of the error curve, the tangent line is flat (slope = 0).\n",
    "  - This is where the error is minimized and the model is best.\n",
    "\n",
    "> **Key idea:**  \n",
    "> The tangent line shows how the error would change if you increased or decreased the weight a little bit. The steeper the tangent, the faster the error is changing at that point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6127a4e5",
   "metadata": {},
   "source": [
    "When the new error value is `greater` than the old error value:\n",
    "\n",
    "1. What is the sign of this fraction? $$ \\frac{f(w+dw) - f(w)}{dw}$$\n",
    "\n",
    "    A. Positive\n",
    "\n",
    "    B. Negative\n",
    "\n",
    "    `YOUR ANSWER HERE`\n",
    "\n",
    "2. This means that increasing $w$, ________ the performance?\n",
    "\n",
    "    A. worsens\n",
    "\n",
    "    B. betters\n",
    "\n",
    "\n",
    "    `YOUR ANSWER HERE`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4fcd887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot showing the derivative of the error curve with respect to weight\n",
    "\n",
    "derivatives = []\n",
    "for w in weights:\n",
    "    grad = get_derivative_error_wrt_weight(get_mean_squared_error, miles, km, w, 0.0)\n",
    "    derivatives.append(grad)\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(weights, derivatives, label=\"Derivative of Error Curve\")\n",
    "plt.axhline(0, color='gray', linestyle='--', linewidth=1)\n",
    "plt.xlabel(\"Weight\")\n",
    "plt.ylabel(\"Derivative (Slope of Error Curve)\")\n",
    "plt.title(\"Derivative of Error with Respect to Weight\")\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a3db41",
   "metadata": {},
   "source": [
    "### Gradient Descent\n",
    "\n",
    "The derivative of the **error with respect to a parameter** tells us two things.\n",
    "\n",
    "#### 1. Direction (sign)\n",
    "- If the derivative is **positive**, increasing the parameter increases the error.  \n",
    "  To reduce error, we should **decrease** the parameter.\n",
    "- If the derivative is **negative**, increasing the parameter decreases the error.  \n",
    "  To reduce error, we should **increase** the parameter.\n",
    "\n",
    "**Key idea:** to reduce error, move the parameter in the **opposite direction of the derivative**.\n",
    "\n",
    "#### 2. Amount (magnitude)\n",
    "- A **large** derivative means the error is changing quickly **at the current parameter value**.\n",
    "- A **small** derivative means the error is changing slowly.\n",
    "\n",
    "The derivative tells us how steep the error surface is, but not how far we should move.\n",
    "\n",
    "Putting this together, we update the parameter by moving **opposite the derivative**, scaled by a small constant that controls how big each step is.\n",
    "\n",
    "This idea is exactly what **gradient descent** formalizes.\n",
    "\n",
    "\n",
    "#### Adding the learning rate\n",
    "\n",
    "The derivative gives us the direction we should move and how steep the error surface is locally. However, we usually do **not** want to move the full amount suggested by the derivative.\n",
    "\n",
    "If we always took the full step, updates could be too large and overshoot the minimum.\n",
    "\n",
    "To control this, we introduce a **learning rate**.\n",
    "\n",
    "The learning rate is a small positive number that **scales the update**.  \n",
    "It lets us move in the right direction, but in **small, controlled steps**.\n",
    "\n",
    "Putting this together, we update the parameter by:\n",
    "- Moving in the **opposite direction of the derivative**, and\n",
    "- Scaling that movement by the **learning rate**.\n",
    "\n",
    "This gives the update rule:\n",
    "\n",
    "$$\n",
    "\\text{new\\_parameter}\n",
    "=\n",
    "\\text{old\\_parameter}\n",
    "-\n",
    "(\\text{learning rate}) \\times (\\text{derivative})\n",
    "$$\n",
    "\n",
    "With this rule:\n",
    "- The **derivative** determines the direction of the update.\n",
    "- The **learning rate** determines how big each step is.\n",
    "\n",
    "Repeating this update over many steps is what we call **gradient descent**.\n",
    "\n",
    "You can see this process visually as moving downhill on the error curve, step by step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068e8259",
   "metadata": {},
   "source": [
    "#### Exercise: Implement Gradient Descent\n",
    "\n",
    "Fill in the missing code below to complete a simple gradient descent algorithm for finding the best weight.  \n",
    "You will:\n",
    "\n",
    "- Use the current weight to make predictions.\n",
    "- Compute the mean squared error for those predictions.\n",
    "- Calculate the derivative of the error with respect to the weight.\n",
    "- Update the weight using the gradient descent rule.\n",
    "\n",
    "**Goal:**  \n",
    "Your code should print:\n",
    "- `Best weight found by gradient descent: 1.6303`\n",
    "- `Mean squared error at best weight: 0.1823`\n",
    "\n",
    "Replace each `# YOUR CODE HERE` with the correct line of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f0e38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Descent - Complete the code below\n",
    "# It should print:\n",
    "# Best weight found by gradient descent: 1.6303\n",
    "# Mean squared error at best weight: 0.1823\n",
    "\n",
    "# Step 0: Initialize parameters\n",
    "w_gd = 10.0\n",
    "b_gd = 0.0\n",
    "\n",
    "# Set the learning rate\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Set the number of gradient descent steps\n",
    "num_steps = 1000\n",
    "\n",
    "# Lists to track how weight and error change over time\n",
    "weights_gd = []\n",
    "errors_gd = []\n",
    "\n",
    "for step in range(num_steps):\n",
    "\n",
    "    # Use the current weight to make predictions\n",
    "    predicted = \n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    # Compute the mean squared error of the predictions\n",
    "    error = \n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    # Track the current weight and error\n",
    "    weights_gd.append(w_gd)\n",
    "    errors_gd.append(error)\n",
    "\n",
    "    # Compute the derivative of the error with respect to the weight\n",
    "    grad = \n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    # Update the weight using gradient descent\n",
    "    w_gd = \n",
    "    # YOUR CODE HERE\n",
    "\n",
    "print(f\"Best weight found by gradient descent: {w_gd:.4f}\")\n",
    "print(f\"Mean squared error at best weight: {errors_gd[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264ee1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(weights_gd, errors_gd, marker='o', markersize=2, label='Gradient Descent Path')\n",
    "plt.xlabel(\"Weight\")\n",
    "plt.ylabel(\"Mean Squared Error\")\n",
    "plt.title(\"Gradient Descent Progress\")\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c2ad55",
   "metadata": {},
   "source": [
    "## Celcius and Fahrenheit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1db4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data for Fahrenheit and Celsius\n",
    "fahrenheit = [32, 50, 68, 86, 104] # x-axis values (INPUT DATA)\n",
    "celsius = [0, 10, 20, 30, 40] # y-axis values (OUTPUT DATA)\n",
    "\n",
    "\n",
    "best_weight = None\n",
    "best_bias = None\n",
    "lowest_error = float('inf')\n",
    "\n",
    "weights = []\n",
    "weight_step_size = 0.01\n",
    "\n",
    "biases = []\n",
    "bias_step_size = 0.5\n",
    "\n",
    "errors_for_plot_mae = []\n",
    "errors_for_plot_mse = []\n",
    "\n",
    "weight = 0.4\n",
    "while weight <= 0.7:\n",
    "    bias = -30.0\n",
    "    while bias <= -10.0:\n",
    "        mean_absolute_error = get_mean_absolute_error(\n",
    "            celsius,\n",
    "            get_predictions_v2(fahrenheit, weight, bias)\n",
    "        )\n",
    "        mean_squared_error = get_mean_squared_error(\n",
    "            celsius,\n",
    "            get_predictions_v2(fahrenheit, weight, bias)\n",
    "        )\n",
    "\n",
    "        weights.append(weight)\n",
    "        biases.append(bias)\n",
    "        errors_for_plot_mae.append(mean_absolute_error)\n",
    "        errors_for_plot_mse.append(mean_squared_error)\n",
    "\n",
    "\n",
    "        if mean_squared_error < lowest_error:\n",
    "            lowest_error = mean_squared_error\n",
    "            best_weight = weight\n",
    "            best_bias = bias\n",
    "\n",
    "        bias += bias_step_size\n",
    "    weight += weight_step_size\n",
    "\n",
    "print(f\"Best weight: {best_weight:.4f}\")\n",
    "print(f\"Best bias: {best_bias:.2f}\")\n",
    "print(f\"Lowest total error: {lowest_error:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d7362d",
   "metadata": {},
   "source": [
    "**Note:**  You do **not** need to understand the code in the next cell.  \n",
    "\n",
    "Just run the cell to see the plot of the error surface in 3D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72614bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "# MAE plot\n",
    "fig_mae = go.Figure(data=[go.Scatter3d(\n",
    "    x=weights,\n",
    "    y=biases,\n",
    "    z=errors_for_plot_mae,\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        size=5,\n",
    "        color=errors_for_plot_mae,\n",
    "        colorscale='Viridis',\n",
    "        colorbar=dict(title='MAE')\n",
    "    )\n",
    ")])\n",
    "fig_mae.add_trace(go.Scatter3d(\n",
    "    \n",
    "    x=[best_weight],\n",
    "    y=[best_bias],\n",
    "    z=[min(errors_for_plot_mae)],\n",
    "    mode='markers',\n",
    "    marker=dict(size=10, color='red'),\n",
    "    name='Best Parameters'\n",
    "))\n",
    "fig_mae.update_layout(\n",
    "    scene=dict(\n",
    "        xaxis_title='Weight',\n",
    "        yaxis_title='Bias',\n",
    "        zaxis_title='MAE'\n",
    "    ),\n",
    "    title='MAE Surface: Weight vs Bias',\n",
    "    \n",
    "    width=1000,  # Increase width\n",
    "    height=800   # Increase height\n",
    ")\n",
    "fig_mae.show()\n",
    "\n",
    "# MSE plot\n",
    "fig_mse = go.Figure(data=[go.Scatter3d(\n",
    "    x=weights,\n",
    "    y=biases,\n",
    "    z=errors_for_plot_mse,\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        size=5,\n",
    "        color=errors_for_plot_mse,\n",
    "        colorscale='Viridis',\n",
    "        colorbar=dict(title='MSE')\n",
    "    )\n",
    ")])\n",
    "fig_mse.add_trace(go.Scatter3d(\n",
    "    x=[best_weight],\n",
    "    y=[best_bias],\n",
    "    z=[min(errors_for_plot_mse)],\n",
    "    mode='markers',\n",
    "    marker=dict(size=10, color='red'),\n",
    "    name='Best Parameters'\n",
    "))\n",
    "fig_mse.update_layout(\n",
    "    scene=dict(\n",
    "        xaxis_title='Weight',\n",
    "        yaxis_title='Bias',\n",
    "        zaxis_title='MSE'\n",
    "    ),\n",
    "    title='MSE Surface: Weight vs Bias',\n",
    "    width=1000,  # Increase width\n",
    "    height=800   # Increase height\n",
    ")\n",
    "fig_mse.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b8f976",
   "metadata": {},
   "source": [
    "### Calculating the Gradient with Respect to Both $w$ and $b$\n",
    "\n",
    "When our model is $y = wx + b$, the error (e.g., mean squared error) depends on both $w$ (weight) and $b$ (bias). To optimize both parameters, we need to compute the partial derivatives (gradients) of the error with respect to each parameter:\n",
    "\n",
    "- $\\frac{\\partial E}{\\partial w}$: How the error changes as $w$ changes, keeping $b$ fixed.\n",
    "- $\\frac{\\partial E}{\\partial b}$: How the error changes as $b$ changes, keeping $w$ fixed.\n",
    "\n",
    "We can approximate these using the finite difference method:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial E}{\\partial w} \\approx \\frac{E(w+\\epsilon, b) - E(w, b)}{\\epsilon}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial E}{\\partial b} \\approx \\frac{E(w, b+\\epsilon) - E(w, b)}{\\epsilon}\n",
    "$$\n",
    "\n",
    "Where $E(w, b)$ is the error function, and $\\epsilon$ is a small number. These gradients can then be used to update both $w$ and $b$ in gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c524e6",
   "metadata": {},
   "source": [
    "### Exercise: Compute Gradients for Both Weight and Bias\n",
    "\n",
    "Fill in the missing code below to complete the function that approximates the partial derivatives (gradients) of the error with respect to both the weight ($w$) and the bias ($b$) using finite differences.\n",
    "\n",
    "You will:\n",
    "- Compute predictions and error at the current $(w, b)$.\n",
    "- Compute predictions and error at $(w+\\epsilon, b)$ and $(w, b+\\epsilon)$.\n",
    "- Approximate the partial derivatives with respect to $w$ and $b$.\n",
    "- Return both gradients.\n",
    "\n",
    "**Goal:**  \n",
    "Replace each `# YOUR CODE HERE` with the correct line of code so the function returns the correct gradients for $w$ and $b$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e671c2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gradients_w_b(error_function, input_values, actual_values, w, b, epsilon=1e-6):\n",
    "    \"\"\"\n",
    "    Approximates the partial derivatives of the error with respect to w and b\n",
    "    using finite differences.\n",
    "\n",
    "    Returns:\n",
    "        (grad_w, grad_b)\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Compute predictions using the current parameters (w, b)\n",
    "    predicted =\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    # Step 2: Compute the error at (w, b)\n",
    "    error =\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    # --- Partial derivative with respect to w ---\n",
    "\n",
    "    # Step 3: Compute predictions at (w + epsilon, b)\n",
    "    predicted_w =\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    # Step 4: Compute the error at (w + epsilon, b)\n",
    "    error_w =\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    # Step 5: Approximate the partial derivative with respect to w\n",
    "    grad_w =\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    # --- Partial derivative with respect to b ---\n",
    "\n",
    "    # Step 6: Compute predictions at (w, b + epsilon)\n",
    "    predicted_b =\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    # Step 7: Compute the error at (w, b + epsilon)\n",
    "    error_b =\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    # Step 8: Approximate the partial derivative with respect to b\n",
    "    grad_b =\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    return grad_w, grad_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82ea35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST CASE FOR GRADIENT FUNCTION (get_gradients_w_b)\n",
    "# Should pring 47.20527200133274 0.8000010005004832\n",
    "\n",
    "w_gradient, b_gradient = get_gradients_w_b(\n",
    "    get_mean_squared_error, fahrenheit, celsius, 0.55, -17\n",
    ")\n",
    "print(w_gradient, b_gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2eb1b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Descent for Fahrenheit to Celsius conversion using both w and b\n",
    "\n",
    "# Initialize parameters\n",
    "w_fc = 1\n",
    "b_fc = -60\n",
    "\n",
    "# Set the learning rate\n",
    "# TRY DIFFERENT LEARNING RATES TO SEE HOW IT AFFECTS CONVERGENCE\n",
    "learning_rate_fc = 0.001\n",
    "\n",
    "# Set the number of gradient descent steps\n",
    "num_steps_fc = 250000\n",
    "\n",
    "grad_weights_fc = []\n",
    "grad_biases_fc = []\n",
    "grad_errors_fc = []\n",
    "\n",
    "for step in range(num_steps_fc):\n",
    "    predicted_fc = get_predictions_v2(fahrenheit, w_fc, b_fc)\n",
    "    error_fc = get_mean_squared_error(celsius, predicted_fc)\n",
    "    grad_weights_fc.append(w_fc)\n",
    "    grad_biases_fc.append(b_fc)\n",
    "    grad_errors_fc.append(error_fc)\n",
    "    grad_w_fc, grad_b_fc = get_gradients_w_b(get_mean_squared_error, fahrenheit, celsius, w_fc, b_fc)\n",
    "    w_fc = w_fc - learning_rate_fc * grad_w_fc\n",
    "    b_fc = b_fc - learning_rate_fc * grad_b_fc\n",
    "    if step % 25000 == 0:\n",
    "        print(f\"Step {step}: w = {w_fc:.4f}, b = {b_fc:.4f}, error = {error_fc:.4f}\")\n",
    "\n",
    "print(f\"Best weight (w) found: {w_fc:.4f}\")\n",
    "print(f\"Best bias (b) found: {b_fc:.4f}\")\n",
    "print(f\"Mean squared error at best parameters: {grad_errors_fc[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203f3bb5",
   "metadata": {},
   "source": [
    "### What just happened?\n",
    "\n",
    "Earlier, we said:\n",
    "- The derivative gives the **direction**.\n",
    "- The learning rate controls **how big the step is**.\n",
    "- The derivative is only a **local** signal.\n",
    "\n",
    "In this run, the learning rate was too large.\n",
    "\n",
    "That caused each update to be much larger than the local information from the derivative could support.  \n",
    "Instead of moving downhill, the algorithm jumped past the minimum and off the error surface.\n",
    "\n",
    "Nothing about the direction was wrong.  \n",
    "The steps were simply too big.\n",
    "\n",
    "When we lowered the learning rate:\n",
    "- The direction stayed the same.\n",
    "- The steps became small enough to stay on the error surface.\n",
    "- The error started decreasing.\n",
    "\n",
    "So this behavior is exactly what we should expect when the learning rate is too large."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pyactivate)",
   "language": "python",
   "name": "pyactivate"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
