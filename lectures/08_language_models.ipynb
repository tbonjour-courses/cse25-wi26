{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CSE 25 – Introduction to Artificial Intelligence  \n",
        "## Worksheet 12: Language Models and N-grams\n",
        "\n",
        "**Today’s focus:**  \n",
        "How do we assign probabilities to words and sentences, and use those probabilities to choose better language outputs?\n",
        "\n",
        ">Language model = a model that assigns probabilities to strings of words.\n",
        "\n",
        "### Learning Objectives\n",
        "By the end of this worksheet, you will be able to:\n",
        "- Explain how language models score candidate text outputs\n",
        "- Compute unigram and bigram probabilities from frequency counts\n",
        "- Apply the chain rule to sentence probability\n",
        "- Explain the Markov assumption in n-gram language models\n",
        "- Identify sparsity/zero-probability issues and apply add-\\(\\alpha\\) smoothing\n",
        "\n",
        "**Instructions:**\n",
        "\n",
        "Create a copy of this notebook and complete it during class.  \n",
        "Work through the cells below **in order**.\n",
        "\n",
        "You may discuss with your neighbors, but make sure you understand  \n",
        "what each step is doing and why.\n",
        "\n",
        "**Submission**\n",
        "\n",
        "When finished, download the notebook as a PDF and upload it to Gradescope under  \n",
        "`In-Class – Week 8 Tuesday`.\n",
        "\n",
        "To download as a PDF on DataHub:  \n",
        "`File -> Save and Export Notebook As -> PDF`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Why do we need language models?\n",
        "\n",
        "Many NLP tasks require natural language output:\n",
        "- Machine translation\n",
        "- Speech recognition\n",
        "- Natural language generation\n",
        "- Spell checking\n",
        "\n",
        "Language models define *probability distributions* over (natural language) strings or sentences.\n",
        "\n",
        "We can use a language model to score possible output strings so that we can choose the best. \n",
        "\n",
        "If $P_{LM}(A) > P_{LM}(B)$, we prefer A."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Natural Language Generation**\n",
        "\n",
        "The sky is _____ \n",
        "\n",
        "`blue`\n",
        "\n",
        "or \n",
        "\n",
        "`cup`\n",
        "\n",
        "**Spell Check** \n",
        "\n",
        "`Their are two tests.`\n",
        "\n",
        "or\n",
        "\n",
        "`There are two tests.`\n",
        "\n",
        "**Grammar**\n",
        "\n",
        "`Everything has improve.`\n",
        "\n",
        "or \n",
        "\n",
        "`Everything has improved.`\n",
        "\n",
        "**Speech Recognition**\n",
        "\n",
        "`I will be back sooninsh.`\n",
        "\n",
        "or\n",
        "\n",
        "`I will be bassoon dish.`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Probability Basics\n",
        "#### Sampling with replacement\n",
        "<img src=\"images/bag-of-shapes.png\" width=\"500\">\n",
        "\n",
        "*Pick a random shape from the bag of shapes, then put it back in the bag.*\n",
        "\n",
        "\n",
        "P(blue) = `YOUR ANSWER HERE`\n",
        "\n",
        "P(square) = `YOUR ANSWER HERE`\n",
        "\n",
        "P(square or triangle) = `YOUR ANSWER HERE`\n",
        "\n",
        "P(blue square or a red triangle) = `YOUR ANSWER HERE`\n",
        "\n",
        "P(blue | square) = `YOUR ANSWER HERE`\n",
        "\n",
        "P(triangle | red) = `YOUR ANSWER HERE`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Drawing sequence of shapes:\n",
        "\n",
        "*Pick a random shape from the bag of shapes, then put it back in the bag.*\n",
        "\n",
        "P(red circle, yellow triangle, blue square) = `YOUR ANSWER HERE`\n",
        "\n",
        "P(red triangle, yellow circle, red triangle) = `YOUR ANSWER HERE`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Suppose now we have some text:\n",
        "\n",
        "A: `the cat sat on the mat . the cat scared the rat that was near the mat .`\n",
        "\n",
        "*Pick a random word from the sentences (bag) above, then put it back.*\n",
        "\n",
        "P(cat) = `YOUR ANSWER HERE`\n",
        "\n",
        "P(.) = `YOUR ANSWER HERE`\n",
        "\n",
        "P(the) = `YOUR ASNWER HERE`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "B: `on the near cat the cat mat scared that the sat mat . the rat was  the  .`\n",
        "\n",
        "*Pick a random word from the sentences (bag) above, then put it back.*\n",
        "\n",
        "P(cat) = `YOUR ANSWER HERE`\n",
        "\n",
        "P(.) = `YOUR ANSWER HERE`\n",
        "\n",
        "P(the) = `YOUR ASNWER HERE`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this *model* (where we just take the P(word)), $P_{model}(A) = P_{model}(B)$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Basic Probability Terminology\n",
        "\n",
        "Before building language models, we review a few key terms.\n",
        "\n",
        "- **Trial (experiment)**  \n",
        "  A single random process.  \n",
        "  Examples: picking a shape from a bag, rolling a die, predicting the next word.\n",
        "\n",
        "- **Sample space ($\\Omega$)**  \n",
        "  The set of all possible outcomes of a trial.  \n",
        "  Examples: all shapes in the bag, all numbers on a die, all words in a vocabulary.\n",
        "\n",
        "- **Event ($A \\subseteq \\Omega$)**  \n",
        "  A subset of possible outcomes.  \n",
        "  Examples: “drawing a blue shape”, “rolling an even number”, “predicting the word *the*”.\n",
        "\n",
        "- **Random variable ($X : \\Omega \\rightarrow T$)**  \n",
        "  A function that assigns a value to each outcome.  \n",
        "  Example: mapping each die roll outcome to its number, or mapping each word to its length.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### What Is a Probability Distribution?\n",
        "\n",
        "A **probability distribution** assigns a probability to every possible outcome of a random process.\n",
        "\n",
        "It must satisfy:\n",
        "\n",
        "1. Each probability is between 0 and 1.\n",
        "2. All probabilities together add up to 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Probability Axioms\n",
        "\n",
        "A function $P$ is a valid probability distribution over $\\Omega$ if:\n",
        "\n",
        "1. **Non-negativity**\n",
        "   $$\n",
        "   0 \\leq P(A) \\leq 1\n",
        "   $$\n",
        "   for every event $A$.\n",
        "\n",
        "2. **Total probability equals 1**\n",
        "   $$\n",
        "   \\sum_{\\omega \\in \\Omega} P(\\omega) = 1\n",
        "   $$\n",
        "   The probabilities of all possible outcomes must add up to 1.\n",
        "\n",
        "3. **Additivity for disjoint events**  \n",
        "   If $A$ and $B$ cannot both happen (they are disjoint), then:\n",
        "   $$\n",
        "   P(A \\cup B) = P(A) + P(B)\n",
        "   $$\n",
        "\n",
        "These rules ensure that probabilities behave consistently and can represent uncertainty about outcomes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Discrete Probability Distributions\n",
        "\n",
        "A probability distribution is **discrete** if there is a fixed (often finite) set of possible outcomes. This means we can list all possible outcomes and assign a probability to each one.\n",
        "\n",
        "##### Bernoulli Distribution\n",
        "\n",
        "A **Bernoulli distribution** models a situation with exactly **two possible outcomes**.\n",
        "\n",
        "We define one outcome as “success” with probability $p$.  \n",
        "The other outcome (“failure”) automatically has probability $1 - p$.\n",
        "\n",
        "Example: flipping a coin once.\n",
        "\n",
        "- Let “success” = getting heads.\n",
        "- $P(\\text{heads}) = p$\n",
        "- $P(\\text{tails}) = 1 - p$\n",
        "\n",
        "If the coin is fair:\n",
        "$$\n",
        "P(\\text{heads}) = 0.5, \\quad P(\\text{tails}) = 0.5\n",
        "$$\n",
        "\n",
        "Bernoulli distributions are commonly used for:\n",
        "- Yes / No decisions\n",
        "- True / False labels\n",
        "- Binary classification problems\n",
        "\n",
        "\n",
        "##### Categorical Distribution\n",
        "\n",
        "A **categorical distribution** generalizes Bernoulli to more than two outcomes.\n",
        "\n",
        "Suppose we have categories $c_1, c_2, \\dots, c_N$.\n",
        "\n",
        "Each category has a probability $p_i$:\n",
        "\n",
        "$$\n",
        "P(c_i) = p_i\n",
        "$$\n",
        "\n",
        "The probabilities must satisfy:\n",
        "\n",
        "$$\n",
        "\\sum_{i=1}^{N} p_i = 1\n",
        "$$\n",
        "\n",
        "Examples:\n",
        "\n",
        "- Rolling a six-sided die  \n",
        "  $P(1) = P(2) = \\dots = P(6) = \\frac{1}{6}$ (if fair)\n",
        "\n",
        "- Picking a shape from a bag  \n",
        "  Each shape has some probability depending on how many are in the bag.\n",
        "\n",
        "- Predicting the next word in a vocabulary of size $N$  \n",
        "  The model assigns a probability to each possible word, and all probabilities must sum to 1.\n",
        "\n",
        "Most language models define a **categorical distribution over words** at each step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Joint and Conditional Probability\n",
        "\n",
        "$$P(X | Y) = \\frac{P(X, Y)} {P(Y)}$$\n",
        "\n",
        "<img src=\"images/bag-of-shapes.png\" width=\"500\">\n",
        "\n",
        "$$P(blue|square) = \\frac{P(blue, square)}{P(square)} $$\n",
        "\n",
        "$P(blue, square)$ = `YOUR ANSWER HERE`\n",
        "\n",
        "$P(square)$ = `YOUR ANSWER HERE`\n",
        "\n",
        "$P(blue|square)$  = `YOUR ASNWER HERE`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### The Chain Rule\n",
        "\n",
        "The joint $P(X,Y)$ can also be expressed in terms of conditional probality\n",
        "\n",
        "$$P(X, Y) = P(X|Y)P(Y)$$\n",
        "\n",
        "This leads to the *chain rule* of probability: \n",
        "$$P(X₁, X₂, ..., Xₙ) =\n",
        "P(X₁)\n",
        "P(X₂ | X₁)\n",
        "...\n",
        "P(Xₙ | X₁, ..., Xₙ₋₁)$$\n",
        "\n",
        "\n",
        "#### Independence\n",
        "\n",
        "X and Y are independent if:\n",
        "\n",
        "P(X, Y) = P(X)P(Y)\n",
        "\n",
        "Then:\n",
        "\n",
        "P(X | Y) = P(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Building a Probability Model\n",
        "\n",
        "1. Define the model  \n",
        "2. Estimate parameters  \n",
        "\n",
        "Models often make independence assumptions\n",
        "to reduce the number of parameters.\n",
        "\n",
        "#### Language Model\n",
        "\n",
        "A *language model* over a vocabulary $V$ assigns probabilities to strings drawn from $V^*$”\n",
        "\n",
        "**The Vocabulary $V$**\n",
        "\n",
        "The vocabulary $V$ is the set of allowed words (or tokens).\n",
        "\n",
        "Example:\n",
        "\n",
        "$$\n",
        "V = \\{\\text{the}, \\text{cat}, \\text{sat}\\}\n",
        "$$\n",
        "\n",
        "These are the basic building blocks.\n",
        "\n",
        "\n",
        "**What Is $V^*$?**\n",
        "\n",
        "$V^*$ means:\n",
        "\n",
        "> All possible finite sequences of words from $V$.\n",
        "\n",
        "If\n",
        "\n",
        "$$\n",
        "V = \\{\\text{the}, \\text{cat}\\}\n",
        "$$\n",
        "\n",
        "then $V^*$ includes:\n",
        "\n",
        "- the  \n",
        "- cat  \n",
        "- the cat  \n",
        "- cat the  \n",
        "- the the  \n",
        "- cat cat  \n",
        "- the cat the  \n",
        "- cat the cat  \n",
        "- and so on...\n",
        "\n",
        "Even if $V$ is small, there are infinitely many possible strings because sentences can keep getting longer.\n",
        "\n",
        "\n",
        "**What Is a Language Model?**\n",
        "\n",
        "A language model assigns a probability to **every possible sentence** made from the vocabulary.\n",
        "\n",
        "Formally, it defines a function:\n",
        "\n",
        "$$\n",
        "P : V^* \\rightarrow [0,1]\n",
        "$$\n",
        "\n",
        "This means:\n",
        "\n",
        "- Every possible sentence gets a probability.\n",
        "- The probabilities over all possible sentences add up to 1.\n",
        "\n",
        "$$\n",
        "\\sum_{s \\in V^*} P(s) = 1\n",
        "$$\n",
        "\n",
        "\n",
        "**Why Is This Important?**\n",
        "\n",
        "We want to compare sentences like:\n",
        "\n",
        "- “I agree.”\n",
        "- “I completely agree.”\n",
        "- “Completely I agree.”\n",
        "\n",
        "To decide which is more likely, they must come from the **same probability distribution**.\n",
        "\n",
        "That’s why a language model defines probabilities over *all possible strings*, not just individual words."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's say we want to compute the probability of the next word given some history:\n",
        "\n",
        "$$\n",
        "P(w \\mid h)\n",
        "$$\n",
        "\n",
        "Suppose the history is:\n",
        "\n",
        "> *On summer evenings the sky looks very*\n",
        "\n",
        "and we want the probability that the next word is *orange*:\n",
        "\n",
        "$$\n",
        "P(\\text{orange} \\mid \\text{On summer evenings the sky looks very})\n",
        "$$\n",
        "\n",
        "A simple idea is to estimate this using counts from a large corpus.\n",
        "\n",
        "We count:\n",
        "\n",
        "- How often we see the full sequence  \n",
        "  *On summer evenings the sky looks very orange*\n",
        "\n",
        "- How often we see the history  \n",
        "  *On summer evenings the sky looks very*\n",
        "\n",
        "This gives the relative-frequency estimate:\n",
        "\n",
        "$$\n",
        "P(w \\mid h)\n",
        "=\n",
        "\\frac{C(h\\,w)}{C(h)}\n",
        "$$\n",
        "\n",
        "In words:  \n",
        "Out of all the times we saw the history $h$, how often was it followed by the word $w$?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now suppose we want the probability of an entire sequence of words rather than just one next word.\n",
        "\n",
        "Using the **chain rule**, we can write:\n",
        "\n",
        "$$\n",
        "P(w_1, w_2, \\dots, w_n)\n",
        "=\n",
        "P(w_1)\n",
        "P(w_2 \\mid w_1)\n",
        "P(w_3 \\mid w_1, w_2)\n",
        "\\cdots\n",
        "P(w_n \\mid w_1, \\dots, w_{n-1})\n",
        "$$\n",
        "\n",
        "<!-- More compactly,\n",
        "\n",
        "$$\n",
        "P(w_1, \\dots, w_n)\n",
        "=\n",
        "\\prod_{k=1}^{n}\n",
        "P(w_k \\mid w_{1:k-1})\n",
        "$$ -->\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Discuss\n",
        "\n",
        "Q. Why is estimating  \n",
        "$$\n",
        "P(w_n \\mid w_1, w_2, \\dots, w_{n-1})\n",
        "$$\n",
        "for long histories unrealistic in practice?\n",
        "\n",
        "`YOUR ANSWER HERE`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<!-- If we had a large enough corpus, we could compute all these counts. However, even the entire web is not large enough to give reliable counts for long histories. Language is creative. New sentences are invented all the time. Most long word sequences will appear rarely — or never — in our data. We cannot expect to see every possible long history $h$ in our training data. If a sequence never appears, our estimate becomes zero. That would mean assigning probability zero to perfectly reasonable sentences.  -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Language Modeling with N-grams\n",
        "\n",
        "An *n-gram language model* is one of the simplest kind of language model. The intuition of the *n-gram* model is that instead of computing the probability of a word given its entire history, we can approximate the history by just the last few words.\n",
        "\n",
        "An *n-gram language model* assumes that each word depends only on the last $n-1$ words:\n",
        "\n",
        "$$P_{ngram} (w_1, w_2, w_3...w_i) =  P(w_1) P(w_2|w_1) P(w_3|w_2, w_1)...P(w_i|w_{i-1}, w_{i-2} ...w_{i-(n+1)}) $$\n",
        "\n",
        "The assumption that the probability of a word depends only on the previous word is called a *Markov assumption*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Language Modeling with N-grams\n",
        "\n",
        "In the previous cell, we saw that computing\n",
        "\n",
        "$$\n",
        "P(w_n \\mid w_1, w_2, \\dots, w_{n-1})\n",
        "$$\n",
        "\n",
        "for long histories is unrealistic in practice.\n",
        "\n",
        "To make the problem tractable, we deliberately simplify the model.\n",
        "\n",
        "Instead of conditioning on the *entire* history, we approximate it using only the last few words.\n",
        "\n",
        "An **n-gram language model** assumes that each word depends only on the previous $n-1$ words:\n",
        "\n",
        "$$\n",
        "P(w_k \\mid w_1, \\dots, w_{k-1})\n",
        "\\approx\n",
        "P(w_k \\mid w_{k-n+1}, \\dots, w_{k-1})\n",
        "$$\n",
        "\n",
        "Using this assumption, the probability of a sequence becomes:\n",
        "\n",
        "$$\n",
        "P(w_1, w_2, \\dots, w_n)\n",
        "\\approx\n",
        "\\prod_{k=1}^{n}\n",
        "P(w_k \\mid w_{k-n+1}, \\dots, w_{k-1})\n",
        "$$\n",
        "\n",
        "This simplification is called a **Markov assumption**. It means that the future depends only on a limited recent past,  \n",
        "not the entire history."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Special Cases:**\n",
        "\n",
        "- **Unigram model ($n=1$):**\n",
        "  $$\n",
        "  P(w_k \\mid w_1, \\dots, w_{k-1})\n",
        "  \\approx\n",
        "  P(w_k)\n",
        "  $$\n",
        "\n",
        "- **Bigram model ($n=2$):**\n",
        "  $$\n",
        "  P(w_k \\mid w_1, \\dots, w_{k-1})\n",
        "  \\approx\n",
        "  P(w_k \\mid w_{k-1})\n",
        "  $$\n",
        "\n",
        "- **Trigram model ($n=3$):**\n",
        "  $$\n",
        "  P(w_k \\mid w_1, \\dots, w_{k-1})\n",
        "  \\approx\n",
        "  P(w_k \\mid w_{k-2}, w_{k-1})\n",
        "  $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Estimating N-gram Probabilities (Maximum Likelihood Estimation)\n",
        "\n",
        "Once we decide how much history to use (unigram, bigram, trigram, etc.),\n",
        "we need a way to compute the probabilities from data.\n",
        "\n",
        "We use **Maximum Likelihood Estimation (MLE)**.\n",
        "\n",
        "The idea of MLE is simple:\n",
        "\n",
        "> Choose the probabilities that make the observed training data most likely.\n",
        "\n",
        "For n-gram models, this leads to **relative frequency estimates**.\n",
        "\n",
        "For a **unigram model**:\n",
        "\n",
        "$$\n",
        "P(w_k)\n",
        "=\n",
        "\\frac{C(w_k)}{\\text{total number of words in the corpus}}\n",
        "$$\n",
        "\n",
        "Count how often the word appears,  \n",
        "divide by the total number of tokens.\n",
        "\n",
        "\n",
        "For a **bigram model**:\n",
        "\n",
        "$$\n",
        "P(w_k \\mid w_{k-1})\n",
        "=\n",
        "\\frac{C(w_{k-1}, w_k)}{C(w_{k-1})}\n",
        "$$\n",
        "\n",
        "Count how often the two-word sequence appears,  \n",
        "divide by how often the first word appears.\n",
        "\n",
        "\n",
        "For a **trigram model**:\n",
        "\n",
        "$$\n",
        "P(w_k \\mid w_{k-2}, w_{k-1})\n",
        "=\n",
        "\\frac{C(w_{k-2}, w_{k-1}, w_k)}\n",
        "     {C(w_{k-2}, w_{k-1})}\n",
        "$$\n",
        "\n",
        "Count how often the three-word sequence appears,  \n",
        "divide by how often the two-word history appears.\n",
        "\n",
        "\n",
        "In general, for an n-gram model:\n",
        "\n",
        "$$\n",
        "P(w_k \\mid w_{k-n+1}, \\dots, w_{k-1})\n",
        "=\n",
        "\\frac{C(w_{k-n+1}, \\dots, w_k)}\n",
        "     {C(w_{k-n+1}, \\dots, w_{k-1})}\n",
        "$$\n",
        "\n",
        "So computing n-gram probabilities always follows the same pattern:\n",
        "\n",
        "1. Count the full sequence (history + next word).  \n",
        "2. Divide by the count of the history."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Toy Exercise\n",
        "\n",
        "We will work with a **tiny toy corpus** so every computation is transparent.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Tiny corpus\n",
        "\n",
        "We will use these tokenized sentences (with boundary tokens):\n",
        "\n",
        "- `<s>` start of sentence\n",
        "- `</s>` end of sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "toy_sentences = [\n",
        "    [\"<s>\", \"to\", \"be\", \"or\", \"not\", \"to\", \"be\", \"</s>\"],\n",
        "    [\"<s>\", \"to\", \"be\", \"a\", \"king\", \"</s>\"],\n",
        "    [\"<s>\", \"to\", \"eat\", \"pizza\", \"</s>\"],\n",
        "]\n",
        "\n",
        "all_tokens = [w for sent in toy_sentences for w in sent]\n",
        "N = len(all_tokens)\n",
        "N, all_tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 1. Estimate unigram probabilities\n",
        "\n",
        "$$\n",
        "\\hat{P}(w) = \\frac{\\text{count}(w)}{\\sum_{w'} \\text{count}(w')}\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Complete the code in the next cell to compute unigram probabilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Let's estimate unigram probabilities by counting tokens and dividing by the total number of tokens (N)\n",
        "\n",
        "word_counts = {}\n",
        "for w in all_tokens:\n",
        "    if w not in word_counts:\n",
        "        word_counts[w] = None # YOUR CODE HERE\n",
        "    else:\n",
        "        word_counts[w] = None # YOUR CODE HERE\n",
        "\n",
        "unigram_probs = {}\n",
        "for w in word_counts:\n",
        "    unigram_probs[w] = None # YOUR CODE HERE\n",
        "\n",
        "print(\"Unigram probabilities:\" )\n",
        "for w in sorted(unigram_probs, key=unigram_probs.get, reverse=True):\n",
        "    print(f\"  {w}: {unigram_probs[w]:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "####  2. Bigram form (language modeling)\n",
        "The MLE bigram estimate is:\n",
        "\n",
        "$$\n",
        "\\hat{P}(w_t \\mid w_{t-1}) \\;=\\; \\frac{\\mathrm{count}(w_{t-1},\\, w_t)}{\\mathrm{count}(w_{t-1})}\n",
        "$$\n",
        "\n",
        "We divide how often `previous_word` is followed by `word` by how often `previous_word` appears as a context."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Complete the code in the next cell to calculate the bigram probabilities of the toy dataset. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Now let's estimate bigram probabilities by counting bigrams and dividing by the count of the previous word (the context)\n",
        "\n",
        "bigram_counts = {}\n",
        "previous_word_counts = {}\n",
        "\n",
        "for sent in toy_sentences:\n",
        "    for i in range(len(sent) - 1):\n",
        "        prev_word = sent[i]\n",
        "        curr_word = sent[i + 1]\n",
        "        new_key = (prev_word, curr_word)\n",
        "        # Update bigram counts\n",
        "        if (prev_word, curr_word) not in bigram_counts:\n",
        "            bigram_counts[(prev_word, curr_word)] = None # YOUR CODE HERE\n",
        "        else:\n",
        "         bigram_counts[(prev_word, curr_word)] = None # YOUR CODE HERE\n",
        "\n",
        "        if prev_word not in previous_word_counts:\n",
        "            previous_word_counts[prev_word] = None # YOUR CODE HERE\n",
        "        else:\n",
        "            previous_word_counts[prev_word] = None # YOUR CODE HERE\n",
        "\n",
        "bigram_probs = {}\n",
        "for (prev_word, curr_word) in bigram_counts:\n",
        "    bigram_probs[(prev_word, curr_word)] = None # YOUR CODE HERE\n",
        "\n",
        "\n",
        "print(\"Bigram probabilities:\")\n",
        "for (prev_word, curr_word) in sorted(bigram_probs):\n",
        "    print(f\"P({curr_word} | {prev_word}) = {bigram_probs[(prev_word, curr_word)]:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Sentence Probability\n",
        "$$\n",
        "P(\\text{sentence}) = \\prod_t P(w_t \\mid w_{t-1})\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def sentence_prob_bigram(sent):\n",
        "    prob = 1.0\n",
        "    for i in range(len(sent)-1):\n",
        "        prob = None # YOUR CODE HERE\n",
        "    return prob\n",
        "\n",
        "test_sent = [\"<s>\", \"to\", \"be\", \"</s>\"]\n",
        "sentence_prob_bigram(test_sent) # 0,25"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Notice: this can become extremely small quickly for long sentences.\n",
        "\n",
        "That is why we use **log-probability**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 3. Log-probability\n",
        "\n",
        "Instead of multiplying probabilities:\n",
        "\n",
        "$$\n",
        "P(\\text{sentence}) = \\prod_t P(w_t \\mid w_{t-1})\n",
        "$$\n",
        "\n",
        "we sum logs:\n",
        "\n",
        "$$\n",
        "\\log P(\\text{sentence})\n",
        "= \\sum_t \\log P(w_t \\mid w_{t-1})\n",
        "$$\n",
        "\n",
        "Benefits:\n",
        "- avoids underflow (numbers becoming 0 in a computer)\n",
        "- turns products into sums (easier to compute)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "def sentence_logprob_bigram(sent):\n",
        "    total = 0.0\n",
        "    for i in range(len(sent)-1):\n",
        "        p = bigram_probs.get((sent[i], sent[i+1]), 0)\n",
        "        if p == 0:\n",
        "            return float(\"-inf\")\n",
        "        total += math.log(p)\n",
        "    return total\n",
        "\n",
        "sentence_logprob_bigram(test_sent)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 4. Sparsity problem (zero probabilities)\n",
        "\n",
        "If a bigram never occurred in training:\n",
        "\n",
        "$$\n",
        "\\hat{P}(w \\mid w_{t-1}) = 0\n",
        "$$\n",
        "\n",
        "Then any sentence containing it gets probability 0 (log probability = -∞).\n",
        "\n",
        "That is too harsh for real language."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "unseen = [\"<s>\", \"to\", \"code\", \"</s>\"]\n",
        "print(\"P(code | to) =\", bigram_probs.get((\"to\", \"code\"), 0))\n",
        "print(\"log P(sentence) =\", sentence_logprob_bigram(unseen))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Laplace (add-α) smoothing\n",
        "\n",
        "We fix zeros by adding a small pseudo-count `α` to every possible next word:\n",
        "\n",
        "$$\n",
        "P_{smooth}(w \\mid c)\n",
        "=\n",
        "\\frac{\\text{count}(c,w) + \\alpha}{\\text{count}(c) + \\alpha |V|}\n",
        "$$\n",
        "\n",
        "- `|V|` = vocabulary size\n",
        "- guarantees **non-zero** probability everywhere\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def sentence_logprob_bigram_smoothed(sent, alpha=1.0):\n",
        "    total = 0.0\n",
        "    vocab_size = len(unigram_probs)\n",
        "    for i in range(len(sent)-1):\n",
        "        prev_word, curr_word = sent[i], sent[i+1]\n",
        "        count_bigram = bigram_counts.get((prev_word, curr_word), 0)\n",
        "        count_prev_word = previous_word_counts.get(prev_word, 0)\n",
        "        # Apply add-alpha smoothing\n",
        "        p = (count_bigram + alpha) / (count_prev_word + alpha * vocab_size)\n",
        "        total += math.log(p)\n",
        "    return total"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"log P(sentence) =\", sentence_logprob_bigram_smoothed(unseen, alpha=1.0))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python (pyactivate)",
      "language": "python",
      "name": "pyactivate"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
