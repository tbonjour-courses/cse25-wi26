{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91fbacd0",
   "metadata": {},
   "source": [
    "# CSE 25 – Introduction to Artificial Intelligence  \n",
    "## Worksheet 10: Hidden Layers and Nonlinearity\n",
    "\n",
    "**Context (from last class)**\n",
    "\n",
    "Previously, we learned how to compute gradients efficiently using **backpropagation** for a single-layer model (sigmoid + binary cross-entropy).\n",
    "\n",
    "We saw:\n",
    "\n",
    "- Why finite-difference gradient computation is inefficient  \n",
    "- How the chain rule enables efficient gradient computation  \n",
    "- How gradients are used to update parameters and reduce loss  \n",
    "\n",
    "However, our model was still a single linear layer.\n",
    "\n",
    "Today, we go one step further. In this worksheet, we will:\n",
    "\n",
    "- Extend the binary classification model to **multi-class classification** using softmax  \n",
    "- Understand why a single linear layer is insufficient for some problems (XOR)  \n",
    "- Introduce hidden layers and activation functions  \n",
    "- Analyze how backpropagation works in a deeper network\n",
    "- Trace gradients through a multi-layer computation graph\n",
    "\n",
    "\n",
    "**Learning Objectives**\n",
    "\n",
    "By the end of today, you should be able to:\n",
    "\n",
    "- Explain how **softmax** generalizes sigmoid to multiple classes\n",
    "- Compute multi-class cross-entropy loss using one-hot encoding\n",
    "- Explain why XOR cannot be solved by a single linear model\n",
    "- Describe how hidden layers increase model expressivity\n",
    "- Apply the chain rule to deeper computation graphs\n",
    "- Explain how gradients propagate through multiple layers\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "Create a copy of this notebook and complete it during class.  \n",
    "Work through the cells below **in order**.\n",
    "\n",
    "You may discuss with your neighbors, but make sure you understand  \n",
    "what each step is doing and why.\n",
    "\n",
    "**Submission**\n",
    "\n",
    "When finished, download the notebook as a PDF and upload it to Gradescope under  \n",
    "`In-Class – Week 6 Thursday`.\n",
    "\n",
    "To download as a PDF on DataHub:  \n",
    "`File -> Save and Export Notebook As -> PDF`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6897c4e",
   "metadata": {},
   "source": [
    "The next cell generates diagrams for the different models discussed so far. You do not need to understand or modify the code - just run the cell to visualize the model structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51ce29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphviz import Digraph\n",
    "\n",
    "def val(dot, name, label=None):\n",
    "    dot.node(name, label if label else name, shape=\"box\")\n",
    "\n",
    "def fn(dot, name, label):\n",
    "    dot.node(name, label, shape=\"circle\")\n",
    "\n",
    "def connect(dot, src, dst, weight=None):\n",
    "    dot.edge(src, dst, label=weight) if weight else dot.edge(src, dst)\n",
    "\n",
    "\n",
    "def add_neuron(dot, suffix, activation, inputs, box_neuron=False, cluster_label=\"Neuron\"):\n",
    "    \"\"\"Creates Σ -> z -> act structure.\"\"\"\n",
    "    s_id, z_id, a_id = f\"sum{suffix}\", f\"z{suffix}\", f\"act{suffix}\"\n",
    "    z_lab = f\"z{suffix}\" if suffix else \"z\"\n",
    "    \n",
    "    if box_neuron:\n",
    "        with dot.subgraph(name=f\"cluster_{suffix}\") as c:\n",
    "            c.attr(label=cluster_label, style=\"dashed\")\n",
    "            fn(c, s_id, \"Σ\")\n",
    "            val(c, z_id, z_lab)\n",
    "            fn(c, a_id, activation)\n",
    "            connect(c, s_id, z_id)\n",
    "            connect(c, z_id, a_id)\n",
    "    else:\n",
    "        fn(dot, s_id, \"Σ\")\n",
    "        val(dot, z_id, z_lab)\n",
    "        fn(dot, a_id, activation)\n",
    "        connect(dot, s_id, z_id)\n",
    "        connect(dot, z_id, a_id)\n",
    "\n",
    "    for src, weight in inputs:\n",
    "        connect(dot, src, s_id, weight)\n",
    "        \n",
    "    return a_id\n",
    "\n",
    "\n",
    "def common_graph(loss_label, activation_label, output_label, box_neuron=False):\n",
    "    dot = Digraph(graph_attr={\"rankdir\": \"LR\"})\n",
    "    \n",
    "    val(dot, \"one\", \"1\"); val(dot, \"x1\", \"x₁\"); val(dot, \"x2\", \"x₂\"); val(dot, \"y\", \"y\")\n",
    "    \n",
    "    last_node = add_neuron(dot, \"\", activation_label, \n",
    "                           [(\"one\", \"* b\"), (\"x1\", \"* w₁\"), (\"x2\", \"* w₂\")], box_neuron)\n",
    "    \n",
    "    val(dot, \"out\", output_label); fn(dot, \"loss\", loss_label); val(dot, \"L\", \"L\")\n",
    "    connect(dot, last_node, \"out\"); connect(dot, \"out\", \"loss\")\n",
    "    connect(dot, \"y\", \"loss\"); connect(dot, \"loss\", \"L\")\n",
    "    return dot\n",
    "\n",
    "# Standard Models\n",
    "def general_linear_model_graph(box_neuron=False):return common_graph(\"L(.)\",\"f(.)\",\"out\",box_neuron=box_neuron)\n",
    "def linear_regression_graph(box_neuron=False): return common_graph(\"MSE\", \"id\", \"ŷ\", box_neuron)\n",
    "def perceptron_graph(box_neuron=False): return common_graph(\"P_Loss\", \"sign\", \"ŷ\", box_neuron)\n",
    "def logistic_regression_graph(box_neuron=False): return common_graph(\"BCE\", \"σ\", \"p\", box_neuron)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ee05a4",
   "metadata": {},
   "source": [
    "We have covered three foundational linear models so far:\n",
    "\n",
    "- **Linear Regression**: Uses the identity activation function and is trained with Mean Squared Error (MSE) loss. This model is suitable for regression tasks where the output is continuous.\n",
    "\n",
    "- **Perceptron**: Uses the sign (or step) activation function and is trained using the perceptron update rule, which can be interpreted as minimizing the perceptron loss. This model is designed for binary classification with hard decision boundaries.\n",
    "\n",
    "- **Logistic Regression**: Uses the sigmoid activation function and is trained with Binary Cross-Entropy (BCE) loss. This model outputs probabilities for binary classification.\n",
    "\n",
    "  *(Note: We did not formally use this term earlier, but the sigmoid + BCE single-layer model is commonly known as Logistic Regression.)*\n",
    "\n",
    "Although these models differ in activation functions and loss functions, they all share the same underlying structure: a single linear decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c8cb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_regression_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b1a769",
   "metadata": {},
   "outputs": [],
   "source": [
    "perceptron_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b4fc64",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70abbaba",
   "metadata": {},
   "source": [
    "#### A Common Computational Structure\n",
    "\n",
    "If we look at the three models we just saw, we notice that they all share the same core structure:\n",
    "\n",
    "1. Compute a weighted sum: $ z = \\sum_{i=1}^{n} w_i x_i + b $\n",
    "\n",
    "2. Apply an activation function: $out = f(.)$\n",
    "\n",
    "3. Compute a loss: $L(.)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29892c8a",
   "metadata": {},
   "source": [
    "We can therefore represent all three models in a unified form:\n",
    "\n",
    "x → (weighted sum) → z → (activation f(.)) → out → (loss L(.))\n",
    "\n",
    "Where:\n",
    "\n",
    "- f(z) could be:\n",
    "  - identity (linear regression)\n",
    "  - sign (perceptron)\n",
    "  - sigmoid (logistic regression)\n",
    "\n",
    "- L(out, y) could be:\n",
    "  - Mean Squared Error (MSE)\n",
    "  - perceptron loss\n",
    "  - binary cross-entropy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9198f122",
   "metadata": {},
   "outputs": [],
   "source": [
    "general_linear_model_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e419ee0",
   "metadata": {},
   "source": [
    "#### The Core Computational Unit: A Neuron\n",
    "\n",
    "The combination of:\n",
    "\n",
    "- a weighted sum\n",
    "- followed by an activation function\n",
    "\n",
    "is often called a **neuron** (or perceptron in a broader sense).\n",
    "\n",
    "Visually, we can group:\n",
    "\n",
    "x → (weighted sum) → z → (activation) → a\n",
    "\n",
    "into a single computational block.\n",
    "\n",
    "\n",
    "##### Biological Inspiration\n",
    "\n",
    "The term *neuron* comes from biology.\n",
    "\n",
    "A biological neuron:\n",
    "\n",
    "- Receives signals through dendrites  \n",
    "- Aggregates those signals  \n",
    "- Generates an output signal when its internal activation exceeds a threshold  \n",
    "\n",
    "The artificial neuron abstracts this idea:\n",
    "\n",
    "- Inputs $x_i$ correspond to incoming signals  \n",
    "- Weights $w_i$ represent synaptic strength  \n",
    "- The weighted sum aggregates signals  \n",
    "- The activation function determines whether and how strongly the unit \"fires\"\n",
    "\n",
    "This abstraction dates back to the seminal 1943 paper by McCulloch and Pitts:\n",
    "W. McCulloch & W. Pitts (1943).  \n",
    "*A Logical Calculus of the Ideas Immanent in Nervous Activity.*\n",
    "\n",
    "Their model formalized the idea of weighted input aggregation followed by a threshold function, laying the foundation for modern neural networks.\n",
    "\n",
    "Modern neural networks are inspired by biology, but they are highly simplified mathematical abstractions rather than detailed models of real neurons.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf922404",
   "metadata": {},
   "outputs": [],
   "source": [
    "general_linear_model_graph(box_neuron=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39fa704",
   "metadata": {},
   "source": [
    "This boxed structure is the fundamental building block of neural networks. A neural network consists of many such units connected together.\n",
    "\n",
    "\n",
    "Before we connect multiple neurons into layered networks, we first extend our single-layer model to handle multiple classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de21d05",
   "metadata": {},
   "source": [
    "#### Multi-Class Classification\n",
    "\n",
    "In binary classification, we used the sigmoid function:\n",
    "\n",
    "$$\n",
    "p = \\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "This produces a single probability $p$, and we use binary cross-entropy loss:\n",
    "\n",
    "$$\n",
    "L = -\\big[y \\log p + (1-y)\\log(1-p)\\big]\n",
    "$$\n",
    "\n",
    "##### Softmax\n",
    "\n",
    "For multi-class classification, instead of producing one score $z$, the model produces a list of scores:\n",
    "\n",
    "$$\n",
    "z_1, z_2, \\dots, z_K\n",
    "$$\n",
    "\n",
    "The **Softmax** function generalizes the sigmoid to the multi-class setting. It takes the raw scores and converts them into a probability distribution:\n",
    "\n",
    "$$\n",
    "p_i = \\frac{e^{z_i}}{\\sum_{j=1}^{K} e^{z_j}}\n",
    "$$\n",
    "\n",
    "Softmax is used because we want the outputs to behave like probabilities across classes:\n",
    "one probability distribution over K classes (not K independent sigmoids).\n",
    "\n",
    "By dividing each exponentiated score by the sum of all exponentiated scores, Softmax ensures that:\n",
    "\n",
    "- $p_i \\ge 0$\n",
    "- $\\sum_{i=1}^{K} p_i = 1$\n",
    "- Larger $z_i$ leads to larger probability\n",
    "\n",
    "##### Cross-Entropy Loss\n",
    "\n",
    "Previously, we used **Binary Cross-Entropy (BCE)**:\n",
    "$$L = -\\big[y \\log p + (1-y)\\log(1-p)\\big]$$\n",
    "\n",
    "For multiple classes, this generalizes to **Categorical Cross-Entropy**. We sum the error across all $K$ classes:\n",
    "\n",
    "$$\n",
    "L = -\\sum_{i=1}^{K} y_i \\log p_i\n",
    "$$\n",
    "\n",
    "**One-hot encoding**\n",
    "\n",
    "If we represent our classes using *one-hot encoding*, then the true label is written as:\n",
    "\n",
    "$$y = [0, \\dots, 1, \\dots, 0]$$\n",
    "\n",
    "This is a list (or vector) where the entry for the correct class is $1$ and all other entries are $0$.\n",
    "\n",
    "**The Simplified Loss**\n",
    "\n",
    "Because $y_i = 0$ for every incorrect class, those terms are multiplied by zero and vanish from the summation. This leaves us with a simplified calculation that only cares about the probability assigned to the correct class:\n",
    "\n",
    "$$L = -\\log(p_{\\text{correct}})$$\n",
    "\n",
    "- If the model is confident in the right answer ($p \\approx 1$), the loss is near **0**.\n",
    "- If the model assigns a low probability to the right answer, the loss becomes very large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733d64a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_logistic_regression_graph(box_neuron=False):\n",
    "    dot = Digraph(graph_attr={\"rankdir\": \"LR\", \"labelloc\": \"t\", \"label\": \"Softmax Logistic Regression\"})\n",
    "    \n",
    "    # Declare inputs first\n",
    "    val(dot, \"one1\", \"1\"); val(dot, \"one2\", \"1\")\n",
    "    val(dot, \"x1\", \"x₁\"); val(dot, \"x2\", \"x₂\")\n",
    "    val(dot, \"y\", \"y\")\n",
    "\n",
    "    # Define Neuron 1 (Top)\n",
    "    n1 = add_neuron(dot, \"1\", \"id\", [(\"x1\", \"* w₁₁\"), (\"x2\", \"* w₂₁\"), (\"one1\", \"* b₁\")], \n",
    "                    box_neuron, \"Neuron (Class 1)\")\n",
    "    \n",
    "    # Define Neuron 2 (Bottom)\n",
    "    n2 = add_neuron(dot, \"2\", \"id\", [(\"x1\", \"* w₁₂\"), (\"x2\", \"* w₂₂\"), (\"one2\", \"* b₂\")], \n",
    "                    box_neuron, \"Neuron (Class 2)\")\n",
    "\n",
    "    fn(dot, \"softmax\", \"softmax\")\n",
    "    connect(dot, n1, \"softmax\"); connect(dot, n2, \"softmax\")\n",
    "    \n",
    "    val(dot, \"p1\", \"p₁\"); val(dot, \"p2\", \"p₂\")\n",
    "    connect(dot, \"softmax\", \"p1\"); connect(dot, \"softmax\", \"p2\")\n",
    "    \n",
    "    fn(dot, \"loss\", \"CCE\")\n",
    "    connect(dot, \"p1\", \"loss\"); connect(dot, \"p2\", \"loss\"); connect(dot, \"y\", \"loss\")\n",
    "    val(dot, \"L\", \"L\"); connect(dot, \"loss\", \"L\")\n",
    "    \n",
    "    return dot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b56ac8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax_logistic_regression_graph(box_neuron=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64c3256",
   "metadata": {},
   "source": [
    "**NOTE: This is still a single layer model**\n",
    "\n",
    "Even though the diagram shows **two neurons** (one per class) and a softmax step, this is still a **single-layer model**.\n",
    "\n",
    "- Each class score is just a **weighted sum of the inputs**:\n",
    "\n",
    "$$\n",
    "z_1 = w_{11}x_1 + w_{21}x_2 + b_1, \\qquad\n",
    "z_2 = w_{12}x_1 + w_{22}x_2 + b_2\n",
    "$$\n",
    "\n",
    "- There are **no hidden layers**.\n",
    "- Softmax only converts scores into probabilities. It does **not** add depth or extra learning capacity.\n",
    "\n",
    "Because of this, the model can only learn **linear decision boundaries**.  It cannot represent curved or complex boundaries (for example, XOR)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57dd7d7",
   "metadata": {},
   "source": [
    "#### The XOR Problem: A Challenge for Single-Layer Linear Models\n",
    "\n",
    "The XOR (exclusive OR) function outputs 1 only when its two binary inputs differ.\n",
    "\n",
    "| x1 | x2 | XOR |\n",
    "|----|----|-----|\n",
    "| 0  | 0  |  0  |\n",
    "| 0  | 1  |  1  |\n",
    "| 1  | 0  |  1  |\n",
    "| 1  | 1  |  0  |\n",
    "\n",
    "If we plot the four possible input combinations, we will see that no single straight line can separate the points where XOR is 1 from those where it is 0.\n",
    "\n",
    "In the next cell: try changing the perceptron parameters (`w1`, `w2`, and `b`). Can you find any set of parameters that perfectly separates the blue and red points?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3917dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact, FloatSlider, Dropdown, VBox, HBox\n",
    "\n",
    "# XOR dataset\n",
    "X_xor = [\n",
    "    [0, 0],\n",
    "    [0, 1],\n",
    "    [1, 0],\n",
    "    [1, 1]\n",
    "]\n",
    "y_xor = [-1, 1, 1, -1]  # XOR labels for perceptron\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def plot_xor_model(w1=0.0, w2=0.0, b=0.0, activation=\"sign\"):\n",
    "    \n",
    "    w = [w1, w2]\n",
    "\n",
    "    # Separate points by class (based on original XOR labels)\n",
    "    X_pos = [x for x, y in zip(X_xor, y_xor) if y == 1]\n",
    "    X_neg = [x for x, y in zip(X_xor, y_xor) if y == -1]\n",
    "\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    plt.scatter([x[0] for x in X_pos], [x[1] for x in X_pos],\n",
    "                marker='x', s=100, label='Class +1')\n",
    "    plt.scatter([x[0] for x in X_neg], [x[1] for x in X_neg],\n",
    "                marker='o', s=100, label='Class -1')\n",
    "\n",
    "    # Decision boundary: w1*x1 + w2*x2 + b = 0\n",
    "    x1_vals = np.linspace(-0.5, 1.5, 100)\n",
    "\n",
    "    if w1 == 0 and w2 == 0:\n",
    "        plt.text(0.5, 0.5, \"No decision boundary\\n(w1=0, w2=0)\",\n",
    "                 fontsize=12, ha='center', va='center',\n",
    "                 transform=plt.gca().transAxes)\n",
    "    elif w2 != 0:\n",
    "        x2_vals = [-(w1/w2)*x1 - b/w2 for x1 in x1_vals]\n",
    "        plt.plot(x1_vals, x2_vals, linestyle='--', label='Decision boundary')\n",
    "    elif w1 != 0:\n",
    "        plt.axvline(x=-b/w1, linestyle='--', label='Decision boundary')\n",
    "\n",
    "    # Predictions\n",
    "    for x, y_true in zip(X_xor, y_xor):\n",
    "        score = w[0] * x[0] + w[1] * x[1] + b\n",
    "\n",
    "        if activation == \"sign\":\n",
    "            y_pred = 1 if score >= 0 else -1\n",
    "            label_text = f\"Pred: {y_pred}\"\n",
    "\n",
    "        elif activation == \"sigmoid\":\n",
    "            p = sigmoid(score)\n",
    "            y_pred = 1 if p >= 0.5 else -1\n",
    "            label_text = f\"p={p:.2f}\"\n",
    "\n",
    "        plt.text(x[0] + 0.05, x[1] + 0.05,\n",
    "                 label_text, fontsize=9)\n",
    "\n",
    "    plt.xlabel(\"x1\")\n",
    "    plt.ylabel(\"x2\")\n",
    "    plt.title(f\"XOR with {activation} activation\")\n",
    "    plt.xlim(-0.5, 1.5)\n",
    "    plt.ylim(-0.5, 1.5)\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "interact(\n",
    "    plot_xor_model,\n",
    "    w1=FloatSlider(value=1, min=-5, max=5, step=0.1, description='w1'),\n",
    "    w2=FloatSlider(value=1, min=-5, max=5, step=0.1, description='w2'),\n",
    "    b=FloatSlider(value=0, min=-5, max=5, step=0.1, description='b'),\n",
    "    activation=Dropdown(\n",
    "        options=[\"sign\", \"sigmoid\"],\n",
    "        value=\"sign\",\n",
    "        description=\"Activation\"\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"Try switching between sign and sigmoid to compare perceptron vs logistic regression behavior.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ebe2e75",
   "metadata": {},
   "source": [
    "#### Multi-Layer Perceptron (MLP) or Feed Forward Neural Network\n",
    "\n",
    "A **hidden layer** is a layer of neurons whose outputs are not the final prediction.\n",
    "Hidden layers sit between the inputs and the output.\n",
    "\n",
    "- The input layer contains the input values \\(x_1, x_2, \\dots, x_n\\).\n",
    "- A hidden layer produces intermediate outputs.\n",
    "- The output layer produces the final prediction.\n",
    "\n",
    "> *NOTE: The term **Multi-Layer Perceptron (MLP)** is historical. The original perceptron (Rosenblatt, 1958) was a single-layer model with a threshold (step) activation function. Earlier work by McCulloch and Pitts (1943) introduced a similar thresholded neuron model without a learning rule. Modern MLPs stack multiple layers and typically use differentiable activation functions such as sigmoid, tanh, or ReLU.*\n",
    "\n",
    "##### What changes when we add a hidden layer?\n",
    "\n",
    "We introduce new intermediate outputs computed from the inputs.\n",
    "\n",
    "Instead of:\n",
    "\n",
    "x → (weighted sum) → output\n",
    "\n",
    "We now have:\n",
    "\n",
    "x → (weighted sum) → activation → (weighted sum of those intermediate outputs) → output\n",
    "\n",
    "The model is now a composition of functions. The output depends on the weights through these intermediate values.\n",
    "\n",
    "To compute gradients efficiently in this composed structure, we apply backpropagation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0614f8",
   "metadata": {},
   "source": [
    "#### Solving XOR with a Small Neural Network (2–2–1)\n",
    "\n",
    "XOR cannot be solved with a single linear decision boundary.\n",
    "\n",
    "To represent XOR, we need a model with a **hidden layer** that creates intermediate outputs, and an output neuron that combines them.\n",
    "\n",
    "A common minimal example is a **2–2–1** feedforward neural network:\n",
    "\n",
    "- **2 inputs**: \\(x_1, x_2\\)\n",
    "- **2 hidden neurons**\n",
    "- **1 output neuron**\n",
    "\n",
    "The key idea is that the hidden layer can create an intermediate representation that makes the classes separable for the final output neuron.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6580df4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xor_mlp_graph(loss_label=\"BCE\", box_neuron=False, activation_label=\"f(.)\"):\n",
    "    dot = Digraph(graph_attr={\"rankdir\": \"LR\"})\n",
    "\n",
    "    val(dot, \"one1\", \"1\"); val(dot, \"one2\", \"1\"); val(dot, \"one3\", \"1\")\n",
    "    val(dot, \"x1\", \"x₁\"); val(dot, \"x2\", \"x₂\"); val(dot, \"y\", \"y\")\n",
    "\n",
    "    # Hidden Layer (with 2 Neurons)\n",
    "    # Hidden Neuron 1 -> h1\n",
    "    h1_act = add_neuron(dot, \"1\", activation_label, \n",
    "                        [(\"x1\", \"* w₁₁\"), (\"x2\", \"* w₁₂\"), (\"one1\", \"* b₁\")], \n",
    "                        box_neuron, \"Hidden Neuron 1\")\n",
    "    val(dot, \"h1\", \"h₁\")\n",
    "    connect(dot, h1_act, \"h1\")\n",
    "\n",
    "    # Hidden Neuron 2 -> h2\n",
    "    h2_act = add_neuron(dot, \"2\", activation_label, \n",
    "                        [(\"x1\", \"* w₂₁\"), (\"x2\", \"* w₂₂\"), (\"one2\", \"* b₂\")], \n",
    "                        box_neuron, \"Hidden Neuron 2\")\n",
    "    val(dot, \"h2\", \"h₂\")\n",
    "    connect(dot, h2_act, \"h2\")\n",
    "\n",
    "    # Output Layer (with 1 Neuron)\n",
    "    # Output Neuron -> p\n",
    "    p_act = add_neuron(dot, \"3\", activation_label, \n",
    "                       [(\"h1\", \"* v₁\"), (\"h2\", \"* v₂\"), (\"one3\", \"* b₃\")], \n",
    "                       box_neuron, \"Output Neuron\")\n",
    "    val(dot, \"p\", \"p\")\n",
    "    connect(dot, p_act, \"p\")\n",
    "\n",
    "    # Loss & Final Scalar\n",
    "    fn(dot, \"loss\", loss_label)\n",
    "    val(dot, \"L\", \"L\")\n",
    "    \n",
    "    connect(dot, \"p\", \"loss\")\n",
    "    connect(dot, \"y\", \"loss\")\n",
    "    connect(dot, \"loss\", \"L\")\n",
    "\n",
    "    return dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175582f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "xor_mlp_graph(loss_label=\"L(.)\", box_neuron=True, activation_label=\"f(.)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f3b331",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "# XOR dataset\n",
    "X = np.array([[0,0],[0,1],[1,0],[1,1]], dtype=float)\n",
    "y = np.array([0,1,1,0], dtype=int)\n",
    "\n",
    "def step(z):\n",
    "    return (z >= 0).astype(float)\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def activation_fn(z, activation):\n",
    "    if activation == \"sign\":\n",
    "        return step(z)\n",
    "    elif activation == \"sigmoid\":\n",
    "        return sigmoid(z)\n",
    "\n",
    "def plot_two_hidden(w11=1.0, w12=1.0, b1=-0.5,\n",
    "                    w21=1.0, w22=1.0, b2=-1.5,\n",
    "                    activation=\"sign\"):\n",
    "\n",
    "    threshold = 0.5\n",
    "\n",
    "    plt.figure(figsize=(6,5))\n",
    "\n",
    "    # Grid\n",
    "    x1_vals = np.linspace(-0.5, 1.5, 300)\n",
    "    x2_vals = np.linspace(-0.5, 1.5, 300)\n",
    "    X1, X2 = np.meshgrid(x1_vals, x2_vals)\n",
    "\n",
    "    Z1 = w11*X1 + w12*X2 + b1\n",
    "    Z2 = w21*X1 + w22*X2 + b2\n",
    "\n",
    "    A1 = activation_fn(Z1, activation)\n",
    "    A2 = activation_fn(Z2, activation)\n",
    "\n",
    "    H1 = (A1 >= threshold).astype(int)\n",
    "    H2 = (A2 >= threshold).astype(int)\n",
    "\n",
    "    region_code = H1*2 + H2\n",
    "\n",
    "    plt.contourf(\n",
    "        X1, X2, region_code,\n",
    "        levels=[-0.1, 0.5, 1.5, 2.5, 3.5],\n",
    "        alpha=0.15\n",
    "    )\n",
    "\n",
    "    # XOR points\n",
    "    plt.scatter(X[y==1,0], X[y==1,1], s=120, marker=\"x\", label=\"Class 1\")\n",
    "    plt.scatter(X[y==0,0], X[y==0,1], s=120, marker=\"o\", label=\"Class 0\")\n",
    "\n",
    "    xs = np.linspace(-0.5, 1.5, 200)\n",
    "\n",
    "    # Hidden 1 boundary\n",
    "    if abs(w12) > 1e-9:\n",
    "        plt.plot(xs, -(w11/w12)*xs - b1/w12, \"--\", linewidth=2, color=\"black\", label=\"Hidden 1 (z1=0)\")\n",
    "    elif abs(w11) > 1e-9:\n",
    "        plt.axvline(x=-b1/w11, linestyle=\"--\", linewidth=2, color=\"black\", label=\"Hidden 1 (z1=0)\")\n",
    "\n",
    "    # Hidden 2 boundary\n",
    "    if abs(w22) > 1e-9:\n",
    "        plt.plot(xs, -(w21/w22)*xs - b2/w22, \"--\", linewidth=2, color=\"gray\", label=\"Hidden 2 (z2=0)\")\n",
    "    elif abs(w21) > 1e-9:\n",
    "        plt.axvline(x=-b2/w21, linestyle=\"--\", linewidth=2, color=\"gray\", label=\"Hidden 2 (z2=0)\")\n",
    "\n",
    "    # Show activations at XOR points\n",
    "    for (x1, x2) in X:\n",
    "        z1 = w11*x1 + w12*x2 + b1\n",
    "        z2 = w21*x1 + w22*x2 + b2\n",
    "        a1 = float(activation_fn(z1, activation))\n",
    "        a2 = float(activation_fn(z2, activation))\n",
    "        h1 = int(a1 >= threshold)\n",
    "        h2 = int(a2 >= threshold)\n",
    "\n",
    "        if activation == \"sign\":\n",
    "            txt = f\"(h1,h2)=({h1},{h2})\"\n",
    "        else:\n",
    "            txt = f\"p=({a1:.2f},{a2:.2f})\"\n",
    "\n",
    "        plt.text(x1-0.22, x2-0.22, txt, fontsize=8)\n",
    "\n",
    "    plt.xlim(-0.5, 1.5)\n",
    "    plt.ylim(-0.5, 1.5)\n",
    "    plt.xlabel(\"x1\")\n",
    "    plt.ylabel(\"x2\")\n",
    "    plt.title(f\"Two Hidden Neurons ({activation})\")\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "    plt.grid(True, alpha=0.25)\n",
    "    plt.show()\n",
    "\n",
    "# Sliders\n",
    "w11_slider = widgets.FloatSlider(value=0.5, min=-3, max=3, step=0.25, description=\"w11\", continuous_update=False)\n",
    "w12_slider = widgets.FloatSlider(value=1.0, min=-3, max=3, step=0.25, description=\"w12\", continuous_update=False)\n",
    "b1_slider  = widgets.FloatSlider(value=-0.5, min=-3, max=3, step=0.25, description=\"b1\",  continuous_update=False)\n",
    "\n",
    "w21_slider = widgets.FloatSlider(value=1.5, min=-3, max=3, step=0.25, description=\"w21\", continuous_update=False)\n",
    "w22_slider = widgets.FloatSlider(value=2.0, min=-3, max=3, step=0.25, description=\"w22\", continuous_update=False)\n",
    "b2_slider  = widgets.FloatSlider(value=-1.5, min=-3, max=3, step=0.25, description=\"b2\",  continuous_update=False)\n",
    "\n",
    "activation_dd = widgets.Dropdown(\n",
    "    options=[(\"sign (step)\", \"sign\"), (\"sigmoid\", \"sigmoid\")],\n",
    "    value=\"sign\",\n",
    "    description=\"activation\",\n",
    ")\n",
    "\n",
    "out = widgets.interactive_output(\n",
    "    plot_two_hidden,\n",
    "    {\n",
    "        \"w11\": w11_slider, \"w12\": w12_slider, \"b1\": b1_slider,\n",
    "        \"w21\": w21_slider, \"w22\": w22_slider, \"b2\": b2_slider,\n",
    "        \"activation\": activation_dd,\n",
    "    }\n",
    ")\n",
    "\n",
    "display(activation_dd,\n",
    "        widgets.HBox([\n",
    "            widgets.VBox([widgets.HTML(\"<b>Hidden Neuron 1</b>\"),\n",
    "                          w11_slider, w12_slider, b1_slider]),\n",
    "            widgets.VBox([widgets.HTML(\"<b>Hidden Neuron 2</b>\"),\n",
    "                          w21_slider, w22_slider, b2_slider])\n",
    "        ]),\n",
    "        out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf005d9",
   "metadata": {},
   "source": [
    "**Note:** This visualization above shows only the **two hidden neurons** and how they partition the input space. In a full 2–2–1 network, an additional **output neuron** would take $(h_1, h_2)$ as inputs to produce the final XOR prediction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95bef9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "xor_mlp_graph(loss_label=\"L(.)\", box_neuron=True, activation_label=\"f(.)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b513945c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XOR dataset\n",
    "X = np.array([[0,0],[0,1],[1,0],[1,1]], dtype=float)\n",
    "y = np.array([0,1,1,0], dtype=int)\n",
    "\n",
    "def step(z):\n",
    "    return (np.asarray(z) >= 0).astype(float)\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def act(z, activation):\n",
    "    if activation == \"sign\":\n",
    "        return step(z)      # {0,1}\n",
    "    elif activation == \"sigmoid\":\n",
    "        return sigmoid(z)   # (0,1)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown activation\")\n",
    "\n",
    "def plot_three_neurons(w11=1.0, w12=1.0, b1=-0.5,\n",
    "                       w21=1.0, w22=1.0, b2=-1.5,\n",
    "                       v1=1.0, v2=-2.0, b3=-0.5,\n",
    "                       hidden_activation=\"sign\",\n",
    "                       output_activation=\"sign\"):\n",
    "\n",
    "    plt.figure(figsize=(6,5))\n",
    "\n",
    "    # Grid\n",
    "    x1_vals = np.linspace(-0.5, 1.5, 300)\n",
    "    x2_vals = np.linspace(-0.5, 1.5, 300)\n",
    "    X1, X2 = np.meshgrid(x1_vals, x2_vals)\n",
    "\n",
    "    # Hidden pre-activations\n",
    "    Z1 = w11*X1 + w12*X2 + b1\n",
    "    Z2 = w21*X1 + w22*X2 + b2\n",
    "\n",
    "    # Hidden activations\n",
    "    A1 = act(Z1, hidden_activation)\n",
    "    A2 = act(Z2, hidden_activation)\n",
    "\n",
    "    # Output pre-activation and activation\n",
    "    Z3 = v1*A1 + v2*A2 + b3\n",
    "    Yout = act(Z3, output_activation)\n",
    "\n",
    "    # Decision: default threshold=0.5 for both\n",
    "    Ypred = (Yout >= 0.5).astype(int)\n",
    "\n",
    "    # Background regions\n",
    "    plt.contourf(\n",
    "        X1, X2, Ypred,\n",
    "        levels=[-0.1, 0.5, 1.1],\n",
    "        alpha=0.25,\n",
    "        colors=[\"red\", \"blue\"]\n",
    "    )\n",
    "\n",
    "    # XOR points\n",
    "    plt.scatter(X[y==1,0], X[y==1,1], s=120, marker=\"x\", label=\"Class 1\")\n",
    "    plt.scatter(X[y==0,0], X[y==0,1], s=120, marker=\"o\", label=\"Class 0\")\n",
    "\n",
    "    xs = np.linspace(-0.5, 1.5, 200)\n",
    "\n",
    "    # Hidden boundaries (z=0 lines)\n",
    "    if abs(w12) > 1e-9:\n",
    "        plt.plot(xs, -(w11/w12)*xs - b1/w12, \"--\", linewidth=2, color=\"black\", label=\"Hidden 1 (z1=0)\")\n",
    "    elif abs(w11) > 1e-9:\n",
    "        plt.axvline(x=-b1/w11, linestyle=\"--\", linewidth=2, color=\"black\", label=\"Hidden 1 (z1=0)\")\n",
    "\n",
    "    if abs(w22) > 1e-9:\n",
    "        plt.plot(xs, -(w21/w22)*xs - b2/w22, \"--\", linewidth=2, color=\"gray\", label=\"Hidden 2 (z2=0)\")\n",
    "    elif abs(w21) > 1e-9:\n",
    "        plt.axvline(x=-b2/w21, linestyle=\"--\", linewidth=2, color=\"gray\", label=\"Hidden 2 (z2=0)\")\n",
    "\n",
    "    # Annotate XOR points\n",
    "    for (x1, x2), yi in zip(X, y):\n",
    "        z1 = w11*x1 + w12*x2 + b1\n",
    "        z2 = w21*x1 + w22*x2 + b2\n",
    "        a1 = float(act(z1, hidden_activation))\n",
    "        a2 = float(act(z2, hidden_activation))\n",
    "\n",
    "        z3 = v1*a1 + v2*a2 + b3\n",
    "        yout = float(act(z3, output_activation))\n",
    "        ypred = int(yout >= 0.5)\n",
    "\n",
    "        if hidden_activation == \"sign\":\n",
    "            htxt = f\"h=({int(a1)},{int(a2)})\"\n",
    "        else:\n",
    "            htxt = f\"a=({a1:.2f},{a2:.2f})\"\n",
    "\n",
    "        if output_activation == \"sign\":\n",
    "            otxt = f\"out={int(yout)}, pred={ypred}\"\n",
    "        else:\n",
    "            otxt = f\"p={yout:.2f}, pred={ypred}\"\n",
    "\n",
    "        plt.text(x1+0.04, x2-0.18, htxt, fontsize=9)\n",
    "        plt.text(x1+0.04, x2-0.32, otxt, fontsize=9)\n",
    "\n",
    "    plt.xlim(-0.5, 1.5)\n",
    "    plt.ylim(-0.5, 1.5)\n",
    "    plt.xlabel(\"x1\")\n",
    "    plt.ylabel(\"x2\")\n",
    "    plt.title(f\"Hidden: {hidden_activation} | Output: {output_activation}\")\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "    plt.grid(True, alpha=0.25)\n",
    "    plt.show()\n",
    "\n",
    "# Dropdowns\n",
    "act_options = [(\"sign (step)\", \"sign\"), (\"sigmoid\", \"sigmoid\")]\n",
    "\n",
    "hidden_act_dd = Dropdown(options=act_options, value=\"sign\", description=\"hidden act\")\n",
    "output_act_dd = widgets.Dropdown(options=act_options, value=\"sign\", description=\"output act\")\n",
    "\n",
    "# Sliders\n",
    "w11_slider = FloatSlider(value=0.5, min=-3, max=3, step=0.25, description=\"w11\", continuous_update=False)\n",
    "w12_slider = FloatSlider(value=1.0, min=-3, max=3, step=0.25, description=\"w12\", continuous_update=False)\n",
    "b1_slider  = FloatSlider(value=-0.5, min=-3, max=3, step=0.25, description=\"b1\",  continuous_update=False)\n",
    "\n",
    "w21_slider = FloatSlider(value=1.5, min=-3, max=3, step=0.25, description=\"w21\", continuous_update=False)\n",
    "w22_slider = FloatSlider(value=2.0, min=-3, max=3, step=0.25, description=\"w22\", continuous_update=False)\n",
    "b2_slider  = FloatSlider(value=-1.5, min=-3, max=3, step=0.25, description=\"b2\",  continuous_update=False)\n",
    "\n",
    "v1_slider  = FloatSlider(value=-1.0, min=-3, max=3, step=0.25, description=\"v1\", continuous_update=False)\n",
    "v2_slider  = FloatSlider(value=2.0, min=-3, max=3, step=0.25, description=\"v2\", continuous_update=False)\n",
    "b3_slider  = FloatSlider(value=0.5, min=-3, max=3, step=0.25, description=\"b3\", continuous_update=False)\n",
    "\n",
    "hidden1_box = widgets.VBox([widgets.HTML(\"<b>Hidden Neuron 1</b>\"), w11_slider, w12_slider, b1_slider])\n",
    "hidden2_box = widgets.VBox([widgets.HTML(\"<b>Hidden Neuron 2</b>\"), w21_slider, w22_slider, b2_slider])\n",
    "output_box  = widgets.VBox([widgets.HTML(\"<b>Output Neuron</b>\"), v1_slider, v2_slider, b3_slider])\n",
    "\n",
    "out = widgets.interactive_output(\n",
    "    plot_three_neurons,\n",
    "    {\n",
    "        \"w11\": w11_slider, \"w12\": w12_slider, \"b1\": b1_slider,\n",
    "        \"w21\": w21_slider, \"w22\": w22_slider, \"b2\": b2_slider,\n",
    "        \"v1\": v1_slider,  \"v2\": v2_slider,  \"b3\": b3_slider,\n",
    "        \"hidden_activation\": hidden_act_dd,\n",
    "        \"output_activation\": output_act_dd,\n",
    "    }\n",
    ")\n",
    "\n",
    "display(widgets.HBox([hidden_act_dd, output_act_dd]),\n",
    "        widgets.HBox([hidden1_box, hidden2_box, output_box]),\n",
    "        out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3c9074",
   "metadata": {},
   "source": [
    "## Common Activation Functions\n",
    "\n",
    "In neural networks, the activation function introduces nonlinearity.\n",
    "Without it, stacking layers would still produce a linear model.\n",
    "\n",
    "Below are three commonly used activation functions:\n",
    "\n",
    "**Sigmoid**\n",
    "$$\n",
    "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "- Output range: (0, 1)\n",
    "- Often used for binary classification\n",
    "\n",
    "**Tanh**\n",
    "\n",
    "$$\n",
    "\\tanh(z) = \\frac{e^{z} - e^{-z}}{e^{-z} + e^{z}}\n",
    "$$\n",
    "\n",
    "- Output range: (-1, 1)\n",
    "- Zero-centered\n",
    "\n",
    "**ReLU (Rectified Linear Unit)**\n",
    "$$\n",
    "\\text{ReLU}(z) = \\max(0, z)\n",
    "$$\n",
    "- Commonly used in hidden layers\n",
    "- Computationally simple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60416801",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Activation functions\n",
    "def sigmoid_plot(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def tanh_plot(z):\n",
    "    return np.tanh(z)\n",
    "\n",
    "def relu_plot(z):\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "def leaky_relu_plot(z, alpha=0.1):\n",
    "    return np.where(z >= 0, z, alpha * z)\n",
    "\n",
    "z = np.linspace(-5, 5, 400)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 8))\n",
    "\n",
    "# Sigmoid\n",
    "axes[0, 0].plot(z, sigmoid_plot(z))\n",
    "axes[0, 0].set_title(\"Sigmoid\\nσ(z) = 1 / (1 + e^{-z})\")\n",
    "axes[0, 0].set_xlabel(\"z\")\n",
    "axes[0, 0].set_ylabel(\"σ(z)\")\n",
    "axes[0, 0].axhline(0,color='black', linewidth=1)\n",
    "axes[0, 0].axvline(0,color='black', linewidth=1)\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Tanh\n",
    "axes[0, 1].plot(z, tanh_plot(z))\n",
    "axes[0, 1].set_title(\"Tanh\\n tanh(z) = (e^z - e^{-z}) / (e^z + e^{-z})\")\n",
    "axes[0, 1].set_xlabel(\"z\")\n",
    "axes[0, 1].set_ylabel(\"tanh(z)\")\n",
    "axes[0, 1].axhline(0,color='black', linewidth=1)\n",
    "axes[0, 1].axvline(0,color='black', linewidth=1)\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "# ReLU\n",
    "axes[1, 0].plot(z, relu_plot(z))\n",
    "axes[1, 0].set_title(\"ReLU\\nReLU(z) = max(0, z)\")\n",
    "axes[1, 0].set_xlabel(\"z\")\n",
    "axes[1, 0].set_ylabel(\"ReLU(z)\")\n",
    "axes[1, 0].axhline(0,color='black', linewidth=1)\n",
    "axes[1, 0].axvline(0,color='black', linewidth=1)\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Leaky ReLU\n",
    "axes[1, 1].plot(z, leaky_relu_plot(z))\n",
    "axes[1, 1].set_title(\"Leaky ReLU\\nLeakyReLU(z) = max(0.1z, z)\")\n",
    "axes[1, 1].set_xlabel(\"z\")\n",
    "axes[1, 1].set_ylabel(\"Leaky ReLU(z)\")\n",
    "axes[1, 1].axhline(0,color='black', linewidth=1)\n",
    "axes[1, 1].axvline(0,color='black', linewidth=1)\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c6cf86",
   "metadata": {},
   "source": [
    "In practice:\n",
    "\n",
    "- Sigmoid is typically used in the output layer for binary classification.\n",
    "- Softmax is used in the output layer for multi-class classification.\n",
    "- ReLU (or variants such as Leaky ReLU) is most commonly used in hidden layers.\n",
    "\n",
    "In PA2, you will implement ReLU inside a multi-layer perceptron."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c036471b",
   "metadata": {},
   "source": [
    "#### Autodiff\n",
    "\n",
    "As we add hidden layers, the number of parameters grows quickly.\n",
    "\n",
    "Each weight now influences the final output through multiple intermediate computations.\n",
    "Computing all derivatives by hand becomes impractical.\n",
    "\n",
    "In the single-layer case, we derived gradients explicitly.\n",
    "For example, we computed derivatives such as:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial w_i}\n",
    "$$\n",
    "\n",
    "In multi-layer networks, the output depends on parameters through\n",
    "many intermediate variables:\n",
    "\n",
    "$$\n",
    "x \\rightarrow z_1 \\rightarrow a_1 \\rightarrow z_2 \\rightarrow \\hat{y} \\rightarrow L\n",
    "$$\n",
    "\n",
    "Keeping track of all these dependencies by hand quickly becomes tedious.\n",
    "\n",
    "To handle this systematically, we use **backpropagation** implemented via\n",
    "**automatic differentiation (autodiff)**.\n",
    "\n",
    "Most modern deep learning libraries compute these gradients automatically.\n",
    "\n",
    "\n",
    "\n",
    "**Important note:**\n",
    "\n",
    "- During training, we apply backpropagation starting from the loss $L$ and set $L.\\text{grad} = 1$.\n",
    "- In this demo, we instead start from the prediction $p$ and set $p.\\text{grad} = 1$ to illustrate how gradients flow through the computation graph.\n",
    "\n",
    "In other words, we are computing: $\\frac{\\partial p}{\\partial (\\text{earlier values})}$\n",
    "\n",
    "The same backpropagation mechanics apply when the final node is $L$ instead of $p$. Only the starting node changes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4d34a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from graphviz import Digraph\n",
    "\n",
    "# Helper functions to build and visualize the computational graph for autograd\n",
    "def trace(root):\n",
    "    nodes, edges = set(), set()\n",
    "    def build(v):\n",
    "        if id(v) in nodes:\n",
    "            return\n",
    "        nodes.add(id(v))\n",
    "        for child in v[\"_prev\"]:\n",
    "            edges.add((id(child), id(v)))\n",
    "            build(child)\n",
    "    build(root)\n",
    "    return nodes, edges\n",
    "\n",
    "def find(root, target):\n",
    "    stack=[root]\n",
    "    seen=set()\n",
    "    while stack:\n",
    "        v=stack.pop()\n",
    "        if id(v)==target:\n",
    "            return v\n",
    "        if id(v) in seen:\n",
    "            continue\n",
    "        seen.add(id(v))\n",
    "        stack.extend(v[\"_prev\"])\n",
    "    return None\n",
    "\n",
    "\n",
    "def draw(root, title=\"\"):\n",
    "    dot = Digraph(graph_attr={\"rankdir\":\"LR\", \"label\":title, \"labelloc\":\"t\"})\n",
    "    nodes, edges = trace(root)\n",
    "\n",
    "    for vid in nodes:\n",
    "        v=find(root,vid)\n",
    "        dot.node(\n",
    "            name=str(vid),\n",
    "            label=f\"{{{v['label']}|data={v['data']:.4f}|grad={v['grad']:.4f}}}\",\n",
    "            shape=\"record\"\n",
    "        )\n",
    "        if v[\"_op\"]:\n",
    "            op_id=str(vid)+v[\"_op\"]\n",
    "            dot.node(op_id, label=v[\"_op\"], shape=\"circle\")\n",
    "            dot.edge(op_id, str(vid))\n",
    "\n",
    "    for c,p in edges:\n",
    "        parent=find(root,p)\n",
    "        if parent[\"_op\"]:\n",
    "            dot.edge(str(c), str(p)+parent[\"_op\"])\n",
    "        else:\n",
    "            dot.edge(str(c), str(p))\n",
    "    return dot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9295bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autodiff implementation with addition, multiplication, and sigmoid operations.\n",
    "\n",
    "# The create_value function initializes a value dictionary that represents a node in the computational graph.\n",
    "# It takes the numerical data and an optional label, and sets up the structure for storing gradients, operation type, and previous nodes.\n",
    "def create_value(data, label=\"\"):\n",
    "    return {\n",
    "        \"data\": float(data),\n",
    "        \"grad\": 0.0,\n",
    "        \"label\": label,\n",
    "        \"_op\": \"\",\n",
    "        \"_prev\": [],\n",
    "    }\n",
    "\n",
    "# Add Operation: The add function creates a new value that represents the sum of two input values.\n",
    "def add(a, b, label=\"\"):\n",
    "    out = create_value(a[\"data\"] + b[\"data\"], label)\n",
    "    out[\"_op\"] = \"+\"\n",
    "    out[\"_prev\"] = [a, b]\n",
    "    return out\n",
    "\n",
    "# Mul Operation: The mul function creates a new value that represents the product of two input values.\n",
    "def mul(a, b, label=\"\"):\n",
    "    out = create_value(a[\"data\"] * b[\"data\"], label)\n",
    "    out[\"_op\"] = \"*\"\n",
    "    out[\"_prev\"] = [a, b]\n",
    "    return out\n",
    "\n",
    "# Sigmoid Operation: The sigmoid_func function creates a new value that represents the sigmoid activation of the input value z.\n",
    "def sigmoid_func(z, label=\"p\"):\n",
    "    p = 1/(1+math.exp(-z[\"data\"]))\n",
    "    out = create_value(p, label)\n",
    "    out[\"_op\"] = \"σ\"\n",
    "    out[\"_prev\"] = [z]\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8fe12c",
   "metadata": {},
   "source": [
    "**Autodiff Demo: Forward Pass for a Single Neuron**\n",
    "\n",
    "We begin with the forward pass of a single neuron with sigmoid activation.\n",
    "\n",
    "- Inputs: $x_1$, $x_2$\n",
    "- Weights: $w_1$, $w_2$\n",
    "- Bias: $b$\n",
    "\n",
    "The neuron computes:\n",
    "\n",
    "$$\n",
    "z = w_1 x_1 + w_2 x_2 + b\n",
    "$$\n",
    "\n",
    "Then applies the sigmoid activation:\n",
    "\n",
    "$$\n",
    "p = \\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "The diagram below shows the computation graph for this forward computation.  \n",
    "At this stage, we are only computing values — no gradients have been propagated yet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e686db40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input Values\n",
    "x1 = create_value(2.0,\"x1\")\n",
    "x2 = create_value(-1.0,\"x2\")\n",
    "\n",
    "# Set parameters\n",
    "w1 = create_value(0.7,\"w1\")\n",
    "w2 = create_value(-1.3,\"w2\")\n",
    "b  = create_value(0.1,\"b\")\n",
    "\n",
    "# FORWARD PASS\n",
    "w1x1 = mul(w1,x1,\"w1*x1\")\n",
    "w2x2 = mul(w2,x2,\"w2*x2\")\n",
    "z_partial = add(w1x1, w2x2, \"z_partial\")\n",
    "z = add(z_partial, b, \"z\")\n",
    "p  = sigmoid_func(z,\"p\")\n",
    "\n",
    "draw(p,\"Forward pass (no gradients yet)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01242ddc",
   "metadata": {},
   "source": [
    "Each rectangle in the diagram above represents a variable in the computation graph:\n",
    "\n",
    "- **Label**: The name of the variable (e.g., `x1`, `w1`, `z`, `p`)\n",
    "- **data**: The value of the variable computed during the forward pass\n",
    "- **grad**: The gradient of the final node with respect to this variable (initialized to 0.0 before backpropagation)\n",
    "\n",
    "Each circle represents an operation (e.g., addition `+`, multiplication `*`, or `sigmoid`) that combines or transforms variables.\n",
    "\n",
    "The arrows show how values flow from inputs through operations to produce the final output.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35539f3d",
   "metadata": {},
   "source": [
    "Recall how we backpropagated the loss through the graph in the previous class.\n",
    "We computed the gradient for BCE + sigmoid and then derived the gradients\n",
    "with respect to the weights and biases.\n",
    "\n",
    "Another way to compute these gradients is through **automatic differentiation (autodiff)**.\n",
    "Modern libraries such as PyTorch, TensorFlow, and JAX use autodiff to compute\n",
    "gradients efficiently.\n",
    "\n",
    "Autodiff works by computing gradients locally at each operation and propagating\n",
    "them backward through the computation graph using the chain rule.\n",
    "\n",
    "In the next few cells, we will manually simulate reverse-mode autodiff:\n",
    "\n",
    "1. Set the gradient of the final node to 1 (since $\\frac{dp}{dp} = 1$).\n",
    "2. Move backward through the graph, applying the chain rule at each operation.\n",
    "3. Distribute or scale gradients according to the operation (addition, multiplication, activation).\n",
    "4. Continue until all input variables have accumulated their gradients.\n",
    "\n",
    "This shows how autodiff systematically computes gradients for all parameters in the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23313e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: dp/dp = 1.0\n",
    "\n",
    "p[\"grad\"]=1.0\n",
    "draw(p,\"Step 1: dp/dp = 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c942ff",
   "metadata": {},
   "source": [
    "In the diagram above, we manually set the gradient of the output $p$ to 1. This represents $\\frac{dp}{dp} = 1$, which is always true for the output node in backpropagation.\n",
    "\n",
    "The next step is to compute the gradient of the sigmoid activation. \n",
    "\n",
    "For $p = \\sigma(z)$, the derivative is $p(1-p)$. \n",
    "\n",
    "This gives us $\\frac{dp}{dz} = p(1-p)$.\n",
    "\n",
    "To propagate the gradient back to $z$, we multiply the gradient at $p$ by $\\frac{dp}{dz}$:\n",
    "$$\n",
    "z.\\text{grad} = p.\\text{grad} \\times \\frac{dp}{dz}\n",
    "$$\n",
    "Since $p.\\text{grad}$ is 1, $z.\\text{grad}$ becomes $p(1-p)$.\n",
    "\n",
    "This process illustrates the chain rule in action, moving gradients backward through the computation graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca1295b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Move back one edge: p to z\n",
    "# dp/dz = p(1-p)\n",
    "dp_dz = # YOUR CODE HERE\n",
    "z[\"grad\"] = p[\"grad\"] * dp_dz\n",
    "\n",
    "draw(p,\"Step 2: dp/dz = dp/dp * dp/dz = 1 * p(1-p)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b38ca8",
   "metadata": {},
   "source": [
    "In the next cell, we propagate the gradient from $z$ to its inputs.\n",
    "\n",
    "Since $z$ was computed as\n",
    "\n",
    "$$\n",
    "z = z_{\\text{partial}} + b,\n",
    "$$\n",
    "\n",
    "the gradient at $z$ is copied to both $z_{\\text{partial}}$ and $b$.\n",
    "\n",
    "For an addition operation $f(a, b) = a + b$, the partial derivatives are\n",
    "\n",
    "$$\n",
    "\\frac{\\partial f}{\\partial a} = 1\n",
    "\\qquad \\text{and} \\qquad\n",
    "\\frac{\\partial f}{\\partial b} = 1.\n",
    "$$\n",
    "\n",
    "This means that a small change in either input causes the same change in the output.\n",
    "During backpropagation, the gradient at the output is therefore copied directly to each input — no scaling is required.\n",
    "\n",
    "This property holds for addition regardless of the values of $a$ and $b$.\n",
    "\n",
    "The diagram will show the updated gradients for these nodes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296a56dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: '+' copies gradient\n",
    "# b.grad = z.grad\n",
    "# z_partial.grad = z.grad\n",
    "\n",
    "b[\"grad\"]= # YOUR CODE HERE\n",
    "z_partial[\"grad\"]= # YOUR CODE HERE\n",
    "\n",
    "draw(p,\"Step 3: '+' copies gradient\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06867c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: '+' copies gradient again\n",
    "# w1x1.grad = z_partial.grad\n",
    "# w2x2.grad = z_partial.grad\n",
    "\n",
    "w1x1[\"grad\"]= # YOUR CODE HERE\n",
    "w2x2[\"grad\"]= # YOUR CODE HERE\n",
    "\n",
    "draw(p,\"Step 4: '+' copies gradient again\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353d1a0a",
   "metadata": {},
   "source": [
    "When backpropagating through a multiplication operation, the gradient is scaled by the value of the other input. \n",
    "\n",
    "For a node $f = a \\times b$, the gradients are:\n",
    "\n",
    "- $\\frac{\\partial f}{\\partial a} = b$\n",
    "- $\\frac{\\partial f}{\\partial b} = a$\n",
    "\n",
    "So, the gradient flowing into $z$ is multiplied by $b$ to update $a$'s gradient, and by $a$ to update $b$'s gradient. This is the chain rule in action for multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27aa1fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: '*' scales gradient by the other input\n",
    "# w1.grad = w1x1.grad * x1.data\n",
    "# x1.grad = w1x1.grad * w1.data\n",
    "# w2.grad = w2x2.grad * x2.data\n",
    "# x2.grad = w2x2.grad * w2.data\n",
    "\n",
    "w1[\"grad\"]=# YOUR CODE HERE\n",
    "x1[\"grad\"]=# YOUR CODE HERE\n",
    "\n",
    "w2[\"grad\"]=# YOUR CODE HERE\n",
    "x2[\"grad\"]=# YOUR CODE HERE\n",
    "\n",
    "draw(p,\"Step 5: '*' scales gradient\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e704d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Topological sort of the graph (from inputs to output)\n",
    "# The following is a depth-first search that builds a list of nodes in topological order (inputs before outputs). \n",
    "# We use a set to track visited nodes to avoid cycles and redundant visits.\n",
    "def topo_sort(root):\n",
    "    topo = []\n",
    "    visited = set()\n",
    "\n",
    "    def build(v):\n",
    "        vid = id(v)\n",
    "        if vid in visited:\n",
    "            return\n",
    "        visited.add(vid)\n",
    "        for child in v[\"_prev\"]:\n",
    "            build(child)\n",
    "        topo.append(v)\n",
    "    build(root)\n",
    "    return topo\n",
    "\n",
    "# Backpropagation: traverse the graph in reverse topological order and call _backward() at each node\n",
    "def backward(root):\n",
    "    topo = topo_sort(root)\n",
    "    root[\"grad\"] = 1.0\n",
    "    for v in reversed(topo):\n",
    "        # print(v[\"label\"])\n",
    "        v[\"_backward\"]()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de6dae4",
   "metadata": {},
   "source": [
    "#### Exercise:\n",
    "Update the `_backward()` functions for `add`, `mul`, and `sigmoid_func` **in the next cell** so they correctly propagate gradients using the chain rule.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7f2504",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Operation: The add function creates a new value that represents the sum of two input values.\n",
    "def add(a, b, label=\"\"):\n",
    "    out = create_value(a[\"data\"] + b[\"data\"], label)\n",
    "    out[\"_op\"] = \"+\"\n",
    "    out[\"_prev\"] = [a, b]\n",
    "\n",
    "    # The backward function for addition is straightforward: \n",
    "    # the gradient just flows back equally to both inputs.\n",
    "    # use += to accumulate gradients in case of multiple paths\n",
    "    # a.grad += out.grad and b.grad += out.grad\n",
    "    def _backward():\n",
    "        a[\"grad\"] += out[\"grad\"]\n",
    "        # YOUR CODE HERE\n",
    "        pass\n",
    "    out[\"_backward\"] = _backward\n",
    "    return out\n",
    "\n",
    "# Mul Operation: The mul function creates a new value that represents the product of two input values.\n",
    "def mul(a, b, label=\"\"):\n",
    "    out = create_value(a[\"data\"] * b[\"data\"], label)\n",
    "    out[\"_op\"] = \"*\"\n",
    "    out[\"_prev\"] = [a, b]\n",
    "\n",
    "    # The backward function for multiplication uses the product rule: \n",
    "    # the gradient with respect to a is b * grad_out, and the gradient with respect to b is a * grad_out.\n",
    "    # use += to accumulate gradients in case of multiple paths\n",
    "    # a.grad += b.data * out.grad and b.grad += a.data * out.grad\n",
    "    def _backward():\n",
    "        # YOUR CODE HERE\n",
    "        pass\n",
    "    out[\"_backward\"] = _backward\n",
    "    return out\n",
    "\n",
    "# Sigmoid Operation: The sigmoid_func function creates a new value that represents the sigmoid activation of the input value z.\n",
    "def sigmoid_func(z, label=\"p\"):\n",
    "    p = 1/(1+math.exp(-z[\"data\"]))\n",
    "    out = create_value(p, label)\n",
    "    out[\"_op\"] = \"σ\"\n",
    "    out[\"_prev\"] = [z]\n",
    "\n",
    "    # The backward function for sigmoid uses the derivative of the sigmoid function: \n",
    "    # grad_z = p * (1 - p) * grad_out, where p is the output of the sigmoid function.\n",
    "    # use += to accumulate gradients in case of multiple paths\n",
    "    # z.grad += p * (1-p) * out.grad\n",
    "    def _backward():\n",
    "        # YOUR CODE HERE\n",
    "        pass\n",
    "    out[\"_backward\"] = _backward\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ddde6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's reset the gradients to zero before backpropagation\n",
    "for v in topo_sort(p):\n",
    "    v[\"grad\"] = 0.0\n",
    "\n",
    "draw(p,\"Forward pass (no gradients yet)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c90d1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BACKWARD PASS (autodiff)\n",
    "backward(p)\n",
    "draw(p, \"After backward() - gradients computed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190c58b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = add(create_value(2.0,\"t1\"), create_value(1.0,\"t2\"), \"t1+t2\")\n",
    "print(temp[\"data\"])\n",
    "out = mul(temp, temp, \"(t1+t2)^2\")\n",
    "\n",
    "draw(out, \"Graph for (t1+t2)^2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356e8d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "backward(out)\n",
    "draw(out, \"Graph for (t1+t2)^2\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pyactivate)",
   "language": "python",
   "name": "pyactivate"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
