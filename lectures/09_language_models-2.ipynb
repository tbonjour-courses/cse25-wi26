{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CSE 25 – Introduction to Artificial Intelligence  \n",
        "## Worksheet 13: Language Models and N-grams\n",
        "\n",
        "> Language model = a model that assigns probabilities to strings of words.\n",
        "\n",
        "### Guiding Questions\n",
        "1. Why is estimating full-history probabilities hard in practice?\n",
        "2. How do unigram, bigram, and trigram models simplify the problem?\n",
        "3. How do we estimate n-gram probabilities from counts?\n",
        "4. Why do we need smoothing and perplexity?\n",
        "\n",
        "### Learning Objectives\n",
        "By the end of this worksheet, you will be able to:\n",
        "- Explain the Markov assumption in n-gram language models\n",
        "- Compute unigram and bigram probabilities from frequency counts\n",
        "- Explain sparsity and apply add-\\(\\alpha\\) smoothing\n",
        "- Compute and interpret log-probability and perplexity\n",
        "\n",
        "**Instructions:**\n",
        "\n",
        "Create a copy of this notebook and complete it during class.  \n",
        "Work through the cells below **in order**.\n",
        "\n",
        "You may discuss with your neighbors, but make sure you understand  \n",
        "what each step is doing and why.\n",
        "\n",
        "**Submission**\n",
        "\n",
        "When finished, download the notebook as a PDF and upload it to Gradescope under  \n",
        "`In-Class – Week 8 Thursday`.\n",
        "\n",
        "To download as a PDF on DataHub:  \n",
        "`File -> Save and Export Notebook As -> PDF`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Language Model\n",
        "\n",
        "A *language model* over a vocabulary $V$ assigns probabilities to strings drawn from $V^*$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's say we want to compute the probability of the next word given some history:\n",
        "\n",
        "$$\n",
        "P(w \\mid h)\n",
        "$$\n",
        "\n",
        "Suppose the history is:\n",
        "\n",
        "> *On summer evenings the sky looks very*\n",
        "\n",
        "and we want the probability that the next word is *orange*:\n",
        "\n",
        "$$\n",
        "P(\\text{orange} \\mid \\text{On summer evenings the sky looks very})\n",
        "$$\n",
        "\n",
        "A simple idea is to estimate this using counts from a large corpus.\n",
        "\n",
        "We count:\n",
        "\n",
        "- How often we see the full sequence  \n",
        "  *On summer evenings the sky looks very orange*\n",
        "\n",
        "- How often we see the history  \n",
        "  *On summer evenings the sky looks very*\n",
        "\n",
        "This gives the relative-frequency estimate:\n",
        "\n",
        "$$\n",
        "P(w \\mid h)\n",
        "=\n",
        "\\frac{C(h\\,w)}{C(h)}\n",
        "$$\n",
        "\n",
        "In words:  \n",
        "Out of all the times we saw the history $h$, how often was it followed by the word $w$?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now suppose we want the probability of an entire sequence of words rather than just one next word.\n",
        "\n",
        "Using the **chain rule**, we can write:\n",
        "\n",
        "$$\n",
        "P(w_1, w_2, \\dots, w_n)\n",
        "=\n",
        "P(w_1)\n",
        "P(w_2 \\mid w_1)\n",
        "P(w_3 \\mid w_1, w_2)\n",
        "\\cdots\n",
        "P(w_n \\mid w_1, \\dots, w_{n-1})\n",
        "$$\n",
        "$$ =\n",
        "\\prod_{k=1}^{n}\n",
        "P(w_k \\mid w_1, \\dots, w_{k-1} )\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Q. Why is estimating $ P(w_n \\mid w_1, w_2, \\dots, w_{n-1})$\n",
        "for long histories unrealistic in practice?\n",
        "\n",
        "If we had a large enough corpus, we could compute all these counts. However, even the entire web is not large enough to give reliable counts for long histories. Language is creative. New sentences are invented all the time. Most long word sequences will appear rarely - or never — in our data. We cannot expect to see every possible long history $h$ in our training data. If a sequence never appears, our estimate becomes zero. That would mean assigning probability zero to perfectly reasonable sentences. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Building a Probability Model\n",
        "\n",
        "1. Define the model  \n",
        "2. Estimate parameters  \n",
        "\n",
        "Models often make independence assumptions to reduce the number of parameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Language Modeling with N-grams\n",
        "\n",
        "In the previous cell, we saw that computing $ P(w_n \\mid w_1, w_2, \\dots, w_{n-1}) $ for long histories is unrealistic in practice.\n",
        "\n",
        "To make the problem tractable, we deliberately simplify the model.\n",
        "\n",
        "Instead of conditioning on the *entire* history, we approximate it using only *the last few words*.\n",
        "\n",
        "An **n-gram language model** assumes that each word depends only on the previous $n-1$ words:\n",
        "\n",
        "$$\n",
        "P(w_k \\mid w_1, \\dots, w_{k-1})\n",
        "\\approx\n",
        "P(w_k \\mid w_{k-n+1}, \\dots, w_{k-1})\n",
        "$$\n",
        "\n",
        "Using this assumption, the probability of a sequence becomes:\n",
        "\n",
        "$$\n",
        "P(w_1, w_2, \\dots, w_n)\n",
        "\\approx\n",
        "\\prod_{k=1}^{n}\n",
        "P(w_k \\mid w_{k-n+1}, \\dots, w_{k-1})\n",
        "$$\n",
        "\n",
        "- This simplification is called a **Markov assumption**. It means that the future depends only on a limited recent past, not the entire history. \n",
        "- This also introduces **position invariance**. In an n-gram model, the probability assigned to a word given a specific local context is the same no matter where that context appears in the sentence.\n",
        "- The conditional probabilities $P(w_k \\mid w_{k-n+1}, \\dots, w_{k-1})$ are the parameters of the model that we estimate from data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Models:**\n",
        "\n",
        "- **Unigram model ($n=1$):**\n",
        "  $\n",
        "  P(w_k \\mid w_1, \\dots, w_{k-1})\n",
        "  \\approx\n",
        "  P(w_k)\n",
        "  $\n",
        "\n",
        "- **Bigram model ($n=2$):**\n",
        "  $\n",
        "  P(w_k \\mid w_1, \\dots, w_{k-1})\n",
        "  \\approx\n",
        "  P(w_k \\mid w_{k-1})\n",
        "  $\n",
        "\n",
        "- **Trigram model ($n=3$):**\n",
        "  $\n",
        "  P(w_k \\mid w_1, \\dots, w_{k-1})\n",
        "  \\approx\n",
        "  P(w_k \\mid w_{k-2}, w_{k-1})\n",
        "  $"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Estimating N-gram Probabilities (Maximum Likelihood Estimation)\n",
        "\n",
        "Once we decide how much history to use (unigram, bigram, trigram, etc.), we need a way to compute the probabilities from data.\n",
        "\n",
        "We use **Maximum Likelihood Estimation (MLE)**.\n",
        "\n",
        "The idea of MLE is simple:\n",
        "\n",
        "> Choose the probabilities that make the observed data most likely, i.e. maximize the likelihood of the data.\n",
        "\n",
        "**What Do We Mean by \"Likelihood of the Data\"?**\n",
        "\n",
        "Suppose our corpus (our dataset of text) is a sequence of $N$ tokens: $ w_1, w_2, \\dots, w_N $\n",
        "\n",
        "An n-gram model assigns a probability to the **entire corpus**:\n",
        "\n",
        "$$\n",
        "P(\\text{corpus})\n",
        "=\n",
        "P(w_1, w_2, \\dots, w_N)\n",
        "=\n",
        "\\prod_{k=1}^{N}\n",
        "P(w_k \\mid w_{k-n+1}, \\dots, w_{k-1})\n",
        "$$\n",
        "\n",
        "This quantity is called the **likelihood** of the data under the model.\n",
        "\n",
        "\n",
        "- The corpus is fixed.  \n",
        "- The probabilities are the parameters we will compute.\n",
        "- MLE selects the probabilities that **maximize this product**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "For n-gram models, MLE leads to **relative frequency estimates**.\n",
        "\n",
        "For a **unigram model**:\n",
        "\n",
        "$$\n",
        "P(w_k)\n",
        "=\n",
        "\\frac{C(w_k)}{\\text{total number of words in the corpus}}\n",
        "$$\n",
        "\n",
        "Count how often the word appears,  \n",
        "divide by the total number of tokens.\n",
        "\n",
        "\n",
        "For a **bigram model**:\n",
        "\n",
        "$$\n",
        "P(w_k \\mid w_{k-1})\n",
        "=\n",
        "\\frac{C(w_{k-1}, w_k)}{C(w_{k-1})}\n",
        "$$\n",
        "\n",
        "Count how often the two-word sequence appears,  \n",
        "divide by how often the first word appears.\n",
        "\n",
        "\n",
        "For a **trigram model**:\n",
        "\n",
        "$$\n",
        "P(w_k \\mid w_{k-2}, w_{k-1})\n",
        "=\n",
        "\\frac{C(w_{k-2}, w_{k-1}, w_k)}\n",
        "     {C(w_{k-2}, w_{k-1})}\n",
        "$$\n",
        "\n",
        "Count how often the three-word sequence appears,  \n",
        "divide by how often the two-word history appears.\n",
        "\n",
        "\n",
        "In general, for an n-gram model:\n",
        "\n",
        "$$\n",
        "P(w_k \\mid w_{k-n+1}, \\dots, w_{k-1})\n",
        "=\n",
        "\\frac{C(w_{k-n+1}, \\dots, w_k)}\n",
        "     {C(w_{k-n+1}, \\dots, w_{k-1})}\n",
        "$$\n",
        "\n",
        "So computing n-gram probabilities always follows the same pattern:\n",
        "\n",
        "1. Count the full sequence (history + next word).  \n",
        "2. Divide by the count of the history."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Toy Exercise\n",
        "\n",
        "We will work with a **tiny toy corpus** so every computation is transparent.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Tiny corpus\n",
        "\n",
        "**Tokenization** is the process of splitting text into smaller units called *tokens*. In practice, tokens can be words, subwords, characters, or punctuation, depending on the tokenizer and model.\n",
        "\n",
        "For this toy exercise, we will treat words as tokens and include boundary markers:\n",
        "\n",
        "- `<s>` start of sentence\n",
        "- `</s>` end of sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Tokenized sentences\n",
        "tokenized_toy_sentences = [\n",
        "    [\"<s>\", \"to\", \"be\", \"or\", \"not\", \"to\", \"be\", \"</s>\"],\n",
        "    [\"<s>\", \"to\", \"be\", \"a\", \"king\", \"</s>\"],\n",
        "    [\"<s>\", \"to\", \"eat\", \"pizza\", \"</s>\"],\n",
        "]\n",
        "\n",
        "all_tokens = []\n",
        "\n",
        "# Create a list of all tokens in the corpus\n",
        "for sentence in tokenized_toy_sentences:\n",
        "    for token in sentence:\n",
        "        all_tokens.append(token)\n",
        "    \n",
        "N = len(all_tokens)\n",
        "print(\"Total Tokens, N:\", N)\n",
        "print(\"Tokens:\", all_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Vocabulary**\n",
        "\n",
        "In language modeling, the **vocabulary** (usually written as $V$) is the set of all **unique tokens** that appear in the corpus. For this toy corpus, the vocabulary includes words and boundary tokens such as `<s>` and `</s>`. Its size is denoted by $|V|$. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vocabulary - the set of unique tokens in the corpus\n",
        "\n",
        "# Create a set of unique words to form the vocabulary:\n",
        "vocab = None # YOUR CODE HERE \n",
        "\n",
        "# Get the size of the vocabulary\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "print(\"Vocabulary, V:\", vocab)\n",
        "print(\"Vocab Size, |V|:\", vocab_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "Q. If the vocabulary size is $|V|$, how many parameters do we need to compute for:\n",
        "- Unigram Model - `YOUR ANSWER HERE`\n",
        "- Bigram Model - `YOUR ANSWER HERE`\n",
        "- Trigram Model - `YOUR ANSWER HERE` "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Estimate unigram probabilities\n",
        "\n",
        "$$\n",
        "\\hat{P}(w_k) = \\frac{\\text{count}(w_k)}{\\sum_{w'} \\text{count}(w')} = \\frac{\\text{count}(w_k)}{N}\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Complete the code in the next cell to compute unigram probabilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Let's estimate unigram probabilities by counting tokens and dividing by the total number of tokens (N)\n",
        "\n",
        "# Initialize a dictionary to count the occurrences of each word\n",
        "word_counts = {}\n",
        "for w in all_tokens:\n",
        "    # If it's the first time we see this word, initialize its count to 1\n",
        "    if w not in word_counts: \n",
        "        word_counts[w] = None # YOUR CODE HERE\n",
        "    # else, increment the existing count by 1\n",
        "    else:\n",
        "        word_counts[w] = None # YOUR CODE HERE\n",
        "\n",
        "# Now we can compute the unigram probabilities \n",
        "# by dividing the count of each word by \n",
        "# the total number of tokens (N)\n",
        "\n",
        "# Initialize a dictionary to store unigram probabilities\n",
        "unigram_probs = {}\n",
        "\n",
        "# Calculate unigram probabilities for each token in the vocabulary\n",
        "for token in vocab:\n",
        "    # Calculate the unigram probability for this token\n",
        "    # This is done by taking the count of the token and dividing it by the total number of tokens (N)\n",
        "    unigram_probs[token] = None # YOUR CODE HERE\n",
        "\n",
        "print(\"Unigram probabilities:\" )\n",
        "for w in sorted(unigram_probs, key=unigram_probs.get, reverse=True):\n",
        "    print(f\"  {w}: {unigram_probs[w]:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "####  Estimate bigram probabilities\n",
        "The MLE bigram estimate is:\n",
        "\n",
        "$$\n",
        "\\hat{P}(w_k \\mid w_{k-1}) \\;=\\; \\frac{\\mathrm{count}(w_{k-1},\\, w_k)}{\\mathrm{count}(w_{k-1})}\n",
        "$$\n",
        "\n",
        "We divide how often `previous_word` $w_{t-1}$ is followed by `word` $w_t$ by how often `previous_word` $w_{t-1}$ appears in the text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Complete the code in the next two cells to calculate the bigram counts and probabilities of the toy dataset. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Now let's estimate bigram probabilities by counting bigrams and dividing by the count of the previous word\n",
        "\n",
        "bigram_counts = {}\n",
        "\n",
        "# Initialize bigram counts for all possible bigrams in the vocabulary\n",
        "for prev_word in vocab:\n",
        "    for curr_word in vocab:\n",
        "        # We can use a tuple as the key for bigram counts \n",
        "        # since tuples are immutable and can be used as dictionary keys\n",
        "        bigram_counts[(prev_word, curr_word)] = 0 # Initialize count to 0 for all possible bigrams\n",
        "\n",
        "\n",
        "# Count bigrams in the tokenized sentences (our text corpus)\n",
        "for sent in tokenized_toy_sentences:\n",
        "    for i in range(len(sent) - 1):\n",
        "        prev_word = sent[i] # w_t-1\n",
        "        curr_word = sent[i + 1] # w_t\n",
        "        \n",
        "        new_key = (prev_word, curr_word) # (w_t-1, w_t)\n",
        "        \n",
        "        # Update bigram counts\n",
        "        if (prev_word, curr_word) not in bigram_counts:\n",
        "            bigram_counts[(prev_word, curr_word)] = None # YOUR CODE HERE\n",
        "        else:\n",
        "            bigram_counts[(prev_word, curr_word)] = None # YOUR CODE HERE\n",
        "\n",
        "# print bigram counts as a table\n",
        "# with rows as previous words and columns as current words\n",
        "vocab_order = sorted(vocab)\n",
        "\n",
        "print(\"\\nBigram Counts:\")\n",
        "print(f\"{'':>10}\", end=\"\")\n",
        "for curr_word in vocab_order:\n",
        "    print(f\"{curr_word:>10}\", end=\"\")\n",
        "print()\n",
        "for prev_word in vocab_order:\n",
        "    print(f\"{prev_word:>10}\", end=\"\")\n",
        "    for curr_word in vocab_order:\n",
        "        print(f\"{bigram_counts[(prev_word, curr_word)]:>10}\", end=\"\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Now we can compute the bigram probabilities\n",
        "# by dividing the count of each bigram by the count of the previous word\n",
        "# P(w_t | w_t-1) = Count(w_t-1, w_t) / Count(w_t-1)\n",
        "\n",
        "bigram_probs = {}\n",
        "for (prev_word, curr_word) in bigram_counts:\n",
        "    bigram_probs[(prev_word, curr_word)] = None # YOUR CODE HERE\n",
        "\n",
        "# Print bigram probabilities as a table \n",
        "# with rows as previous words \n",
        "# and columns as current words\n",
        "\n",
        "print(\"\\nBigram Probabilities:\")\n",
        "print(f\"{'':>7}\", end=\"\")\n",
        "for curr_word in vocab_order:\n",
        "    print(f\"{curr_word:>7}\", end=\"\")\n",
        "print()\n",
        "for prev_word in vocab_order:\n",
        "    print(f\"{prev_word:>7}\", end=\"\")\n",
        "    for curr_word in vocab_order:\n",
        "        print(f\"{bigram_probs[(prev_word, curr_word)]:>7.2f}\", end=\"\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Probability of a sequence of tokens\n",
        "\n",
        "From chain rule we know: \n",
        "\n",
        "\n",
        "$$P(w_1, w_2, w_3..., w_n) = P(w_1) \\cdot P(w_2 \\mid w_1) \\cdot P(w_3\\mid w_1, w_2) \\cdot ...\\cdot P(w_n \\mid w_1, w_2, ... w_n)$$\n",
        "\n",
        "$$ = \\prod_{k=1}^n P(w_k \\mid w_1, w_2, ... w_k) $$\n",
        "\n",
        "For a bigram model, we have $P(w_k \\mid w_1, w_2, ... w_k)  = P(w_k \\mid w_{k-1})$\n",
        "\n",
        "$$\n",
        "P(w_1, w_2, w_3..., w_n) = \\prod_{k=1}^n P(w_k \\mid w_{k-1})\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def sequence_prob_bigram(tokenized_sequence):\n",
        "    # Calculate the probability of a sequence using bigram probabilities\n",
        "    # Initialize the probability of the sequence to 1 (because we will multiply probabilities)\n",
        "    prob = 1.0\n",
        "    for i in range(len(tokenized_sequence)-1):\n",
        "        # We can use the bigram probabilities \n",
        "        # we computed above to calculate the probability of the sequence\n",
        "\n",
        "        # Get the previous word and current word to form the key for bigram probabilities\n",
        "        prev_word = tokenized_sequence[i]\n",
        "        curr_word = tokenized_sequence[i + 1]\n",
        "\n",
        "        # Get the bigram probability for the current bigram (prev_word, curr_word)\n",
        "        # Multiply the probabilities of each bigram in the sequence to get the overall sequence probability\n",
        "        prob = None # YOUR CODE HERE\n",
        "\n",
        "    return prob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_seq = [\"<s>\", \"to\", \"be\", \"or\", \"not\", \"to\", \"be\", \"</s>\"]\n",
        "sequence_prob_bigram(test_seq) # Should print 0.0625"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Q. What happens to the probability of a sequence as the sequence gets longer?\n",
        "\n",
        "*Hint: Remember that sequence probabilities are computed by multiplying many conditional probabilities together.*\n",
        "\n",
        "`YOUR ANSWER HERE`\n",
        "\n",
        "\n",
        "Q. Consider the sequence: `<s> to eat a pizza </s>`. What would happen if we tried to get the probability of this sequence? \n",
        "\n",
        "`YOUR ANSWER HERE`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# This is a very small corpus so the non-zero bigram probabilities are relatively large (0.25, 0.5, etc.)\n",
        "# With a larger corpus, the bigram probabilities will be much smaller, and the probability of a sentence will be much smaller as well\n",
        "\n",
        "# For demonstration purposes, we can repeat the same sentence multiple times to create a longer sentence and see how the probability decreases\n",
        "\n",
        "# Long sentence\n",
        "test_seq_long = [\"<s>\", \"to\", \"be\", \"or\", \"not\", \"to\", \"be\",\"or\", \"not\", \"to\", \"be\",\"or\", \"not\", \"to\", \"be\",\"or\", \"not\", \"to\", \"be\",\"or\", \"not\", \"to\", \"be\",\"or\", \"not\", \"to\", \"be\",\"or\", \"not\", \"to\", \"be\",\"or\", \"not\", \"to\", \"be\",\"or\", \"not\", \"to\", \"be\",\"or\", \"not\", \"to\", \"be\",\"or\", \"not\", \"to\", \"be\",\"or\", \"not\", \"to\", \"be\",\"or\", \"not\", \"to\", \"be\",\"or\", \"not\", \"to\", \"be\",\"or\", \"not\", \"to\", \"be\",\"or\", \"not\", \"to\", \"be\",\"or\", \"not\", \"to\", \"be\",\"</s>\"]\n",
        "\n",
        "print(sequence_prob_bigram(test_seq_long))\n",
        "\n",
        "# Notice: this can become extremely small quickly for long sentences.\n",
        "# That is why we use **log-probability**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Tokenized sequence `<s> to eat a pizza </s>`\n",
        "test_seq_2 = [\"<s>\", \"to\", \"eat\", \"a\", \"pizza\", \"</s>\"]\n",
        "\n",
        "print(sequence_prob_bigram(test_seq_2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Log-probability of a sequence of tokens\n",
        "\n",
        "Instead of multiplying probabilities:\n",
        "\n",
        "$$\n",
        "P(w_1, w_2, w_3..., w_n) = \\prod_{k=1}^n P(w_k \\mid w_{k-1})\n",
        "$$\n",
        "\n",
        "we can sum logs:\n",
        "\n",
        "$$\n",
        "\\log P(w_1, w_2, w_3..., w_n) = \\sum_{k=1}^n \\log P(w_k \\mid w_{k-1})\n",
        "$$\n",
        "\n",
        "Benefits:\n",
        "- avoids underflow (numbers becoming 0 in a computer)\n",
        "- turns products into sums (easier to compute)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "def sequence_logprob_bigram(tokenized_sequence):\n",
        "    total = 0.0\n",
        "    for i in range(len(tokenized_sequence)-1):\n",
        "        # Get the previous word and current word to form the key for bigram probabilities\n",
        "        prev_word = tokenized_sequence[i]\n",
        "        curr_word = tokenized_sequence[i + 1]\n",
        "\n",
        "        # Get the bigram probability for the current bigram (prev_word, curr_word)\n",
        "        p = bigram_probs[(prev_word, curr_word)]\n",
        "\n",
        "        # If the bigram probability is zero, we return negative infinity for the log-probability of the sequence\n",
        "        if p == 0.0:\n",
        "            return float(\"-inf\")\n",
        "        \n",
        "        # Otherwise, we add the log of the bigram probability to the total log-probability of the sequence\n",
        "        total += math.log(p)\n",
        "    \n",
        "    return total\n",
        "\n",
        "print(\"Test Sequence Log Probability:\", sequence_logprob_bigram(test_seq))\n",
        "print(\"Long Sequence Log Probability:\", sequence_logprob_bigram(test_seq_long))\n",
        "print(\"Unseen Bigram Sequence Log Probability:\", sequence_logprob_bigram(test_seq_2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Sparsity problem (zero probabilities)\n",
        "\n",
        "If a bigram never occurred in training:\n",
        "\n",
        "$$\n",
        "\\hat{P}(w_k \\mid w_{k-1}) = 0\n",
        "$$\n",
        "\n",
        "Then any sentence containing it gets probability 0 (log probability = $-\\infty$).\n",
        "\n",
        "That is too harsh for real language."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Laplace Smoothing (or Add-1 Smoothing) \n",
        "\n",
        "**Smoothing** is a technique that assigns non-zero probability to unseen events by redistributing probability mass from seen events.\n",
        "\n",
        "In Laplace or Add-1 smoothing, we fix zeros by adding 1 to every possible next word:\n",
        "\n",
        "$$\n",
        "P_{smooth}(w_t \\mid w_{k-1})\n",
        "=\n",
        "\\frac{\\text{count}(w_{k-1}, w_k) + 1}{\\text{count}(w_{k-1}) + |V|}\n",
        "$$\n",
        "\n",
        "- $|V|$ = vocabulary size\n",
        "- For a given previous word $w_{k-1}$, there are $|V|$ possible next words\n",
        "- Adding 1 to the denominator for each possible next word ensures probabilities sum to 1\n",
        "- This guarantees **non-zero** probability everywhere"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Let's add 1 to all bigram counts to perform add-one smoothing, and then recompute the bigram probabilities and sentence log probabilities\n",
        "\n",
        "# 1. Add 1 to all bigram counts\n",
        "bigram_counts_smoothed = {}\n",
        "for (prev_word, curr_word) in bigram_counts:\n",
        "    bigram_counts_smoothed[(prev_word, curr_word)] = bigram_counts[(prev_word, curr_word)] + 1\n",
        "\n",
        "# Print smoothed bigram counts as a table\n",
        "print(\"Smoothed Bigram Counts:\")\n",
        "print(f\"{'':>7}\", end=\"\")\n",
        "for curr_word in vocab_order:\n",
        "    print(f\"{curr_word:>7}\", end=\"\")\n",
        "print()\n",
        "for prev_word in vocab_order:\n",
        "    print(f\"{prev_word:>7}\", end=\"\")\n",
        "    for curr_word in vocab_order:\n",
        "        print(f\"{bigram_counts_smoothed[(prev_word, curr_word)]:>7}\", end=\"\")\n",
        "    print()\n",
        "\n",
        "# 2. Add |V| to all word counts to account for the added counts in bigrams\n",
        "word_counts_smoothed = {}\n",
        "for prev in word_counts:\n",
        "    word_counts_smoothed[prev] = word_counts[prev] + vocab_size # Add 1 for each possible current word\n",
        "\n",
        "# 3. Recompute bigram probabilities with smoothing\n",
        "bigram_probs_smoothed = {}\n",
        "for (prev_word, curr_word) in bigram_counts_smoothed:\n",
        "   bigram_probs_smoothed[(prev_word, curr_word)] = bigram_counts_smoothed[(prev_word, curr_word)] / word_counts_smoothed[prev_word]\n",
        "\n",
        "# Print smoothed bigram probabilities as a table\n",
        "print(\"\\nSmoothed Bigram Probabilities:\")\n",
        "print(f\"{'':>7}\", end=\"\")\n",
        "for curr_word in vocab_order:\n",
        "    print(f\"{curr_word:>7}\", end=\"\")\n",
        "print()\n",
        "for prev_word in vocab_order:\n",
        "    print(f\"{prev_word:>7}\", end=\"\")\n",
        "    for curr_word in vocab_order:\n",
        "        print(f\"{bigram_probs_smoothed[(prev_word, curr_word)]:>7.2f}\", end=\"\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Now we can compute the probability and log-probability of sequences using the smoothed bigram probabilities \n",
        "\n",
        "def sequence_prob_bigram_smoothed(tokenized_sequence):\n",
        "    prob = 1.0\n",
        "    for i in range(len(tokenized_sequence)-1):\n",
        "        prev_word = tokenized_sequence[i]\n",
        "        curr_word = tokenized_sequence[i + 1]\n",
        "        prob *= bigram_probs_smoothed[(prev_word, curr_word)]\n",
        "\n",
        "    return prob\n",
        "\n",
        "def sequence_logprob_bigram_smoothed(tokenized_sequence):\n",
        "    total = 0.0\n",
        "    for i in range(len(tokenized_sequence)-1):\n",
        "        prev_word = tokenized_sequence[i]\n",
        "        curr_word = tokenized_sequence[i + 1]\n",
        "\n",
        "        p = bigram_probs_smoothed[(prev_word, curr_word)]\n",
        "        if p == 0.0:\n",
        "            return float(\"-inf\")\n",
        "        total += math.log(p)\n",
        "    return total\n",
        "\n",
        "\n",
        "print(\"Test Sequence Smoothed Probability:\", sequence_prob_bigram_smoothed(test_seq))\n",
        "print(\"Long Sequence Smoothed Probability:\", sequence_prob_bigram_smoothed(test_seq_long))\n",
        "print(\"Unseen Bigram Sequence Smoothed Probability:\", sequence_prob_bigram_smoothed(test_seq_2))\n",
        "print()\n",
        "print(\"*\"*20)\n",
        "print()\n",
        "print(\"Test Sequence Smoothed Log Probability:\", sequence_logprob_bigram_smoothed(test_seq))\n",
        "print(\"Long Sequence Smoothed Log Probability:\", sequence_logprob_bigram_smoothed(test_seq_long))\n",
        "print(\"Unseen Bigram Sequence Smoothed Log Probability:\", sequence_logprob_bigram_smoothed(test_seq_2))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Add-$\\alpha$ Smoothing**\n",
        "\n",
        "Laplace smoothing is a special case where we add 1 to every count.\n",
        "\n",
        "A more flexible version is to add a small positive value $\\alpha$:\n",
        "\n",
        "$$\n",
        "P_{\\alpha}(w \\mid c)\n",
        "=\n",
        "\\frac{\\mathrm{count}(c,w)+\\alpha}\n",
        "{\\mathrm{count}(c)+\\alpha |V|}\n",
        "$$\n",
        "\n",
        "where:\n",
        "\n",
        "- $c$ is the context (e.g., previous word in a bigram model)\n",
        "- $w$ is the next word\n",
        "- $|V|$ is vocabulary size\n",
        "- $\\alpha > 0$ is the smoothing hyperparameter that controls how much we smooth the probabilities.\n",
        "\n",
        "Strength of Smoothing:\n",
        "\n",
        "- $\\alpha = 1$ $\\rightarrow$ strong smoothing (Laplace or Add-1)\n",
        "- $\\alpha = 0.1$ $\\rightarrow$ mild smoothing\n",
        "- $\\alpha \\to 0$ $\\rightarrow$ approaches MLE (unsmoothed counts)\n",
        "\n",
        ">NOTE: Add-1 and Add-$\\alpha$ are simple smoothing methods. They work reasonably well for small models or text classification tasks.  \n",
        "For large-vocabulary language models, more advanced smoothing methods usually perform much better."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Evaluating Language Models\n",
        "\n",
        "There are two main ways to evaluate a language model.\n",
        "\n",
        "##### 1. Extrinsic Evaluation\n",
        "\n",
        "The most meaningful way to evaluate a language model is to embed it inside a real application and measure task performance.\n",
        "\n",
        "Examples:\n",
        "- Use it in speech recognition $\\rightarrow$ measure transcription accuracy.\n",
        "- Use it in machine translation $\\rightarrow$ measure translation quality.\n",
        "\n",
        "If one language model leads to better downstream performance, it is the better model.\n",
        "\n",
        "This is called **extrinsic evaluation**.\n",
        "\n",
        "- Measures real-world impact  \n",
        "- Often expensive and slow  \n",
        "\n",
        "##### 2. Intrinsic Evaluation\n",
        "\n",
        "Instead of evaluating the full system, we evaluate the language model by itself.\n",
        "\n",
        "The main idea: \n",
        "\n",
        "> A better language model assigns higher probability to real, unseen text.\n",
        "\n",
        "Given a test corpus $W = w_1, w_2, \\dots, w_N$, we compute: $P(W)$\n",
        "\n",
        "The model that assigns **higher probability** to the test corpus is better. This is called **intrinsic evaluation**.\n",
        "\n",
        "However, raw probabilities of long sequences become extremely small and hard to interpret. So instead of raw probability, we use a normalized metric called *Perplexity*. Perplexity is the standard intrinsic evaluation metric for language models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Reminder: Train, Validation, and Test Splits**\n",
        "\n",
        "As we have seen before, we use three distinct datasets:\n",
        "\n",
        "**Training Set**\n",
        "- Used to learn model parameters  \n",
        "- For n-grams: used to compute counts and probabilities  \n",
        "\n",
        "**Validation Set**\n",
        "- Used to tune hyperparameters  \n",
        "- Used to compare model variants  \n",
        "- Helps prevent overfitting \n",
        "\n",
        "**Test Set**\n",
        "- Completely held out  \n",
        "- Used only once at the very end  \n",
        "- Provides an unbiased estimate of generalization  \n",
        "\n",
        "> We must **never** train on the test set. Doing so causes *data leakage*, which artificially inflates probabilities and leads to misleading evaluation results.\n",
        "\n",
        "\n",
        "**Note on Out of Vocabulary (OOV) Tokens**\n",
        "\n",
        "Some tokens/words may appear in validation or test data that were not seen during training. If a word has zero count, the model assigns it probability 0, which makes the entire sequence probability 0.\n",
        "\n",
        "To handle this, we introduce a special token `<UNK>`:\n",
        "\n",
        "- During training, rare words are replaced with `<UNK>`.\n",
        "- The model learns a probability for `<UNK>`.\n",
        "- Any unseen word in validation or test data is mapped to `<UNK>`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Perplexity (PPL)\n",
        "\n",
        "When evaluating a language model, we want to know how **surprised** the model is by a corpus. If the model assigns high probability, it is not very surprised.  If it assigns low probability, it is very surprised.\n",
        "\n",
        "A better language model is therefore one that is **less surprised** by real, unseen text.\n",
        "\n",
        "Surprise is inversely related to probability: $\\frac{1}{P(W)}$\n",
        "\n",
        "However, value of $P(W)$ depends on sequence length of $W$.  \n",
        "Longer texts automatically have smaller probabilities. \n",
        "To remove the effect of length, we normalize (take the geometric mean) per token by taking the $N^{th}$ root.\n",
        "\n",
        "This gives the definition of **perplexity**:\n",
        "\n",
        "$$\n",
        "\\text{PPL}(W) = P(W)^{-\\frac{1}{N}}\n",
        "$$\n",
        "$$ = \\sqrt[N]{\\frac{1}{P(W)}}\n",
        "$$\n",
        "\n",
        "Lower perplexity indicates that the model assigns higher probability to the corpus and is therefore a better predictor of the data.\n",
        "\n",
        "**Log Form of Perplexity (Used in Practice)**\n",
        "\n",
        "In practice, we use the log form. Using the chain rule,\n",
        "\n",
        "$$\n",
        "P(W) = \\prod_{t=1}^{N} P(w_t \\mid \\text{history})\n",
        "$$\n",
        "\n",
        "Taking logs,\n",
        "\n",
        "$$\n",
        "\\log P(W) = \\sum_{t=1}^{N} \\log P(w_t \\mid \\text{history})\n",
        "$$\n",
        "\n",
        "Substituting into the definition,\n",
        "\n",
        "$$\n",
        "\\text{PPL}(W)\n",
        "=\n",
        "\\exp\\left(\n",
        "-\\frac{1}{N}\n",
        "\\sum_{t=1}^{N}\n",
        "\\log P(w_t \\mid \\text{history})\n",
        "\\right)\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Limitations of N-gram Models\n",
        "- The number of parameters grows rapidly with vocabulary size  \n",
        "- Data sparsity becomes severe for large $n$  \n",
        "- Long-range dependencies are ignored  \n",
        "- Words are treated as discrete symbols  \n",
        "- No notion of similarity between words  "
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python (pyactivate)",
      "language": "python",
      "name": "pyactivate"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
