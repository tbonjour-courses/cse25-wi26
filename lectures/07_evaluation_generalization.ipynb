{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f1a0f46",
   "metadata": {},
   "source": [
    "# CSE 25 – Introduction to Artificial Intelligence  \n",
    "## Worksheet 11: Evaluation for Classification Tasks\n",
    "\n",
    "**Today’s focus:** \n",
    "Once a classification model is trained, how do we **evaluate** it and decide whether it **generalizes** to new data?\n",
    "\n",
    ">Generalization = performance on new, unseen data.\n",
    "\n",
    "### Guiding Questions\n",
    "1. What does **accuracy** measure, and when can it be misleading?\n",
    "2. What do **precision**, **recall**, and **F1** measure?\n",
    "3. Why do we use **train / validation / test** splits?\n",
    "4. What is **overfitting**, and how do we detect and deal with it?\n",
    "\n",
    "### Learning Objectives\n",
    "By the end of this worksheet, you will be able to:\n",
    "- Compute **accuracy** from predictions and labels (or class)\n",
    "- Use a **confusion matrix** to compute precision, recall, and F1 score\n",
    "- Interpret a **training vs validation** curve and identify overfitting\n",
    "- Explain the purpose of **train / validation / test** splits\n",
    "\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "Create a copy of this notebook and complete it during class.  \n",
    "Work through the cells below **in order**.\n",
    "\n",
    "You may discuss with your neighbors, but make sure you understand  \n",
    "what each step is doing and why.\n",
    "\n",
    "**Submission**\n",
    "When finished, download the notebook as a PDF and upload it to Gradescope under  \n",
    "`In-Class – Week 7 Tuesday`.\n",
    "\n",
    "To download as a PDF on DataHub:  \n",
    "`File -> Save and Export Notebook As -> PDF`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a195d8",
   "metadata": {},
   "source": [
    "### Accuracy\n",
    "\n",
    "In **classification**, we often use **accuracy** to evaluate a model. It measures how often a model predicts the correct class label.\n",
    "\n",
    "$$\n",
    "\\text{Accuracy} = \\frac{\\text{Number of Correct Predictions}}{\\text{Total Number of Predictions}}\n",
    "$$\n",
    "\n",
    "We will work with a **binary classification** setting where labels are:\n",
    "- `1` = positive class\n",
    "- `0` = negative class\n",
    "\n",
    "A model outputs a **predicted label** for each example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd791af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Toy example: True labels and model predictions\n",
    "y_true = [1, 0, 1, 0, 0, 1, 1, 0] # true labels\n",
    "y_pred = [1, 0, 0, 0, 0, 1, 0, 0] # model predictions\n",
    "\n",
    "# Let's look at them side-by-side\n",
    "list(zip(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf2fd51",
   "metadata": {},
   "source": [
    "Q. Compute the **accuracy** for the toy example above.\n",
    "Write your answer below (as a fraction).\n",
    "\n",
    "`YOUR ANSWER HERE`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cacd0620",
   "metadata": {},
   "source": [
    "Q. Complete the next code cell by writing the function `accuracy(y_true, y_pred)` that returns:\n",
    "\n",
    "$$\n",
    "\\text{accuracy} = \\frac{\\#\\text{ correct predictions}}{\\#\\text{ total predictions}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79160197",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Return accuracy = (# correct) / (total).\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07686d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test cases for the accuracy function\n",
    "assert accuracy([1, 0, 1], [1, 0, 1]) == 1.0  # all correct\n",
    "assert accuracy([1, 0, 1], [0, 0, 1]) == 2/3  # one incorrect\n",
    "assert accuracy([0, 0, 0], [1, 1, 1]) == 0.0  # all incorrect\n",
    "assert accuracy([1, 1, 0, 0], [1, 0, 0, 1]) == 0.5  # two correct\n",
    "\n",
    "print(\"All accuracy tests passed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9bf376",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (accuracy(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffccece0",
   "metadata": {},
   "source": [
    "Q. Why might accuracy be misleading in some cases, e.g. on **imbalanced** datasets?\n",
    "\n",
    "*Hint: Suppose 95% of examples are negative (0), and a model predicts **all zeros**.*\n",
    "\n",
    "\n",
    "`YOUR ANSWER HERE`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cccc41b",
   "metadata": {},
   "source": [
    "To evaluate the imbalanced example, call you `accuarcy` function on `imbalanced_y_true` and `imbalanced_y_pred` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7ab25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's see what happens when we have an imbalanced dataset\n",
    "imbalanced_y_true = [0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1] # true labels                   \n",
    "imbalanced_y_pred = [0]*21 # model predictions\n",
    "\n",
    "print(\"Accuracy on imbalanced dataset:\", accuracy(imbalanced_y_true, imbalanced_y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ebae395",
   "metadata": {},
   "source": [
    "What accuracy do you get? Is accuracy the right measure in this case?\n",
    "\n",
    "`YOUR ANSWER HERE`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42201716",
   "metadata": {},
   "source": [
    "This commonly happens in problems where one class is rare but important, such as:\n",
    "- *fraud detection*, \n",
    "- *disease screening*, \n",
    "- *spam filtering*, or \n",
    "- *anomaly detection*.\n",
    "\n",
    "Because most examples belong to the majority class, a model can predict only that class and still achieve very high accuracy. The metric hides the fact that the model completely fails to detect the rare events we actually care about."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1512bfc6",
   "metadata": {},
   "source": [
    "### Confusion Matrix\n",
    "\n",
    "Accuracy tells us overall correctness. A **confusion matrix** tells us *what kinds* of mistakes the model makes.\n",
    "\n",
    "For binary classification:\n",
    "\n",
    "- **TP**: true positive (true 1, predicted 1)  \n",
    "- **FP**: false positive (true 0, predicted 1)  \n",
    "- **FN**: false negative (true 1, predicted 0)  \n",
    "- **TN**: true negative (true 0, predicted 0)\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{array}{c|cc}\n",
    " & \\textbf{Predicted 1} & \\textbf{Predicted 0} \\\\\n",
    "\\hline\n",
    "\\textbf{Actual 1} & TP & FN \\\\\n",
    "\\textbf{Actual 0} & FP & TN \\\\\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae208e18",
   "metadata": {},
   "source": [
    "Q. Complete the next code cell for the function `confusion_counts(y_true, y_pred)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3731231",
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_counts(y_true, y_pred):\n",
    "    TP = FP = FN = TN = 0\n",
    "    for yt, yp in zip(y_true, y_pred):\n",
    "        pass\n",
    "        # YOUR CODE HERE \n",
    "    return {\"TP\": TP, \"FP\": FP, \"FN\": FN, \"TN\": TN}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c5aecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test cases for the confusion_counts function\n",
    "\n",
    "# Case 1: All correct predictions\n",
    "assert confusion_counts([1, 0, 1, 0], [1, 0, 1, 0]) == {'TP': 2, 'FP': 0, 'FN': 0, 'TN': 2}\n",
    "\n",
    "# Case 2: All incorrect predictions\n",
    "assert confusion_counts([1, 1, 0, 0], [0, 0, 1, 1]) == {'TP': 0, 'FP': 2, 'FN': 2, 'TN': 0}\n",
    "\n",
    "# Case 3: Only true positives\n",
    "assert confusion_counts([1, 1, 1], [1, 1, 1]) == {'TP': 3, 'FP': 0, 'FN': 0, 'TN': 0}\n",
    "\n",
    "# Case 4: Only true negatives\n",
    "assert confusion_counts([0, 0, 0], [0, 0, 0]) == {'TP': 0, 'FP': 0, 'FN': 0, 'TN': 3}\n",
    "\n",
    "# Case 5: Mixed predictions\n",
    "assert confusion_counts([1, 0, 1, 0], [0, 1, 1, 0]) == {'TP': 1, 'FP': 1, 'FN': 1, 'TN': 1}\n",
    "\n",
    "print(\"All confusion_counts tests passed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae25c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = confusion_counts(y_true, y_pred)\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efb37cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "imbalanced_counts = confusion_counts(imbalanced_y_true, imbalanced_y_pred)\n",
    "print(imbalanced_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37f5520",
   "metadata": {},
   "source": [
    "We can also compute the accuracy from these counts:\n",
    "\n",
    "$$\n",
    "\\text{Accuracy}=\\frac{TP+TN}{TP+TN+FP+FN}\n",
    "$$\n",
    "\n",
    "Accuracy is the fraction of all predictions that are correct (correct positives + correct negatives over all cases)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b52b54",
   "metadata": {},
   "source": [
    "#### Precision, Recall, F1\n",
    "\n",
    "- **Precision**: Of all the examples the model *predicted* as positive, how many were actually positive?\n",
    "\n",
    "$$\n",
    "\\text{Precision} = \\frac{TP}{TP + FP}\n",
    "$$\n",
    "\n",
    "- **Recall**: Of all the examples that are *actually* positive, how many did the model correctly identify? Recall is also known as *True Positive Rate (TPR)*.\n",
    "\n",
    "$$\n",
    "\\text{Recall} = \\frac{TP}{TP + FN}\n",
    "$$\n",
    "\n",
    "\n",
    "- **F1**: single score combining precision and recall (harmonic mean). It is high only when both precision and recall are high.\n",
    "$$\n",
    "F_1 = 2\\cdot\\frac{\\text{Precision}\\cdot\\text{Recall}}{\\text{Precision}+\\text{Recall}}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3caf4b1",
   "metadata": {},
   "source": [
    "![Precision vs Recall](images/Precisionrecall.png)\n",
    "\n",
    "<sub>\n",
    "Image source: \"Precisionrecall.svg\" via Wikimedia Commons. Licensed under CC BY-SA 3.0.  \n",
    "https://commons.wikimedia.org/wiki/File:Precisionrecall.svg\n",
    "</sub>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afec67c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_recall_f1(y_true, y_pred):\n",
    "    \"Return precision, recall, and F1 score for binary classification.\"\n",
    "    \"If undefined, return 0.0 for that metric.\"\n",
    "    precision, recall, f1 = 0.0, 0.0, 0.0\n",
    "    # YOUR CODE HERE\n",
    "    return precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf75448",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test cases for precision_recall_f1\n",
    "\n",
    "def _close(a, b, tol=1e-9):\n",
    "    return abs(a - b) < tol\n",
    "\n",
    "# Case 1: Existing toy example\n",
    "p, r, f = precision_recall_f1(y_true, y_pred)\n",
    "assert _close(p, 1.0)\n",
    "assert _close(r, 0.5)\n",
    "assert _close(f, 2/3)\n",
    "\n",
    "# Case 2: Imbalanced example (predict all zeros)\n",
    "p, r, f = precision_recall_f1(imbalanced_y_true, imbalanced_y_pred)\n",
    "assert _close(p, 0.0)\n",
    "assert _close(r, 0.0)\n",
    "assert _close(f, 0.0)\n",
    "\n",
    "# Case 3: All predictions are positive\n",
    "p, r, f = precision_recall_f1([1, 0, 1, 0], [1, 1, 1, 1])\n",
    "assert _close(p, 0.5)     # 2 TP / (2 TP + 2 FP)\n",
    "assert _close(r, 1.0)     # 2 TP / (2 TP + 0 FN)\n",
    "assert _close(f, 2/3)\n",
    "\n",
    "# Case 4: No actual positives\n",
    "p, r, f = precision_recall_f1([0, 0, 0], [0, 1, 0])\n",
    "assert _close(p, 0.0)     # TP=0, FP=1\n",
    "assert _close(r, 0.0)     # TP=0, FN=0 -> handled as 0.0\n",
    "assert _close(f, 0.0)\n",
    "\n",
    "# Case 5: No predicted positives\n",
    "p, r, f = precision_recall_f1([1, 0, 1], [0, 0, 0])\n",
    "assert _close(p, 0.0)     # TP=0, FP=0 -> handled as 0.0\n",
    "assert _close(r, 0.0)     # TP=0, FN=2\n",
    "assert _close(f, 0.0)\n",
    "\n",
    "print(\"All precision_recall_f1 tests passed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f972940b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prec, rec, f1 = precision_recall_f1(y_true, y_pred)\n",
    "prec, rec, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451b09d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "prec_imb, rec_imb, f1_imb = precision_recall_f1(imbalanced_y_true, imbalanced_y_pred)\n",
    "prec_imb, rec_imb, f1_imb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78327c38",
   "metadata": {},
   "source": [
    "##### Precision vs Recall in Real-World Scenarios\n",
    "\n",
    "**Discuss and answer the following**\n",
    "\n",
    "Q. Look at the formulas for precision and recall above and answer the following in plain English:\n",
    "\n",
    "- If **precision** is high, what does that tell us about the model's positive predictions?\n",
    "- If **recall** is high, what does that tell us about the model's ability to find actual positives?\n",
    "\n",
    "`YOUR ANSWER HERE`\n",
    "\n",
    "Below are two example situations. Read each one and think about which type of mistake matters more.\n",
    "\n",
    "\n",
    "**Example 1**\n",
    "\n",
    "A medical screening system checks images for signs of a serious disease. Missing a real case could delay treatment, but extra follow-up tests are acceptable.\n",
    "\n",
    "- Would you prioritize **precision** or **recall** here?  \n",
    "- Which mistake is worse: a false positive or a false negative?  \n",
    "- Briefly explain your reasoning.\n",
    "\n",
    "`YOUR ANSWER HERE`\n",
    "\n",
    "\n",
    "**Example 2**\n",
    "\n",
    "An automated system flags students for academic misconduct.  A false accusation could unfairly penalize a student and cause stress.\n",
    "\n",
    "- Would you prioritize **precision** or **recall** here?  \n",
    "- Which mistake is worse: a false positive or a false negative?  \n",
    "- Briefly explain your reasoning.\n",
    "\n",
    "`YOUR ANSWER HERE`\n",
    "\n",
    "\n",
    "##### Your Turn\n",
    "\n",
    "Come up with **two real-world scenarios**:\n",
    "\n",
    "- One where you would prefer **high precision**.\n",
    "- One where you would prefer **high recall**.\n",
    "\n",
    "For each scenario, describe:\n",
    "\n",
    "- What counts as a positive prediction?\n",
    "- What is a false positive?\n",
    "- What is a false negative?\n",
    "- Which mistake matters more, and why?\n",
    "\n",
    "`YOUR ANSWER HERE`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93556aee",
   "metadata": {},
   "source": [
    "#### HOMEWORK (No Submission Required)\n",
    "\n",
    "Run the interactive demo in the next cell to explore how evaluation metrics change. This is for your own understanding and practice. You do **not** need to submit your answers.\n",
    "\n",
    "##### Explore the Interactive Threshold Demo\n",
    "\n",
    "Move the sliders and observe how the confusion matrix and metrics change.\n",
    "\n",
    "Answer the following questions as you explore.\n",
    "\n",
    "\n",
    "**1. Threshold effects**\n",
    "\n",
    "As you slowly increase the threshold from left to right:\n",
    "\n",
    "- What trend do you notice in **precision**?\n",
    "- What trend do you notice in **recall**?\n",
    "- Explain your observations using TP, FP, and FN.\n",
    "\n",
    "`YOUR ANSWER HERE`\n",
    "\n",
    "\n",
    "**2. Extreme thresholds**\n",
    "\n",
    "- Set the threshold very close to **0** (almost everything predicted positive).  \n",
    "  What happens to recall? What happens to precision?\n",
    "- Set the threshold very close to **1** (almost nothing predicted positive).  \n",
    "  What happens to recall? What happens to precision?\n",
    "\n",
    "`YOUR ANSWER HERE`\n",
    "\n",
    "\n",
    "**3. Imbalanced data (prevalence slider)**\n",
    "\n",
    "- Lower the positive rate so that positives become rare.\n",
    "- What happens to **accuracy** compared to precision and recall?\n",
    "- Why might accuracy remain high even when recall is low?\n",
    "\n",
    "`YOUR ANSWER HERE`\n",
    "\n",
    "\n",
    "**4. Separability (model quality)**\n",
    "\n",
    "- Increase separability. What changes do you observe in the confusion matrix?\n",
    "- Does changing the threshold matter as much when separability is high? Why or why not?\n",
    "\n",
    "`YOUR ANSWER HERE`\n",
    "\n",
    "\n",
    "**5. Color mode (TP/FP/FN/TN)**\n",
    "\n",
    "Switch to the outcome coloring.\n",
    "\n",
    "- Where do most **false positives** appear relative to the threshold line?\n",
    "- Where do most **false negatives** appear?\n",
    "\n",
    "`YOUR ANSWER HERE`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253f69df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "\n",
    "def confusion_counts(y_true, y_pred):\n",
    "    y_true_ = np.asarray(y_true).astype(int)\n",
    "    y_pred = np.asarray(y_pred).astype(int)\n",
    "    tp = int(np.sum((y_true == 1) & (y_pred == 1)))\n",
    "    fp = int(np.sum((y_true == 0) & (y_pred == 1)))\n",
    "    fn = int(np.sum((y_true == 1) & (y_pred == 0)))\n",
    "    tn = int(np.sum((y_true == 0) & (y_pred == 0)))\n",
    "    return tp, fp, fn, tn\n",
    "\n",
    "def safe_div(num, den):\n",
    "    return float(num) / float(den) if den != 0 else np.nan\n",
    "\n",
    "def compute_metrics(tp, fp, fn, tn):\n",
    "    acc  = safe_div(tp + tn, tp + fp + fn + tn)\n",
    "    prec = safe_div(tp, tp + fp)\n",
    "    rec  = safe_div(tp, tp + fn)   # Recall / TPR\n",
    "    f1   = safe_div(2*prec*rec, prec + rec) if (prec == prec and rec == rec and (prec + rec) != 0) else np.nan\n",
    "    return acc, prec, rec, f1\n",
    "\n",
    "def fmt(x):\n",
    "    return \"undefined\" if (x != x) else f\"{x:.3f}\"\n",
    "\n",
    "def make_dataset(n=100, prevalence=0.2, separability=2.0):\n",
    "    rng = np.random.default_rng(0)  # fixed seed\n",
    "    n_pos = int(round(n * prevalence))\n",
    "    n_neg = n - n_pos\n",
    "\n",
    "    pos_raw = rng.normal(loc=+separability/2, scale=1.0, size=n_pos)\n",
    "    neg_raw = rng.normal(loc=-separability/2, scale=1.0, size=n_neg)\n",
    "\n",
    "    sigmoid = lambda z: 1 / (1 + np.exp(-z))\n",
    "    pos_scores = sigmoid(pos_raw)\n",
    "    neg_scores = sigmoid(neg_raw)\n",
    "\n",
    "    y_true  = np.array([1]*n_pos + [0]*n_neg)\n",
    "    y_score = np.concatenate([pos_scores, neg_scores])\n",
    "\n",
    "    idx = rng.permutation(n)\n",
    "    return y_true[idx], y_score[idx]\n",
    "\n",
    "\n",
    "def plot_confusion(ax, tp, fp, fn, tn):\n",
    "    cm = np.array([[tp, fn],\n",
    "                   [fp, tn]], dtype=float)\n",
    "\n",
    "    vmax = max(cm.max(), 1.0)\n",
    "    im = ax.imshow(cm, vmin=0, vmax=vmax, cmap=\"Blues\")\n",
    "\n",
    "    ax.set_title(\"Confusion Matrix\")\n",
    "    ax.set_xticks([0, 1], labels=[\"Pred 1\", \"Pred 0\"])\n",
    "    ax.set_yticks([0, 1], labels=[\"Actual 1\", \"Actual 0\"])\n",
    "\n",
    "    threshold = vmax * 0.55\n",
    "    for (i, j), v in np.ndenumerate(cm):\n",
    "        color = \"white\" if v >= threshold else \"black\"\n",
    "        ax.text(j, i, f\"{int(v)}\", ha=\"center\", va=\"center\",\n",
    "                color=color, fontsize=12, fontweight=\"bold\")\n",
    "\n",
    "    cb = plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "    cb.set_label(\"Count\", rotation=270, labelpad=12)\n",
    "\n",
    "def plot_score_hist(ax, y_true, y_score, y_pred, thr, color_mode):\n",
    "    bins = np.linspace(0, 1, 21)\n",
    "\n",
    "    if color_mode == \"By true label\":\n",
    "        pos = y_score[y_true == 1]\n",
    "        neg = y_score[y_true == 0]\n",
    "        ax.hist(neg, bins=bins, alpha=0.6, label=\"Actual 0 (neg)\")\n",
    "        ax.hist(pos, bins=bins, alpha=0.6, label=\"Actual 1 (pos)\")\n",
    "\n",
    "    elif color_mode == \"By outcome (TP/FP/FN/TN)\":\n",
    "        tp_mask = (y_true == 1) & (y_pred == 1)\n",
    "        fp_mask = (y_true == 0) & (y_pred == 1)\n",
    "        fn_mask = (y_true == 1) & (y_pred == 0)\n",
    "        tn_mask = (y_true == 0) & (y_pred == 0)\n",
    "\n",
    "        ax.hist(y_score[tp_mask], bins=bins, alpha=0.7, label=\"TP (correct +)\")\n",
    "        ax.hist(y_score[tn_mask], bins=bins, alpha=0.7, label=\"TN (correct -)\")\n",
    "        ax.hist(y_score[fp_mask], bins=bins, alpha=0.7, label=\"FP (false alarm)\")\n",
    "        ax.hist(y_score[fn_mask], bins=bins, alpha=0.7, label=\"FN (missed +)\")\n",
    "\n",
    "    ax.axvline(thr, linewidth=2, label=f\"threshold = {thr:.2f}\")\n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_title(\"Score Distributions + Threshold\")\n",
    "    ax.set_xlabel(\"Model score / probability\")\n",
    "    ax.set_ylabel(\"Count\")\n",
    "    ax.legend(loc=\"center left\", bbox_to_anchor=(1.02, 0.5), frameon=False)\n",
    "\n",
    "\n",
    "def render(threshold, prevalence, separability, color_mode):\n",
    "    y_true, y_score = make_dataset(n=100,\n",
    "                                   prevalence=float(prevalence),\n",
    "                                   separability=float(separability))\n",
    "\n",
    "    y_pred = (y_score >= threshold).astype(int)\n",
    "\n",
    "    tp, fp, fn, tn = confusion_counts(y_true, y_pred)\n",
    "    acc, prec, rec, f1 = compute_metrics(tp, fp, fn, tn)\n",
    "\n",
    "    clear_output(wait=True)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(13, 4))\n",
    "    plot_confusion(axes[0], tp, fp, fn, tn)\n",
    "    plot_score_hist(axes[1], y_true, y_score, y_pred, threshold, color_mode)\n",
    "    plt.show()\n",
    "\n",
    "    n_pos = int(np.sum(y_true == 1))\n",
    "    n_neg = int(np.sum(y_true == 0))\n",
    "    print(f\"Dataset: n=100  positives={n_pos}  negatives={n_neg}  prevalence={n_pos/100:.2f}\")\n",
    "    print(f\"Threshold: {threshold:.2f}\")\n",
    "    print(f\"TP={tp}  FP={fp}  FN={fn}  TN={tn}\\n\")\n",
    "    print(f\"Accuracy   = {fmt(acc)}\")\n",
    "    print(f\"Precision  = {fmt(prec)}\")\n",
    "    print(f\"Recall/TPR = {fmt(rec)}\")\n",
    "    print(f\"F1         = {fmt(f1)}\")\n",
    "\n",
    "thr = widgets.FloatSlider(\n",
    "    value=0.50, min=0.0, max=1.0, step=0.01,\n",
    "    description=\"Threshold\",\n",
    "    continuous_update=True,\n",
    "    readout_format=\".2f\",\n",
    "    style={\"description_width\": \"initial\"},\n",
    "    layout=widgets.Layout(width=\"650px\")\n",
    ")\n",
    "\n",
    "prevalence = widgets.FloatSlider(\n",
    "    value=0.20, min=0.01, max=0.99, step=0.01,\n",
    "    description=\"Positive rate (prevalence)\",\n",
    "    continuous_update=False,\n",
    "    readout_format=\".2f\",\n",
    "    style={\"description_width\": \"initial\"},\n",
    "    layout=widgets.Layout(width=\"650px\")\n",
    ")\n",
    "\n",
    "separability = widgets.FloatSlider(\n",
    "    value=2.0, min=0.0, max=5.0, step=0.1,\n",
    "    description=\"Separability (hard → easy)\",\n",
    "    continuous_update=False,\n",
    "    readout_format=\".1f\",\n",
    "    style={\"description_width\": \"initial\"},\n",
    "    layout=widgets.Layout(width=\"650px\")\n",
    ")\n",
    "\n",
    "color_mode = widgets.ToggleButtons(\n",
    "    options=[\"By true label\", \"By outcome (TP/FP/FN/TN)\"],\n",
    "    value=\"By true label\",\n",
    "    description=\"Histogram colors\",\n",
    "    style={\"description_width\": \"initial\"},\n",
    "    layout=widgets.Layout(width=\"650px\")\n",
    ")\n",
    "\n",
    "ui = widgets.VBox([thr, prevalence, separability, color_mode])\n",
    "out = widgets.interactive_output(render, {\n",
    "    \"threshold\": thr,\n",
    "    \"prevalence\": prevalence,\n",
    "    \"separability\": separability,\n",
    "    \"color_mode\": color_mode\n",
    "})\n",
    "\n",
    "display(ui, out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19406f74",
   "metadata": {},
   "source": [
    "Use the controls at the top to explore how model behavior changes. This interactive graph shows how predictions and evaluation metrics depend on the **decision threshold**, the **data distribution**, and how well the model separates the classes.\n",
    "\n",
    "**Controls**\n",
    "\n",
    "- **Threshold**: changes how strict the classifier is.  \n",
    "  - Lower threshold -> more positives (higher recall, more FP).  \n",
    "  - Higher threshold -> fewer positives (higher precision, more FN).\n",
    "- **Positive rate (prevalence)**: changes how common positives are, illustrating class imbalance.\n",
    "- **Separability**: controls how easy the classification task is (how well scores separate the classes).\n",
    "\n",
    "**What you are seeing**\n",
    "\n",
    "- Each example has a **model score** (a confidence value between 0 and 1).\n",
    "- The histogram shows the distribution of many examples; each bar represents multiple data points.\n",
    "- The vertical line is the current **threshold**.  \n",
    "- Scores to the right of the line are predicted as positive.\n",
    "\n",
    "**Color Modes**\n",
    "\n",
    "- **By true label**: Colors show the actual class (positive vs negative).\n",
    "- **By outcome (TP/FP/FN/TN)**: Colors show whether predictions are correct or incorrect:\n",
    "  - TP and TN are correct predictions.\n",
    "  - FP are false alarms.\n",
    "  - FN are missed positives.\n",
    "\n",
    "**Confusion Matrix**\n",
    "\n",
    "- **TP**: correct positive predictions  \n",
    "- **FP**: false alarms  \n",
    "- **FN**: missed positives  \n",
    "- **TN**: correct negative predictions  \n",
    "\n",
    "As you adjust the controls, examples move between these categories, which changes the metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835b5e5c",
   "metadata": {},
   "source": [
    "*Note: In this course, we focus on **accuracy**, **precision**, **recall**, and the **F1 score**. In practice, other metrics such as specificity, false positive rate (FPR), ROC-AUC, and PR-AUC may also be used depending on the application. Different metrics emphasize different types of mistakes, so the “best” metric depends on the goal of the model.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad9f31c",
   "metadata": {},
   "source": [
    "### Train / Validation / Test Split\n",
    "\n",
    "Earlier in the course, we introduced the idea of splitting data into a **training set** and a **test set**.  \n",
    "We trained a model on one portion of the data and then evaluated it on *new, unseen examples* to ask:\n",
    "\n",
    "*Does the model **generalize**, or did it just memorize the training data?*\n",
    "\n",
    "We observed that:\n",
    "\n",
    "- Training performance is often **better** than test performance.\n",
    "- A model that fits the training data well may still struggle on new data.\n",
    "- Evaluation on unseen data helps us understand whether the model truly learned patterns.\n",
    "\n",
    "Today, we refine that idea by introducing a **third split** called the **validation set**.\n",
    "\n",
    "\n",
    "#### Training set\n",
    "Used to fit model parameters (weights and biases) through gradient descent. The model sees these examples during learning.\n",
    "\n",
    "#### Validation set\n",
    "Used during development to monitor performance, compare models, tune *hyperparameters*, and detect **overfitting** (when a model learns the training data too closely and performs worse on new data). The model does **not** update its parameters on this data.\n",
    "\n",
    "#### Test set\n",
    "Used only once at the very end to report final performance. It acts as an unbiased estimate of how the model performs on truly new data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d32947",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "> **Caution: Protect Your Test Set**\n",
    ">\n",
    "> The test set should be used **only once**, after all model decisions are finalized.\n",
    ">\n",
    "> Do **not** use the test set to:\n",
    ">\n",
    "> - tune hyperparameters  \n",
    "> - choose model architectures  \n",
    "> - decide when to stop training  \n",
    "> - compare multiple versions during development\n",
    ">\n",
    "> If you repeatedly look at test performance while making decisions, the model indirectly adapts to the test data, and the reported results are no longer an unbiased measure of generalization.\n",
    "\n",
    "> **Note:** Using your test set to make model or training decisions is a form of *data leakage*. It can give a false sense of how well your model actually works.\n",
    "\n",
    "\n",
    "- **Training set** teaches the model.\n",
    "- **Validation set** guides development decisions.\n",
    "- **Test set** is the final report card."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ffee2a",
   "metadata": {},
   "source": [
    "##### Hyperparameters vs Parameters\n",
    "\n",
    "**Parameters:**\n",
    "- Weights and biases (learned via gradient descent)\n",
    "\n",
    "**Some Hyperparameters:**\n",
    "- Number of layers\n",
    "- Number of neurons\n",
    "- Learning rate\n",
    "- Number of epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836cd180",
   "metadata": {},
   "source": [
    "#### Overfitting via Train vs Validation Curves\n",
    "\n",
    "Overfitting often looks like:\n",
    "\n",
    "- training metric keeps improving\n",
    "- validation metric improves, then gets worse\n",
    "\n",
    "We will simulate a plausible training/validation accuracy curve.\n",
    "\n",
    "If training performance keeps improving but validation performance worsens, the model is overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0ac405",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "rng = np.random.default_rng(0)\n",
    "n_train, n_val = 12, 60\n",
    "\n",
    "x_train = np.sort(rng.uniform(0, 1, n_train))\n",
    "y_train = np.sin(2*np.pi*x_train) + rng.normal(0, 0.25, n_train)\n",
    "\n",
    "x_val = np.sort(rng.uniform(0, 1, n_val))\n",
    "y_val = np.sin(2*np.pi*x_val) + rng.normal(0, 0.25, n_val)\n",
    "\n",
    "x_plot = np.linspace(0, 1, 300)\n",
    "\n",
    "def mse(y, yhat):\n",
    "    return float(np.mean((np.asarray(y) - np.asarray(yhat))**2))\n",
    "\n",
    "\n",
    "deg_slider = widgets.IntSlider(value=1, min=0, max=10, step=1, description=\"degree\")\n",
    "show_true = widgets.Checkbox(value=False, description=\"show true curve\")\n",
    "show_val  = widgets.Checkbox(value=False, description=\"show val data\")\n",
    "out = widgets.Output()\n",
    "\n",
    "def redraw(degree, show_true_curve, show_val_data):\n",
    "    with out:\n",
    "        out.clear_output(wait=True)\n",
    "\n",
    "        coeffs = np.polyfit(x_train, y_train, deg=degree)\n",
    "\n",
    "        yhat_train = np.polyval(coeffs, x_train)\n",
    "        yhat_val   = np.polyval(coeffs, x_val)\n",
    "        yhat_plot  = np.polyval(coeffs, x_plot)\n",
    "\n",
    "        train_mse = mse(y_train, yhat_train)\n",
    "        val_mse   = mse(y_val, yhat_val)\n",
    "\n",
    "        plt.figure(figsize=(7,4))\n",
    "\n",
    "        plt.scatter(x_train, y_train, label=\"train\")\n",
    "\n",
    "        # show validation points only if toggled\n",
    "        if show_val_data:\n",
    "            plt.scatter(x_val, y_val, alpha=0.35, label=\"val\")\n",
    "\n",
    "        plt.plot(x_plot, yhat_plot, linewidth=2, label=f\"fit (deg={degree})\")\n",
    "\n",
    "        if show_true_curve:\n",
    "            plt.plot(x_plot, np.sin(2*np.pi*x_plot), linestyle=\"--\", label=\"true: sin(2πx)\")\n",
    "\n",
    "        if show_val_data:\n",
    "            title = f\"Train MSE: {train_mse:.3f} | Val MSE: {val_mse:.3f}\"\n",
    "        else:\n",
    "            title = f\"Train MSE: {train_mse:.3f}\"\n",
    "\n",
    "        plt.title(title)\n",
    "        plt.ylim(-2, 2)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "ui = widgets.VBox([deg_slider, show_true, show_val, out])\n",
    "display(ui)\n",
    "\n",
    "widgets.interactive_output(\n",
    "    redraw,\n",
    "    {\n",
    "        \"degree\": deg_slider,\n",
    "        \"show_true_curve\": show_true,\n",
    "        \"show_val_data\": show_val\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be888711",
   "metadata": {},
   "source": [
    "Q. In the above graph, which model degree would you choose based on the validation loss?\n",
    "\n",
    "`YOUR ANSWER HERE`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb82629",
   "metadata": {},
   "source": [
    "#### What are we seeing here?\n",
    "\n",
    "We are fitting a **curve** to a small set of training data.\n",
    "\n",
    "Use the **degree** slider to change how flexible the model is:\n",
    "\n",
    "- Small degree -> simple curve\n",
    "- Large degree -> very bendy curve\n",
    "\n",
    "The model is trying to follow the pattern in the training points.\n",
    "\n",
    "The polynomial degree is a *hyperparameter* that controls model capacity.\n",
    "\n",
    "Try this:\n",
    "\n",
    "1. Increase the degree until the curve looks very wiggly.\n",
    "2. Turn on **show val data**.\n",
    "3. Notice that the complicated curve may not match the validation points well.\n",
    "\n",
    "Even though the model fits training data very closely, it may perform worse on new data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692d99d9",
   "metadata": {},
   "source": [
    "##### Some Ways We Can Reduce Overfitting\n",
    "\n",
    "- **Use a validation set (early stopping)**  \n",
    "  Monitor validation loss during training and stop when it begins to worsen.\n",
    "\n",
    "- **Simplify the model**  \n",
    "  Use fewer parameters (for example, fewer layers, fewer neurons, or a lower-degree model).\n",
    "\n",
    "- **Add regularization**  \n",
    "  Add a small penalty to discourage very large weights, encouraging smoother solutions.\n",
    "\n",
    "- **Increase the training data**  \n",
    "  More data makes it harder for the model to memorize noise.\n",
    "\n",
    "- **Use data augmentation**  \n",
    "  Create additional training examples by slightly modifying existing ones (for example, flipping or rotating images)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f780d185",
   "metadata": {},
   "source": [
    "#### Visual Examples: Learning Curve and Training Curve\n",
    "\n",
    "In the next two cells, focus on interpreting the **graphs** (not the code).\n",
    "\n",
    "- The **Learning Curve** shows how training and validation accuracy change as training set size increases.\n",
    "- The **Training Curve** shows how training loss and validation loss change across epochs.\n",
    "\n",
    "Use these plots to connect ideas from this worksheet:\n",
    "\n",
    "- **Generalization** (how well the model performs on unseen data)\n",
    "- **Overfitting** (training keeps improving while validation stops improving or worsens)\n",
    "- **Model selection / early stopping** (choosing settings based on validation behavior)\n",
    "\n",
    "As you read the plots, ask:\n",
    "\n",
    "1. Is there a gap between train and validation performance?\n",
    "2. Does the gap shrink as more data is used?\n",
    "3. At what epoch does validation loss reach its minimum?\n",
    "4. What might happen if training continues far beyond that point?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f31055",
   "metadata": {},
   "source": [
    "##### Learning Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c50b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load and preprocess the 8x8 MNIST digits dataset\n",
    "X, y = load_digits(return_X_y=True)\n",
    "\n",
    "# Shuffle the dataset to ensure random distribution of classes\n",
    "rng = np.random.default_rng(42) # fixed seed for reproducibility\n",
    "perm = rng.permutation(len(X))\n",
    "X, y = X[perm], y[perm]\n",
    "\n",
    "# Train / validation split \n",
    "X_train_full, X_val, y_train_full, y_val = train_test_split(\n",
    "    X, y, test_size=300, stratify=y, random_state=0\n",
    ")\n",
    "\n",
    "# Standardize features for better convergence of logistic regression\n",
    "scaler = StandardScaler()\n",
    "X_train_full = scaler.fit_transform(X_train_full)\n",
    "X_val = scaler.transform(X_val)\n",
    "\n",
    "# Shuffle the training data to ensure random distribution of classes for learning curve\n",
    "perm = rng.permutation(len(X_train_full))\n",
    "X_train_full = X_train_full[perm]\n",
    "y_train_full = y_train_full[perm]\n",
    "\n",
    "# Define training set sizes for learning curve\n",
    "train_sizes = np.arange(100, len(X_train_full), 80)\n",
    "\n",
    "# Compute training and validation accuracy for each training set size\n",
    "train_acc, val_acc = [], []\n",
    "\n",
    "# Loop over different training set sizes, fit the model, and evaluate accuracy\n",
    "for n in train_sizes:\n",
    "    X_train = X_train_full[:n]\n",
    "    y_train = y_train_full[:n]\n",
    "\n",
    "    model = LogisticRegression(\n",
    "        solver=\"lbfgs\",\n",
    "        C=0.1,\n",
    "        max_iter=500\n",
    "    )\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    train_acc.append(model.score(X_train, y_train))\n",
    "    val_acc.append(model.score(X_val, y_val))\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(train_sizes, train_acc, \"-o\", linewidth=3, label=\"Train\")\n",
    "plt.plot(train_sizes, val_acc, \"-s\", linewidth=3, label=\"Validation\")\n",
    "plt.text(\n",
    "    0.2, 0.05,\n",
    "    \"Validation set kept constant while training set size increases.\",\n",
    "    transform=plt.gca().transAxes,\n",
    "    fontsize=10,\n",
    "    bbox=dict(boxstyle=\"round\", alpha=0.2)\n",
    ")\n",
    "plt.figtext(\n",
    "    0.5, -0.005,\n",
    "    \"Learning curve: training vs validation accuracy as training set size increases.\",\n",
    "    ha=\"center\",\n",
    "    fontsize=11\n",
    ")\n",
    "plt.xlabel(\"Training Set Size\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Softmax Logistic Regression on 8×8 MNIST Digits\")\n",
    "plt.legend(loc=\"center left\", bbox_to_anchor=(1.02, 0.5), frameon=False)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.savefig(\"images/learning_curve_digits.png\", dpi=300, bbox_inches=\"tight\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bccbfe8",
   "metadata": {},
   "source": [
    "#### Training Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410bebd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import log_loss, accuracy_score\n",
    "\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "\n",
    "X, y = load_digits(return_X_y=True)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=300, stratify=y, random_state=0\n",
    ")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val   = scaler.transform(X_val)\n",
    "\n",
    "max_epochs = 500\n",
    "\n",
    "mlp = MLPClassifier(\n",
    "    hidden_layer_sizes=(8,),\n",
    "    activation=\"relu\",\n",
    "    solver=\"sgd\",\n",
    "    learning_rate_init=0.08,\n",
    "    batch_size=128,\n",
    "    max_iter=1,      # one epoch per .fit()\n",
    "    warm_start=True, # continue training\n",
    "    shuffle=True,\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "train_loss, val_loss = [], []\n",
    "val_acc = [] \n",
    "\n",
    "for epoch in range(1, max_epochs + 1):\n",
    "    mlp.fit(X_train, y_train)\n",
    "\n",
    "    # training loss from sklearn\n",
    "    train_loss.append(mlp.loss_)\n",
    "\n",
    "    # validation loss computed from predicted probabilities\n",
    "    val_probs = mlp.predict_proba(X_val)\n",
    "    val_loss.append(log_loss(y_val, val_probs))\n",
    "\n",
    "    # optional: validation accuracy (not plotted)\n",
    "    val_acc.append(accuracy_score(y_val, mlp.predict(X_val)))\n",
    "\n",
    "epochs = np.arange(1, max_epochs + 1)\n",
    "\n",
    "\n",
    "best_loss_epoch = int(np.argmin(val_loss) + 1)\n",
    "best_loss = float(np.min(val_loss))\n",
    "acc_at_best_loss = float(val_acc[best_loss_epoch - 1])\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8,4.5))\n",
    "plt.plot(epochs, train_loss, label=\"training loss\", linewidth=2)\n",
    "plt.plot(epochs, val_loss, label=\"validation loss\", linewidth=2)\n",
    "\n",
    "plt.axvline(best_loss_epoch, linestyle=\"--\", linewidth=2,\n",
    "            label=f\"lowest val loss = {best_loss_epoch}\")\n",
    "plt.scatter([best_loss_epoch], [best_loss], s=60)\n",
    "\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Cross-Entropy Loss\")\n",
    "plt.title(\"Neural Network on 8×8 Digits: Loss vs Epoch\")\n",
    "\n",
    "\n",
    "plt.figtext(\n",
    "    0.5, -0.01,\n",
    "    \"Training Curve: Loss vs Epoch\",\n",
    "    ha=\"center\",\n",
    "    fontsize=11\n",
    ")\n",
    "plt.grid(alpha=0.3)\n",
    "plt.legend(loc=\"center left\", bbox_to_anchor=(1.02, 0.5), frameon=False)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"images/loss_curve_digits.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"Lowest validation loss = {best_loss:.3f} at epoch {best_loss_epoch}\")\n",
    "print(f\"Final validation loss = {val_loss[-1]:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pyactivate)",
   "language": "python",
   "name": "pyactivate"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
