{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming Assignment 2: Multi-Layer Perceptrons (MLPs)\n",
    "## Feed-Forward Neural Networks from Scratch\n",
    "\n",
    "**Objectives:**\n",
    "- Understand the building blocks of a feed-forward neural network\n",
    "- Implement a computational graph using only dictionaries and lists (no classes!)\n",
    "- Implement forward and backward passes for common operations\n",
    "- Train an MLP on the MNIST handwritten digit dataset\n",
    "- **(Extra Credit)** Implement a vectorized version for faster training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Background: From Perceptrons to Multi-Layer Perceptrons\n",
    "\n",
    "### Quick Recap — What We Built in PA1\n",
    "\n",
    "In PA1, you implemented a **perceptron**: a single neuron that computes a weighted sum of its inputs, adds a bias, and passes the result through an activation function (a step function). The perceptron can learn to classify data that is **linearly separable** — meaning you can draw a straight line (or hyperplane) to separate the classes.\n",
    "\n",
    "$$y = \\text{step}\\left(\\sum_{i} w_i x_i + b\\right)$$\n",
    "\n",
    "But here's the problem: **most real-world data is not linearly separable.** A single perceptron cannot learn XOR, cannot recognize handwritten digits, and cannot do much of anything interesting. So what do we do?\n",
    "\n",
    "### Multi-Layer Perceptrons\n",
    "\n",
    "The answer is beautifully simple: **connect multiple perceptrons together in layers.** This gives us a **Multi-Layer Perceptron (MLP)**, also called a **Feed-Forward Neural Network (FFNN)**.\n",
    "\n",
    "The word \"feed-forward\" means data flows in one direction: from input → through hidden layers → to output. There are no loops or cycles.\n",
    "\n",
    "```\n",
    "INPUT LAYER          HIDDEN LAYER          OUTPUT LAYER\n",
    "  (784 neurons)       (e.g. 16 neurons)     (10 neurons)\n",
    "\n",
    "    x₁ ──────┐\n",
    "    x₂ ──────┤──→  h₁ ──┐\n",
    "    x₃ ──────┤──→  h₂ ──┤──→  y₁ (digit 0?)\n",
    "     ⋮        ├──→  h₃ ──┤──→  y₂ (digit 1?)\n",
    "     ⋮        ├──→   ⋮   ├──→   ⋮\n",
    "   x₇₈₄ ────┘──→  h₁₆ ─┘──→  y₁₀ (digit 9?)\n",
    "```\n",
    "\n",
    "**Why does stacking layers help?**  Each hidden layer learns to detect increasingly complex **features**:\n",
    "- **Layer 1** might learn to detect edges and simple strokes\n",
    "- **Layer 2** might combine edges into curves, loops, and corners\n",
    "- **Output layer** combines those features to recognize full digits\n",
    "\n",
    "This is the core idea of **deep learning** — building up complex representations from simple ones.\n",
    "\n",
    "### Anatomy of a Single Layer\n",
    "\n",
    "Each layer in an MLP does two things:\n",
    "\n",
    "**1. Linear Transformation:** Every neuron computes a weighted sum of ALL inputs from the previous layer, plus a bias:\n",
    "\n",
    "$$z_j = \\sum_{i} w_{ji} \\cdot a_i + b_j$$\n",
    "\n",
    "Or in matrix form (which you'll see in the vectorized version):\n",
    "\n",
    "$$\\mathbf{Z} = \\mathbf{W} \\cdot \\mathbf{A} + \\mathbf{b}$$\n",
    "\n",
    "where $\\mathbf{W}$ is the weight matrix, $\\mathbf{A}$ is the input from the previous layer, and $\\mathbf{b}$ is the bias vector.\n",
    "\n",
    "**2. Non-Linear Activation:** The result is passed through a non-linear function. Without non-linearity, stacking layers would be pointless — a stack of linear transformations is just one big linear transformation! Common activations include:\n",
    "\n",
    "| Activation | Formula | When to Use |\n",
    "|-----------|---------|-------------|\n",
    "| **ReLU** | $\\max(0, z)$ | Hidden layers (most common) |\n",
    "| **Sigmoid** | $\\frac{1}{1+e^{-z}}$ | Binary classification output |\n",
    "| **Softmax** | $\\frac{e^{z_i}}{\\sum_j e^{z_j}}$ | Multi-class classification output |\n",
    "\n",
    "### How Does an MLP Learn? The Training Loop\n",
    "\n",
    "Training an MLP follows a simple recipe repeated thousands of times:\n",
    "\n",
    "1. **Forward Pass:** Feed an input through the network to get a prediction\n",
    "2. **Compute Loss:** Measure how wrong the prediction was (using a loss function like cross-entropy)\n",
    "3. **Backward Pass (Backpropagation):** Compute how much each weight contributed to the error\n",
    "4. **Update Weights:** Nudge each weight in the direction that reduces the error\n",
    "\n",
    "The tricky part is step 3 — **backpropagation**. How do we figure out the gradient of the loss with respect to a weight buried deep in the network? The answer is the **chain rule** from calculus, applied systematically through a **computational graph**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Computational Graph: Our Secret Weapon\n",
    "\n",
    "Here's where things get elegant. Instead of manually deriving gradients for every possible network architecture, we build a **computational graph** — a record of every operation performed during the forward pass. Then we walk backward through this graph, applying the chain rule at each step.\n",
    "\n",
    "**Example:** Consider computing $L = (a \\cdot b + c)^2$\n",
    "\n",
    "```\n",
    "Forward:                          Backward (chain rule):\n",
    "a ─┐                              dL/da = dL/dd · dd/de · de/da\n",
    "    ├─ multiply → e ─┐                  = 2d    · 1     · b\n",
    "b ─┘                  │\n",
    "                      ├─ add → d ─── power(2) → L\n",
    "c ────────────────────┘\n",
    "                              dL/dd = 2d\n",
    "                              dL/de = dL/dd · dd/de = 2d · 1\n",
    "                              dL/da = dL/de · de/da = 2d · b\n",
    "                              dL/db = dL/de · de/db = 2d · a\n",
    "                              dL/dc = dL/dd · dd/dc = 2d · 1\n",
    "```\n",
    "\n",
    "Each node in the graph stores:\n",
    "- Its **value** (computed during the forward pass)\n",
    "- Its **gradient** (computed during the backward pass)\n",
    "- A **`_backward` function** that knows how to propagate gradients to its inputs\n",
    "\n",
    "This is exactly what PyTorch's `autograd` does under the hood! In this assignment, **you will build this system from scratch** using nothing but Python dictionaries.\n",
    "\n",
    "### What You're Building in This Assignment\n",
    "\n",
    "Here's the roadmap:\n",
    "\n",
    "| Part | What You Build | Why It Matters |\n",
    "|------|---------------|----------------|\n",
    "| **Part 1** | `create_value` — the Value node | The fundamental building block |\n",
    "| **Part 2** | Operations (`add`, `multiply`, `relu`, etc.) | Each op builds the graph AND knows its gradient |\n",
    "| **Part 3** | `softmax`, `initialize_model`, `forward` | Assembling neurons into a full network |\n",
    "| **Part 4** | `cross_entropy_loss` | Measuring prediction error |\n",
    "| **Part 5** | `backward` (backpropagation) | Automatically computing ALL gradients |\n",
    "| **Part 6** | Training loop + MNIST | Putting it all together on real data |\n",
    "| **Part 7** | Vectorized version (extra credit) | Making it 100-1000x faster with NumPy |\n",
    "\n",
    "### MNIST: The Dataset\n",
    "\n",
    "MNIST is the \"Hello, World!\" of machine learning. It contains 70,000 grayscale images of handwritten digits (0-9), each 28×28 pixels. Your network will take a flattened 784-dimensional vector as input and output probabilities for each of the 10 digit classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Part 1: Value Nodes and the Computational Graph](#part1)\n",
    "2. [Part 2: Operations (Forward and Backward)](#part2)\n",
    "3. [Part 3: Building the MLP](#part3)\n",
    "4. [Part 4: Loss Functions](#part4)\n",
    "5. [Part 5: Backpropagation](#part5)\n",
    "6. [Part 6: Training on MNIST](#part6)\n",
    "8. [Part 7 (Extra Credit): Vectorized Implementation](#part7)\n",
    "\n",
    "**Note:** Some parts may be already implemented which will have **(Done for you)** and would be worth **0 points**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Run the cells below to install dependencies and import libraries. You only need to run the `pip install` cell once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b060b589-57d9-4498-b573-9f4748fbf586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (run once and then comment out)\n",
    "!pip install scikit-learn matplotlib numpy tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c31acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Dict, List, Tuple, Union\n",
    "import tqdm as tqdm\n",
    "# For reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"part1\"></a>\n",
    "## Part 1: Value Nodes and the Computational Graph (0 points)\n",
    "\n",
    "### The Big Picture\n",
    "\n",
    "Every number in our neural network — every weight, every bias, every intermediate result — will live inside a **Value node**. A Value node is simply a Python dictionary that wraps a number and carries along some bookkeeping information for automatic differentiation.\n",
    "\n",
    "Think of it like this: instead of doing `3.0 + 4.0 = 7.0` and forgetting how we got 7, we create Value nodes that *remember* the computation. Later, when we need gradients, we can trace back through the chain of operations.\n",
    "\n",
    "### What's Inside a Value Node\n",
    "\n",
    "Each Value node is a dictionary with these keys:\n",
    "\n",
    "| Key | Type | Purpose |\n",
    "|-----|------|---------|\n",
    "| `'data'` | `float` | The actual number this node holds |\n",
    "| `'grad'` | `float` | The gradient of the loss w.r.t. this value (starts at 0.0) |\n",
    "| `'_backward'` | `function` | A function that computes gradients for this node's parents |\n",
    "| `'_prev'` | `list` | The parent Value nodes that were used to create this one |\n",
    "| `'_op'` | `str` | A label describing the operation ('+', '*', 'ReLU', etc.) |\n",
    "\n",
    "**Key insight:** The `'grad'` field will eventually hold $\\frac{\\partial \\text{Loss}}{\\partial \\text{this value}}$ — how much the final loss would change if we slightly changed this value. This is exactly what we need for gradient descent!\n",
    "\n",
    "### Task 1.1: Implement `create_value` (Done for you)\n",
    "\n",
    "The `create_value` function has been already implemented for you. But it is important for you to understand how it works as much of the later parts will require the use of values.\n",
    "\n",
    "**Study this carefully** — every operation you implement later will call `create_value` to create its output node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9678a991",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_value(data: float, _children: Tuple = (), _op: str = '') -> Dict:\n",
    "    \"\"\"\n",
    "    Create a scalar value node in the computational graph.\n",
    "    \n",
    "    Args:\n",
    "        data: A scalar number\n",
    "        _children: Tuple of Value nodes that produced this value\n",
    "        _op: String describing the operation ('+', '*', 'ReLU', etc.)\n",
    "    \n",
    "    Returns:\n",
    "        A dictionary with keys: 'data', 'grad', '_backward', '_prev', '_op'\n",
    "    \"\"\"\n",
    "    return {\n",
    "        'data': float(data),\n",
    "        'grad': 0.0,\n",
    "        '_backward': lambda: None,\n",
    "        '_prev': list(_children),\n",
    "        '_op': _op\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"part2\"></a>\n",
    "## Part 2: Operations — Forward and Backward (15 points)\n",
    "\n",
    "### How Operations Build the Computational Graph\n",
    "\n",
    "Now we'll implement the operations that connect Value nodes together. Each operation does **two things**:\n",
    "\n",
    "1. **Forward computation:** Computes the result (e.g., `a + b = 5`)\n",
    "2. **Defines `_backward`:** Stores a function that knows how to push gradients backward through this operation\n",
    "\n",
    "When we later call `backward()` on the final loss, it will walk through the graph in reverse order, calling each node's `_backward` function. This is how gradients automatically flow from the loss all the way back to the first layer's weights.\n",
    "\n",
    "### The Chain Rule — The Heart of Backpropagation\n",
    "\n",
    "Every `_backward` function implements the **chain rule**. If an operation computes `out = f(a, b)`, then:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial a} += \\frac{\\partial f}{\\partial a} \\times \\frac{\\partial L}{\\partial \\text{out}}$$\n",
    "\n",
    "In code, `out['grad']` already contains $\\frac{\\partial L}{\\partial \\text{out}}$ (computed by downstream nodes). Our job is to multiply it by the **local derivative** and add it to `a['grad']`.\n",
    "\n",
    "**Why `+=` instead of `=`?** Because a single Value node might be used in multiple operations. For example, if `a` is used in both `a + b` and `a * c`, gradients from both paths must be summed.\n",
    "\n",
    "### Visual Intuition\n",
    "\n",
    "```\n",
    "          ┌─── out['grad'] is already filled in by downstream operations\n",
    "          │\n",
    "    a ───[operation]──→ out\n",
    "    b ───┘\n",
    "          │\n",
    "          └─── _backward() uses out['grad'] and the local derivative\n",
    "               to compute gradients for a and b\n",
    "```\n",
    "\n",
    "### Task 2.1: Implement `add` (2.5 points)\n",
    "\n",
    "**Addition: `out = a + b`**\n",
    "\n",
    "The derivative of `a + b` with respect to `a` is 1, and with respect to `b` is also 1. So the gradient passes through unchanged to both inputs:\n",
    "\n",
    "$$\\frac{\\partial (a+b)}{\\partial a} = 1, \\quad \\frac{\\partial (a+b)}{\\partial b} = 1$$\n",
    "\n",
    "Therefore: `a.grad += 1 * out.grad` and `b.grad += 1 * out.grad`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d508f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add(a: Dict, b: Dict) -> Dict:\n",
    "    \"\"\"\n",
    "    Addition operation: out = a + b\n",
    "    \n",
    "    The gradient flows equally to both inputs (derivative of a+b \n",
    "    with respect to both a and b is 1).\n",
    "    \"\"\"\n",
    "    out = create_value(a['data'] + b['data'], (a, b), '+')\n",
    "    \n",
    "    def _backward():\n",
    "        # ============================================================\n",
    "        # TODO: Implement the backward pass for addition\n",
    "        # Hint: The gradient of a sum flows equally to both inputs\n",
    "        # Remember to use += (not =) since a value may be used \n",
    "        # in multiple operations\n",
    "        # ============================================================\n",
    "        pass\n",
    "    \n",
    "    out['_backward'] = _backward\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.2: Implement `multiply` (2.5 points)\n",
    "\n",
    "**Multiplication: `out = a * b`**\n",
    "\n",
    "The derivative of `a * b` with respect to `a` is `b`, and with respect to `b` is `a`. This is the \"swap rule\" — the gradient for one input is the *other* input times the upstream gradient:\n",
    "\n",
    "$$\\frac{\\partial (a \\cdot b)}{\\partial a} = b, \\quad \\frac{\\partial (a \\cdot b)}{\\partial b} = a$$\n",
    "\n",
    "Therefore: `a.grad += b.data * out.grad` and `b.grad += a.data * out.grad`\n",
    "\n",
    "**Intuition:** If `a = 3` and `b = 4`, then `out = 12`. If we increase `a` by a tiny amount $\\epsilon$, the output increases by $4\\epsilon$ (i.e., the gradient w.r.t. `a` is `b = 4`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5446ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiply(a: Dict, b: Dict) -> Dict:\n",
    "    \"\"\"\n",
    "    Multiplication operation: out = a * b\n",
    "    \n",
    "    The gradient with respect to one input is the other input \n",
    "    times the upstream gradient.\n",
    "    \"\"\"\n",
    "    out = create_value(a['data'] * b['data'], (a, b), '*')\n",
    "    \n",
    "    def _backward():\n",
    "        # ============================================================\n",
    "        # TODO: Implement the backward pass for multiplication\n",
    "        # Hint: d(a*b)/da = b and d(a*b)/db = a\n",
    "        # ============================================================\n",
    "        pass\n",
    "    \n",
    "    out['_backward'] = _backward\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.3: Implement `power` (2.5 points)\n",
    "\n",
    "**Power: `out = a^n`** (where `n` is a constant, not a Value node)\n",
    "\n",
    "From the power rule in calculus:\n",
    "\n",
    "$$\\frac{\\partial (a^n)}{\\partial a} = n \\cdot a^{n-1}$$\n",
    "\n",
    "Therefore: `a.grad += n * a.data^(n-1) * out.grad`\n",
    "\n",
    "**Note:** We will use `power` with `n = -1` later to compute division (via `a^(-1) = 1/a`). This is a common trick in computational graphs — building complex operations from simpler primitives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d85d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def power(a: Dict, n: Union[int, float]) -> Dict:\n",
    "    \"\"\"\n",
    "    Power operation: out = a^n (where n is a constant, not a Value node)\n",
    "    \"\"\"\n",
    "    out = create_value(a['data'] ** n, (a,), f'**{n}')\n",
    "    \n",
    "    def _backward():\n",
    "        # ============================================================\n",
    "        # TODO: Implement the backward pass for power\n",
    "        # Hint: d(a^n)/da = n * a^(n-1)\n",
    "        # ============================================================\n",
    "        pass\n",
    "    \n",
    "    out['_backward'] = _backward\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.4: Implement `relu` (2.5 points)\n",
    "\n",
    "**ReLU (Rectified Linear Unit): `out = max(0, a)`**\n",
    "\n",
    "ReLU is the most popular activation function for hidden layers. It's simple but powerful:\n",
    "\n",
    "$$\\text{ReLU}(a) = \\begin{cases} a & \\text{if } a > 0 \\\\ 0 & \\text{if } a \\leq 0 \\end{cases}$$\n",
    "\n",
    "Its derivative is equally simple:\n",
    "\n",
    "$$\\frac{\\partial \\text{ReLU}(a)}{\\partial a} = \\begin{cases} 1 & \\text{if } a > 0 \\\\ 0 & \\text{if } a \\leq 0 \\end{cases}$$\n",
    "\n",
    "**Intuition:** ReLU acts like a gate. When the input is positive, the gradient flows straight through (multiplied by 1). When the input is negative, the gradient is blocked (multiplied by 0). This is what allows neural networks to learn non-linear decision boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c5afde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(a: Dict) -> Dict:\n",
    "    \"\"\"\n",
    "    ReLU activation: out = max(0, a)\n",
    "    \n",
    "    Gradient is 1 where input > 0, and 0 otherwise.\n",
    "    \"\"\"\n",
    "    out = create_value(max(0, a['data']), (a,), 'ReLU')\n",
    "    \n",
    "    def _backward():\n",
    "        # ============================================================\n",
    "        # TODO: Implement the backward pass for ReLU\n",
    "        # Hint: Gradient passes through where input was positive,\n",
    "        #       and is blocked where input was negative/zero\n",
    "        # ============================================================\n",
    "        pass\n",
    "    \n",
    "    out['_backward'] = _backward\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.5: Implement `sigmoid` (2.5 points)\n",
    "\n",
    "**Sigmoid: `out = σ(a) = 1 / (1 + e^(-a))`**\n",
    "\n",
    "Sigmoid squashes any input into the range (0, 1), making it useful as an output activation for binary classification (interpreting the output as a probability).\n",
    "\n",
    "The sigmoid has a beautifully clean derivative:\n",
    "\n",
    "$$\\frac{\\partial \\sigma(a)}{\\partial a} = \\sigma(a) \\cdot (1 - \\sigma(a))$$\n",
    "\n",
    "Notice that the derivative is expressed in terms of the output itself — so we can compute the gradient using `s` (the sigmoid output) without needing to recompute the exponential.\n",
    "\n",
    "**Note:** We clip the input to `[-50, 50]` to prevent numerical overflow in `math.exp()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf7cfb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(a: Dict) -> Dict:\n",
    "    \"\"\"\n",
    "    Sigmoid activation: out = 1 / (1 + e^(-a))\n",
    "    \n",
    "    Derivative: sigmoid(x) * (1 - sigmoid(x))\n",
    "    \"\"\"\n",
    "    # Clip input for numerical stability\n",
    "    x = max(min(a['data'], 50), -50)\n",
    "    s = 1.0 / (1.0 + math.exp(-x))\n",
    "    out = create_value(s, (a,), 'sigmoid')\n",
    "    \n",
    "    def _backward():\n",
    "        # ============================================================\n",
    "        # TODO: Implement the backward pass for sigmoid\n",
    "        # Hint: d(sigmoid)/da = sigmoid * (1 - sigmoid)\n",
    "        # ============================================================\n",
    "        pass\n",
    "    \n",
    "    out['_backward'] = _backward\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.6: Implement `log_op` and `exp_op` (2.5 points)\n",
    "\n",
    "We need the natural logarithm and exponential for building our loss function and softmax activation.\n",
    "\n",
    "**Natural log: `out = ln(a)`**\n",
    "$$\\frac{\\partial \\ln(a)}{\\partial a} = \\frac{1}{a}$$\n",
    "\n",
    "**Exponential: `out = e^a`**\n",
    "$$\\frac{\\partial e^a}{\\partial a} = e^a$$\n",
    "\n",
    "(The exponential is its own derivative — one of the most beautiful facts in calculus!)\n",
    "\n",
    "**Numerical safety:** We use `max(a, 1e-10)` for `log` (to avoid `log(0)`) and clip the input for `exp` (to avoid overflow)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e00f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_op(a: Dict) -> Dict:\n",
    "    \"\"\"Natural logarithm: out = ln(a)\"\"\"\n",
    "    val = max(a['data'], 1e-10)  # Prevent log(0)\n",
    "    out = create_value(math.log(val), (a,), 'log')\n",
    "    \n",
    "    def _backward():\n",
    "        # ============================================================\n",
    "        # TODO: Implement backward for log\n",
    "        # Hint: d(ln(a))/da = 1/a\n",
    "        # ============================================================\n",
    "        pass\n",
    "    \n",
    "    out['_backward'] = _backward\n",
    "    return out\n",
    "\n",
    "def exp_op(a: Dict) -> Dict:\n",
    "    \"\"\"Exponential: out = e^a\"\"\"\n",
    "    clipped = max(min(a['data'], 88), -88)  # Prevent overflow\n",
    "    out = create_value(math.exp(clipped), (a,), 'exp')\n",
    "    \n",
    "    def _backward():\n",
    "        # ============================================================\n",
    "        # TODO: Implement backward for exp\n",
    "        # Hint: d(e^a)/da = e^a = out.data\n",
    "        # ============================================================\n",
    "        pass\n",
    "    \n",
    "    out['_backward'] = _backward\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test your operations\n",
    "\n",
    "Run the cell below to verify your forward passes are correct. If any assertion fails, go back and check your implementation. (These only test the forward pass — backward pass correctness will be tested later when we run backpropagation.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db106627",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test add\n",
    "a = create_value(2.0)\n",
    "b = create_value(3.0)\n",
    "c = add(a, b)\n",
    "assert c['data'] == 5.0, f\"add forward failed: expected 5.0, got {c['data']}\"\n",
    "\n",
    "# Test multiply\n",
    "a = create_value(2.0)\n",
    "b = create_value(3.0)\n",
    "c = multiply(a, b)\n",
    "assert c['data'] == 6.0, f\"multiply forward failed: expected 6.0, got {c['data']}\"\n",
    "\n",
    "# Test power\n",
    "a = create_value(3.0)\n",
    "c = power(a, 2)\n",
    "assert c['data'] == 9.0, f\"power forward failed: expected 9.0, got {c['data']}\"\n",
    "\n",
    "# Test relu\n",
    "a = create_value(-3.0)\n",
    "c = relu(a)\n",
    "assert c['data'] == 0.0, f\"relu forward failed for negative input\"\n",
    "a = create_value(3.0)\n",
    "c = relu(a)\n",
    "assert c['data'] == 3.0, f\"relu forward failed for positive input\"\n",
    "\n",
    "# Test sigmoid\n",
    "a = create_value(0.0)\n",
    "c = sigmoid(a)\n",
    "assert abs(c['data'] - 0.5) < 1e-6, f\"sigmoid(0) should be 0.5, got {c['data']}\"\n",
    "\n",
    "print(\"✓ All forward passes work correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"part3\"></a>\n",
    "## Part 3: Softmax and Building the MLP (30 points)\n",
    "\n",
    "Now we assemble our operations into a complete neural network! This part has three components:\n",
    "\n",
    "1. **Softmax** — converts raw scores (logits) into probabilities\n",
    "2. **Model initialization** — creates the network's weights and biases\n",
    "3. **Forward pass** — feeds data through the network\n",
    "\n",
    "### Task 3.1: Implement `softmax` (0 points) (Done for you)\n",
    "\n",
    "Softmax converts a list of logits (raw scores) into a probability distribution that sums to 1. It's used as the output activation for **multi-class classification**.\n",
    "\n",
    "$$\\text{softmax}(x_i) = \\frac{e^{x_i - \\max(x)}}{\\sum_j e^{x_j - \\max(x)}}$$\n",
    "\n",
    "**Why subtract the max?** For numerical stability. Without it, `e^x` can overflow for large values. Subtracting the max doesn't change the result (it cancels out) but keeps the numbers manageable.\n",
    "\n",
    "**Why is this done for you?** The implementation is a great example of building a complex operation from our primitives. Study it carefully — it uses `add`, `multiply`, `exp_op`, and `power` to build softmax entirely within our computational graph, so gradients flow automatically!\n",
    "\n",
    "**How it works step-by-step:**\n",
    "1. Find the maximum logit value (for stability)\n",
    "2. For each logit: compute `exp(logit - max)`\n",
    "3. Sum all the exponentials (by chaining `add` operations)\n",
    "4. Divide each exponential by the sum (using `power(sum, -1)` to get `1/sum`, then `multiply`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26acadfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(logits: List[Dict]) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Softmax activation applied to a list of Value nodes.\n",
    "    \n",
    "    Args:\n",
    "        logits: List of Value dictionaries (the raw scores)\n",
    "    \n",
    "    Returns:\n",
    "        List of Value dictionaries (probabilities that sum to 1)\n",
    "    \n",
    "    Implementation steps:\n",
    "        1. Find max value (for numerical stability) — no grad needed\n",
    "        2. Compute exp(x_i - max) for each logit\n",
    "        3. Sum all exponentials\n",
    "        4. Divide each exp by the sum (use power(sum, -1) then multiply)\n",
    "    \"\"\"\n",
    "    max_val = max(v['data'] for v in logits)\n",
    "    max_node = create_value(max_val)\n",
    "    \n",
    "    # Exponentials\n",
    "    exps = []\n",
    "    for v in logits:\n",
    "        shifted = add(v, multiply(max_node, create_value(-1.0)))\n",
    "        exps.append(exp_op(shifted))\n",
    "    \n",
    "    # Sum\n",
    "    sum_node = exps[0]\n",
    "    for i in range(1, len(exps)):\n",
    "        sum_node = add(sum_node, exps[i])\n",
    "    \n",
    "    # Probabilities\n",
    "    inv_sum = power(sum_node, -1)\n",
    "    probs = [multiply(e, inv_sum) for e in exps]\n",
    "    \n",
    "    return probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.2: Implement `initialize_model` (5 points)\n",
    "\n",
    "Now we create the actual neural network structure. Our MLP is stored as a dictionary of layers, where each layer contains a list of neurons, and each neuron has its own weights and bias — all as Value nodes.\n",
    "\n",
    "**The data structure looks like this:**\n",
    "\n",
    "```python\n",
    "model = {\n",
    "    'layers': [\n",
    "        {   # Layer 0 (first hidden layer)\n",
    "            'neurons': [\n",
    "                {'w': [Value, Value, ...], 'b': Value},  # Neuron 0\n",
    "                {'w': [Value, Value, ...], 'b': Value},  # Neuron 1\n",
    "                ...\n",
    "            ]\n",
    "        },\n",
    "        {   # Layer 1 (output layer)\n",
    "            'neurons': [...]\n",
    "        }\n",
    "    ],\n",
    "    'activations': ['relu', 'softmax']\n",
    "}\n",
    "```\n",
    "\n",
    "**Weight Initialization Matters!** We use **Xavier initialization**: weights are drawn from a Gaussian distribution with standard deviation $\\sqrt{1/n_{\\text{in}}}$ where $n_{\\text{in}}$ is the number of inputs to the layer. This keeps the signal from exploding or vanishing as it passes through layers.\n",
    "\n",
    "**What you need to implement:** For each layer, create `layer_nout` neurons, where each neuron has:\n",
    "- `'w'`: a list of `layer_nin` Value nodes, each initialized with `random.gauss(0, scale)`\n",
    "- `'b'`: a single Value node initialized to `0.0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6293784",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model(nin: int, layer_sizes: List[int], activations: List[str] = None) -> Dict:\n",
    "    \"\"\"\n",
    "    Create an MLP as a dictionary of layers.\n",
    "    \n",
    "    Args:\n",
    "        nin: Input dimension (e.g., 784 for MNIST)\n",
    "        layer_sizes: List of output sizes for each layer (e.g., [16, 10])\n",
    "        activations: List of activation functions (e.g., ['relu', 'softmax'])\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with 'layers' and 'activations' keys\n",
    "    \"\"\"\n",
    "    if activations is None:\n",
    "        activations = ['relu'] * (len(layer_sizes) - 1) + ['linear']\n",
    "    \n",
    "    sz = [nin] + layer_sizes\n",
    "    layers = []\n",
    "    \n",
    "    for i in range(len(layer_sizes)):\n",
    "        layer_nin = sz[i]\n",
    "        layer_nout = sz[i + 1]\n",
    "        scale = math.sqrt(1.0 / layer_nin)  # Xavier initialization\n",
    "        \n",
    "        # ============================================================\n",
    "        # TODO: Create a list of neurons for this layer.\n",
    "        # Each neuron is a dictionary with:\n",
    "        #   'w': list of layer_nin Value nodes (weights)\n",
    "        #        initialized with random.gauss(0, scale)\n",
    "        #   'b': one Value node (bias) initialized to 0.0\n",
    "        # \n",
    "        # Append {'neurons': neurons_list} to layers\n",
    "        # ============================================================\n",
    "        pass\n",
    "    \n",
    "    return {\n",
    "        'layers': layers,\n",
    "        'activations': activations\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.3: Implement `forward` (25 points)\n",
    "\n",
    "The forward pass is where data actually flows through the network. For each layer, you'll:\n",
    "\n",
    "1. **Linear transformation:** For each neuron, compute the weighted sum of inputs plus bias\n",
    "2. **Activation:** Apply the layer's activation function\n",
    "\n",
    "**The linear pass for a single neuron:**\n",
    "```\n",
    "act = bias\n",
    "for each (weight, input) pair:\n",
    "    act = add(act, multiply(weight, input))\n",
    "```\n",
    "\n",
    "This computes $z = w_1 x_1 + w_2 x_2 + \\cdots + w_n x_n + b$\n",
    "\n",
    "**Then apply activation** based on `mlp['activations'][i]`:\n",
    "- `'relu'` → apply `relu()` to each value independently\n",
    "- `'sigmoid'` → apply `sigmoid()` to each value independently  \n",
    "- `'softmax'` → apply `softmax()` to the entire list (since softmax depends on ALL values)\n",
    "- `'linear'` → no activation (pass through)\n",
    "\n",
    "**Important:** Every operation here (`add`, `multiply`, `relu`, etc.) is building the computational graph! When we later call `backward()`, gradients will flow through all these operations automatically.\n",
    "\n",
    "**Hint:** The function receives `x_raw` as a list of plain floats. The first thing it does is wrap them in Value nodes with `create_value`. After that, everything should use our graph operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088486ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(mlp: Dict, x_raw: List[float]) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Forward pass through the MLP.\n",
    "    \n",
    "    Args:\n",
    "        mlp: The model dictionary\n",
    "        x_raw: List of input values (raw floats)\n",
    "    \n",
    "    Returns:\n",
    "        List of output Value nodes\n",
    "    \"\"\"\n",
    "    # Convert raw inputs to Value nodes\n",
    "    current_x = [create_value(v) for v in x_raw]\n",
    "    \n",
    "    for i, layer in enumerate(mlp['layers']):\n",
    "        next_x = []\n",
    "        activation = mlp['activations'][i]\n",
    "        \n",
    "        # ============================================================\n",
    "        # TODO: \n",
    "        # 1. LINEAR PASS: For each neuron in this layer:\n",
    "        #    - Start with the bias: act = neuron['b']\n",
    "        #    - For each (weight, input) pair:\n",
    "        #        act = add(act, multiply(weight, input))\n",
    "        #    - Append act to next_x\n",
    "        #\n",
    "        # 2. ACTIVATION: Apply the appropriate activation\n",
    "        #    - 'relu':    next_x = [relu(x) for x in next_x]\n",
    "        #    - 'sigmoid': next_x = [sigmoid(x) for x in next_x]\n",
    "        #    - 'softmax': next_x = softmax(next_x)\n",
    "        #    - 'linear':  no change\n",
    "        # ============================================================\n",
    "        pass\n",
    "        \n",
    "        current_x = next_x\n",
    "    \n",
    "    return current_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.4: Implement `get_parameters` (Done for you)\n",
    "\n",
    "This helper function collects all trainable parameters (weights and biases) from the model into a flat list. We need this during training to:\n",
    "1. Zero out all gradients before each backward pass\n",
    "2. Update all parameters after each backward pass\n",
    "\n",
    "Study how it traverses the nested structure: layers → neurons → weights + bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30482133",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parameters(mlp: Dict) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Get all parameter Value nodes from the model.\n",
    "    \n",
    "    Returns a flat list of all weight and bias Value nodes.\n",
    "    \"\"\"\n",
    "    params = []\n",
    "    for layer in mlp['layers']:\n",
    "        for n in layer['neurons']:\n",
    "            params.extend(n['w'])\n",
    "            params.append(n['b'])\n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test your MLP\n",
    "\n",
    "Run the cell below to verify that your model initializes correctly and that the forward pass produces valid softmax probabilities (they should sum to 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1bafa5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a small network\n",
    "model = initialize_model(3, [4, 2], activations=['relu', 'softmax'])\n",
    "\n",
    "# Check structure\n",
    "assert len(model['layers']) == 2, \"Should have 2 layers\"\n",
    "assert len(model['layers'][0]['neurons']) == 4, \"First layer should have 4 neurons\"\n",
    "assert len(model['layers'][1]['neurons']) == 2, \"Second layer should have 2 neurons\"\n",
    "assert len(model['layers'][0]['neurons'][0]['w']) == 3, \"First layer neurons should have 3 weights\"\n",
    "\n",
    "# Test forward\n",
    "x = [1.0, 2.0, 3.0]\n",
    "out = forward(model, x)\n",
    "assert len(out) == 2, \"Output should have 2 values\"\n",
    "\n",
    "# Check softmax sums to 1\n",
    "prob_sum = sum(o['data'] for o in out)\n",
    "assert abs(prob_sum - 1.0) < 1e-6, f\"Softmax should sum to 1, got {prob_sum}\"\n",
    "\n",
    "# Check parameters\n",
    "params = get_parameters(model)\n",
    "expected_params = 3*4 + 4 + 4*2 + 2  # weights + biases for each layer\n",
    "assert len(params) == expected_params, f\"Expected {expected_params} params, got {len(params)}\"\n",
    "\n",
    "print(f\"✓ MLP created successfully! Total parameters: {len(params)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"part4\"></a>\n",
    "## Part 4: Loss Functions (5 points)\n",
    "\n",
    "### Why We Need a Loss Function\n",
    "\n",
    "The loss function measures **how wrong** our network's predictions are. During training, our goal is to minimize this loss. The gradients computed by backpropagation tell us how to adjust each weight to reduce the loss.\n",
    "\n",
    "### Task 4.1: Implement Cross-Entropy Loss (5 points)\n",
    "\n",
    "For multi-class classification, we use **cross-entropy loss**:\n",
    "\n",
    "$$\\mathcal{L} = -\\log(p_{\\text{target}})$$\n",
    "\n",
    "where $p_{\\text{target}}$ is the predicted probability for the **correct** class.\n",
    "\n",
    "**Intuition:**\n",
    "- If the model assigns probability 0.9 to the correct class: $L = -\\log(0.9) = 0.105$ (low loss — good!)\n",
    "- If the model assigns probability 0.1 to the correct class: $L = -\\log(0.1) = 2.303$ (high loss — bad!)\n",
    "- If the model assigns probability 0.01 to the correct class: $L = -\\log(0.01) = 4.605$ (very high loss — very bad!)\n",
    "\n",
    "The loss grows rapidly as the predicted probability for the correct class approaches 0, creating a strong gradient signal that pushes the network to fix its mistake.\n",
    "\n",
    "**Build this using operations you've already defined** (`log_op`, `multiply`, `create_value`). This is key — because the loss is built from our graph operations, gradients flow automatically through the loss into the network!\n",
    "\n",
    "**Implementation hint:** It's just one line: `multiply(log_op(probs[target_idx]), create_value(-1.0))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abaaebfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(probs: List[Dict], target_idx: int) -> Dict:\n",
    "    \"\"\"\n",
    "    Cross-entropy loss for classification.\n",
    "    \n",
    "    Args:\n",
    "        probs: List of probability Value nodes (output of softmax)\n",
    "        target_idx: Index of the correct class\n",
    "    \n",
    "    Returns:\n",
    "        loss: A scalar Value node\n",
    "    \"\"\"\n",
    "    # ============================================================\n",
    "    # TODO: Compute loss = -log(probs[target_idx])\n",
    "    #\n",
    "    # Hint: Use log_op() and multiply() with create_value(-1.0)\n",
    "    # loss = multiply(log_op(probs[target_idx]), create_value(-1.0))\n",
    "    # ============================================================\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test your loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8f9b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test cross-entropy loss\n",
    "# If model is confident and correct, loss should be low\n",
    "probs = [create_value(0.9), create_value(0.1)]\n",
    "loss = cross_entropy_loss(probs, target_idx=0)\n",
    "assert loss['data'] < 0.2, f\"Loss should be low for confident correct prediction, got {loss['data']}\"\n",
    "\n",
    "# If model is confident and wrong, loss should be high\n",
    "loss_wrong = cross_entropy_loss(probs, target_idx=1)\n",
    "assert loss_wrong['data'] > 2.0, f\"Loss should be high for wrong prediction, got {loss_wrong['data']}\"\n",
    "\n",
    "print(f\"✓ Loss for correct prediction: {loss['data']:.4f}\")\n",
    "print(f\"✓ Loss for wrong prediction: {loss_wrong['data']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"part5\"></a>\n",
    "## Part 5: Backpropagation (0 points)\n",
    "\n",
    "### Task 5.1: `backward` — Automatic Differentiation (Done for you)\n",
    "\n",
    "This is where all the magic comes together! The `backward` function performs **automatic differentiation** — it computes the gradient of the loss with respect to **every single value** in the computational graph, all in one pass.\n",
    "\n",
    "**How it works, step by step:**\n",
    "\n",
    "**Step 1: Topological Sort.** We need to process nodes in the right order — a node's gradient can only be computed after all its *downstream* nodes have been processed. Topological sort gives us this ordering using depth-first search (DFS).\n",
    "\n",
    "**Step 2: Seed the gradient.** We set `loss['grad'] = 1.0` because $\\frac{\\partial L}{\\partial L} = 1$.\n",
    "\n",
    "**Step 3: Walk backward.** We iterate through the nodes in *reverse* topological order. For each node, we call its `_backward()` function, which pushes gradients to its parent nodes using the chain rule.\n",
    "\n",
    "```\n",
    "Forward:   input → hidden → output → loss\n",
    "                                       ↓ (grad = 1.0)\n",
    "Backward:  input ← hidden ← output ← loss\n",
    "           (grads flow backward through _backward functions)\n",
    "```\n",
    "\n",
    "**After `backward()` completes**, every parameter's `.grad` field contains $\\frac{\\partial L}{\\partial \\text{param}}$ — exactly what we need for gradient descent!\n",
    "\n",
    "**Study this implementation carefully** — it's the same algorithm that powers PyTorch, TensorFlow, and every other modern deep learning framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69f006a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(loss: Dict):\n",
    "    \"\"\"\n",
    "    Backpropagation: compute gradients for all nodes in the graph.\n",
    "    \n",
    "    Args:\n",
    "        loss: The scalar loss Value node (output of cross_entropy_loss)\n",
    "    \n",
    "    Steps:\n",
    "        1. Build topological order using DFS\n",
    "        2. Set loss.grad = 1.0\n",
    "        3. Call _backward() on each node in reverse topological order\n",
    "    \"\"\"\n",
    "    topo = []\n",
    "    visited = set()\n",
    "    def build_topo(v):\n",
    "        if id(v) not in visited:\n",
    "            visited.add(id(v))\n",
    "            for child in v.get('_prev', []):\n",
    "                build_topo(child)\n",
    "            topo.append(v)\n",
    "    build_topo(loss)\n",
    "    \n",
    "    loss['grad'] = 1.0\n",
    "    for node in reversed(topo):\n",
    "        node['_backward']()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"part6\"></a>\n",
    "## Part 6: Training on MNIST (50 points)\n",
    "\n",
    "Now we put everything together and train a real neural network on real data! This is where you'll see all the pieces — Value nodes, operations, forward pass, loss, and backward pass — working together.\n",
    "\n",
    "### 6.1: Load and Preprocess MNIST (5 points)\n",
    "\n",
    "The code below loads the MNIST dataset, normalizes pixel values to [0, 1], shuffles the data, and splits it into train/validation/test sets.\n",
    "\n",
    "**Data shapes** (note the transposition — features are rows, samples are columns):\n",
    "- `X_train`: shape `(784, 400)` — 400 training images, each 784 pixels\n",
    "- `Y_train_oh`: shape `(10, 400)` — one-hot encoded labels\n",
    "\n",
    "We use a small subset (400 train, 100 val, 100 test) because our scalar implementation processes one operation at a time — it works, but it's slow! The extra credit vectorized version will handle much larger datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48abcccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"Loading MNIST...\")\n",
    "mnist = fetch_openml('mnist_784', version=1, as_frame=False)\n",
    "X_all, Y_all = mnist.data, mnist.target.astype(int)\n",
    "X_all = X_all / 255.0\n",
    "\n",
    "# --- SHUFFLING STEP ---\n",
    "# Create an array of indices from 0 to len(X_all)\n",
    "np.random.seed(42)\n",
    "indices = np.random.permutation(len(X_all))\n",
    "# Reorder both X and Y using these indices\n",
    "X_all = X_all[indices]\n",
    "Y_all = Y_all[indices]\n",
    "# ----------------------\n",
    "\n",
    "# Define split sizes\n",
    "num_train = 400\n",
    "num_val = 100\n",
    "num_test = 100\n",
    "\n",
    "# Slicing the data (Transposing X so features are rows: 784 x N)\n",
    "X_train = X_all[:num_train].T\n",
    "Y_train = Y_all[:num_train]\n",
    "\n",
    "X_val = X_all[num_train : num_train + num_val].T\n",
    "Y_val = Y_all[num_train : num_train + num_val]\n",
    "\n",
    "X_test = X_all[num_train + num_val : num_train + num_val + num_test].T\n",
    "Y_test = Y_all[num_train + num_val : num_train + num_val + num_test]\n",
    "\n",
    "def to_one_hot(labels, num_classes=10):\n",
    "    one_hot = np.zeros((num_classes, len(labels)))\n",
    "    for i, label in enumerate(labels):\n",
    "        one_hot[label, i] = 1.0\n",
    "    return one_hot\n",
    "\n",
    "Y_train_oh = to_one_hot(Y_train)\n",
    "Y_val_oh = to_one_hot(Y_val)\n",
    "Y_test_oh = to_one_hot(Y_test)\n",
    "\n",
    "print(f\"Shuffled Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}\")\n",
    "\n",
    "# Visualization to confirm variety after shuffling\n",
    "fig, axes = plt.subplots(1, 5, figsize=(12, 3))\n",
    "for i in range(5):\n",
    "    axes[i].imshow(X_train[:, i].reshape(28, 28), cmap='gray')\n",
    "    axes[i].set_title(f\"Label: {Y_train[i]}\")\n",
    "    axes[i].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2: Implement the `get_accuracy` function (5 points)\n",
    "\n",
    "This function evaluates how well the model is doing. For each sample:\n",
    "1. Extract the input (a column of `X`) and convert to a Python list\n",
    "2. Get the true label (using `np.argmax` on the one-hot encoded `Y`)\n",
    "3. Run the forward pass to get output probabilities\n",
    "4. The predicted class is the one with highest probability (`np.argmax`)\n",
    "5. Count how many predictions match the true labels\n",
    "\n",
    "**Note:** This is an evaluation function — we don't need gradients here, just predictions. But since our forward pass always builds a graph, it works fine (we just ignore the gradients)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60e1033",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(model: Dict, X: np.ndarray, Y: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Compute classification accuracy.\n",
    "    \n",
    "    Args:\n",
    "        model: The MLP dictionary\n",
    "        X: Input data, shape (features, num_samples) — columns are samples\n",
    "        Y: Labels, shape (num_classes, num_samples) — one-hot encoded\n",
    "    \n",
    "    Returns:\n",
    "        Accuracy as a percentage (0-100)\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    num_samples = X.shape[1]\n",
    "    \n",
    "    # ============================================================\n",
    "    # TODO: For each sample:\n",
    "    #   1. Get the input column: X[:, i].tolist()\n",
    "    #   2. Get the true label: np.argmax(Y[:, i])\n",
    "    #   3. Run forward pass\n",
    "    #   4. Get predicted class: np.argmax of output data values\n",
    "    #   5. Count correct predictions\n",
    "    #\n",
    "    # Return (correct / num_samples) * 100\n",
    "    # ============================================================\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3: Implement the Training Loop (20 points)\n",
    "\n",
    "This is the core of machine learning — the training loop that iteratively improves the model. Each iteration processes one training sample and performs these five steps:\n",
    "\n",
    "**1. Zero Gradients:** Before computing new gradients, reset all parameter gradients to 0. (Otherwise, gradients would accumulate from previous iterations!)\n",
    "\n",
    "```python\n",
    "for p in params:\n",
    "    p['grad'] = 0.0\n",
    "```\n",
    "\n",
    "**2. Forward Pass:** Feed the input through the network to get predictions.\n",
    "\n",
    "```python\n",
    "outputs = forward(model, x)\n",
    "```\n",
    "\n",
    "**3. Compute Loss:** Measure how wrong the prediction was.\n",
    "\n",
    "```python\n",
    "loss = cross_entropy_loss(outputs, target)\n",
    "```\n",
    "\n",
    "**4. Backward Pass:** Compute gradients of the loss w.r.t. all parameters.\n",
    "\n",
    "```python\n",
    "backward(loss)\n",
    "```\n",
    "\n",
    "**5. Update Parameters (SGD):** Nudge each parameter in the direction that reduces the loss.\n",
    "\n",
    "```python\n",
    "for p in params:\n",
    "    p['data'] -= learning_rate * p['grad']\n",
    "```\n",
    "\n",
    "This is **Stochastic Gradient Descent (SGD)** — \"stochastic\" because we update after each individual sample (rather than computing the average gradient over the entire dataset).\n",
    "\n",
    "**The learning rate** controls how big each update step is. Too large → the model overshoots and diverges. Too small → training is painfully slow. `0.01` is a reasonable starting point.\n",
    "\n",
    "**DO NOT CHANGE CODE OUTSIDE OF TODO!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d472169",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model: Dict, X_train: np.ndarray, Y_train: np.ndarray, \n",
    "          X_val: np.ndarray,Y_val: np.ndarray,X_test: np.ndarray, Y_test: np.ndarray,\n",
    "          epochs: int = 5, learning_rate: float = 0.01, \n",
    "          print_every: int = 200):\n",
    "    \"\"\"\n",
    "    Train the MLP using stochastic gradient descent (SGD).\n",
    "    \n",
    "    Args:\n",
    "        model: The MLP dictionary\n",
    "        X_train: Training inputs (784, N)\n",
    "        Y_train: Training labels (10, N) one-hot\n",
    "        X_val: Validation inputs\n",
    "        Y_val: Validation labels\n",
    "        X_test: Test inputs\n",
    "        Y_test: Test labels\n",
    "        epochs: Number of passes through the data\n",
    "        learning_rate: Step size for parameter updates\n",
    "        print_every: Print loss every N samples\n",
    "    \"\"\"\n",
    "    params = get_parameters(model)\n",
    "    num_samples = X_train.shape[1]\n",
    "    # Initialize history dictionaries\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'train_acc': [],\n",
    "        'val_loss': [],\n",
    "        'val_acc': []\n",
    "    }\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0.0\n",
    "        \n",
    "        # Shuffle training data\n",
    "        indices = list(range(num_samples))\n",
    "        random.shuffle(indices)\n",
    "        \n",
    "        for step, idx in enumerate(indices):\n",
    "            # Get single sample\n",
    "            x = X_train[:, idx].tolist()     # Input features\n",
    "            target = int(np.argmax(Y_train[:, idx]))  # True class\n",
    "\n",
    "            # progress bar\n",
    "            pbar = tqdm(enumerate(indices), total=num_samples, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "            # ============================================================\n",
    "            # TODO: Training step\n",
    "            #\n",
    "            # 1. ZERO GRADIENTS: Set grad to 0.0 for all params\n",
    "            #    for p in params:\n",
    "            #        p['grad'] = 0.0\n",
    "            #\n",
    "            # 2. FORWARD PASS: \n",
    "            #    outputs = forward(model, x)\n",
    "            #\n",
    "            # 3. COMPUTE LOSS:\n",
    "            #    loss = cross_entropy_loss(outputs, target)\n",
    "            #\n",
    "            # 4. BACKWARD PASS:\n",
    "            #    backward(loss)\n",
    "            #\n",
    "            # 5. UPDATE PARAMETERS (SGD):\n",
    "            #    for p in params:\n",
    "            #        p['data'] -= learning_rate * p['grad']\n",
    "            # ============================================================\n",
    "            pass\n",
    "            \n",
    "            # Track loss (uncomment after implementing above)\n",
    "            # epoch_loss += loss['data']\n",
    "            # if (step + 1) % print_every == 0:\n",
    "            #     avg_loss = epoch_loss / (step + 1)\n",
    "            #     print(f\"  Epoch {epoch+1}, Step {step+1}/{num_samples}, \"\n",
    "            #           f\"Avg Loss: {avg_loss:.4f}\")\n",
    "        \n",
    "        # Training Metrics\n",
    "        avg_train_loss = epoch_loss / num_samples\n",
    "        train_acc = get_accuracy(model, X_train, Y_train)\n",
    "        \n",
    "        # Validation Metrics \n",
    "        # (Assuming you have a function to calculate total loss over a set)\n",
    "        val_acc = get_accuracy(model, X_val, Y_val)\n",
    "        # For val_loss, we'll approximate using the average of a few samples or the full set\n",
    "        val_loss = get_avg_loss(model, X_val, Y_val) \n",
    "        \n",
    "        # Store in history\n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        \n",
    "        tqdm.write(f\"Epoch {epoch+1}: Train Loss: {avg_train_loss:.4f}, Val Acc: {val_acc:.1f}%\")\n",
    "    \n",
    "    return history\n",
    "\n",
    "def get_avg_loss(model, X, Y):\n",
    "    \"\"\"Helper to get average loss over a dataset.\"\"\"\n",
    "    total_loss = 0\n",
    "    num_samples = X.shape[1]\n",
    "    for i in range(num_samples):\n",
    "        x = X[:, i].tolist()\n",
    "        target = int(np.argmax(Y[:, i]))\n",
    "        outputs = forward(model, x)\n",
    "        loss = cross_entropy_loss(outputs, target)\n",
    "        total_loss += loss['data']\n",
    "    return total_loss / num_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4: Train the model! (10 points)\n",
    "\n",
    "Now let's actually train! We create a network with architecture `784 → 16 → 10`:\n",
    "- **784** input neurons (one per pixel)\n",
    "- **16** hidden neurons with ReLU activation\n",
    "- **10** output neurons with softmax activation (one per digit class)\n",
    "\n",
    "**What to expect:**\n",
    "- The scalar implementation processes ~5-20 samples/second\n",
    "- Each epoch takes ~2 minutes on 400 samples\n",
    "- 5 epochs total ≈ ~10 minutes\n",
    "- You should see test accuracy above **75%** when training finishes\n",
    "\n",
    "**Patience is a virtue here!** The scalar approach is intentionally slow so you can see every operation happening. The vectorized version (Part 7) will do this in under 5 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaed955f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a small MLP: 784 -> 16 -> 10\n",
    "# Using a small hidden layer because scalar operations are slow\n",
    "model = initialize_model(784, [16, 10], activations=['relu', 'softmax'])\n",
    "params = get_parameters(model)\n",
    "print(f\"Model: 784 -> 16 -> 10\")\n",
    "print(f\"Total parameters: {len(params)}\")\n",
    "print(f\"Training on {X_train.shape[1]} samples...\\n\")\n",
    "\n",
    "# Train! (this will take a few minutes)\n",
    "loss_history = train(\n",
    "    model, X_train, Y_train_oh, X_test, X_val, Y_val_oh, Y_test_oh,\n",
    "    epochs=5,\n",
    "    learning_rate=0.01,\n",
    "    print_every=200\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Training Results\n",
    "\n",
    "The plots below show how loss and accuracy evolve during training. You should see:\n",
    "- **Loss decreasing** over epochs (the model is getting better at predicting)\n",
    "- **Accuracy increasing** over epochs\n",
    "- If validation loss starts increasing while training loss decreases, that's **overfitting** — the model is memorizing the training data rather than learning general patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c241152",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_results(history):\n",
    "    epochs = range(1, len(history['train_loss']) + 1)\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "    ax1.plot(epochs, history['train_loss'], 'b-o', label='Train Loss')\n",
    "    ax1.plot(epochs, history['val_loss'], 'r-o', label='Val Loss')\n",
    "    ax1.set_title('Training & Validation Loss')\n",
    "    ax1.set_xlabel('Epochs')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "\n",
    "    ax2.plot(epochs, history['train_acc'], 'b-o', label='Train Acc')\n",
    "    ax2.plot(epochs, history['val_acc'], 'r-o', label='Val Acc')\n",
    "    ax2.set_title('Training & Validation Accuracy')\n",
    "    ax2.set_xlabel('Epochs')\n",
    "    ax2.set_ylabel('Accuracy (%)')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_training_results(loss_history)\n",
    "\n",
    "final_test_acc = get_accuracy(model, X_test, Y_test_oh)\n",
    "print(f\"Final Test Accuracy: {final_test_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Predictions\n",
    "\n",
    "Let's look at some actual predictions! Green titles mean the model got it right, red means it got it wrong. Even with a tiny network and limited data, you should see many correct predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff7340f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some predictions\n",
    "fig, axes = plt.subplots(2, 5, figsize=(14, 6))\n",
    "for i in range(10):\n",
    "    ax = axes[i // 5][i % 5]\n",
    "    idx = i\n",
    "    x = X_test[:, idx].tolist()\n",
    "    true_label = int(np.argmax(Y_test_oh[:, idx]))\n",
    "    \n",
    "    outputs = forward(model, x)\n",
    "    pred_label = int(np.argmax([o['data'] for o in outputs]))\n",
    "    confidence = max(o['data'] for o in outputs)\n",
    "    \n",
    "    ax.imshow(X_test[:, idx].reshape(28, 28), cmap='gray')\n",
    "    color = 'green' if pred_label == true_label else 'red'\n",
    "    ax.set_title(f\"Pred: {pred_label} (True: {true_label})\", color=color)\n",
    "    ax.axis('off')\n",
    "plt.suptitle(\"Model Predictions (Green=Correct, Red=Wrong)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5: Improve the model (10 points)\n",
    "\n",
    "Now it's your turn to experiment. Can you beat the baseline accuracy?\n",
    "\n",
    "**Hyperparameters to try tuning:**\n",
    "- **Learning rate:** Try values like 0.001, 0.005, 0.01, 0.05, 0.1. Larger learning rates train faster but may overshoot; smaller ones are more stable but slower.\n",
    "- **Number of epochs:** More epochs = more passes through the data = potentially better accuracy (but watch for overfitting!)\n",
    "- **Training size:** More data generally helps. Try increasing `num_train`.\n",
    "\n",
    "**Architecture changes:**\n",
    "- **More hidden units:** Try `[32, 10]` or `[64, 10]` instead of `[16, 10]` (but beware: larger networks take longer to train with our scalar implementation)\n",
    "- **More layers:** Try `[32, 16, 10]` for a 3-layer network. More layers can learn more complex features, but are harder to train.\n",
    "- **Different activations:** Try `sigmoid` instead of `relu` for hidden layers. You could also experiment with `linear` (though this usually performs worse because it removes the non-linearity that makes deep networks powerful).\n",
    "\n",
    "**Do not change the output layer activation** — it must remain `softmax` for multi-class classification to work correctly.\n",
    "\n",
    "**Target:** Your custom model should achieve around **~82%** test accuracy for full credit. **Plot the training and accuracy curves** for your improved model. **Print out the test accuracy and accuracy plots and keep them displayed when submitting.**\n",
    "\n",
    "**Note:** A larger model may take significantly longer to train. Plan accordingly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213d620f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"part7\"></a>\n",
    "## Part 7 (Extra Credit): Vectorized Implementation (20 points)\n",
    "\n",
    "### Why Vectorize?\n",
    "\n",
    "Our scalar implementation processes **one number at a time** — each weight, each multiplication, each addition is a separate operation. For a network with 12,000+ parameters processing 400 samples, that's millions of individual Python operations per epoch. Python is slow at this!\n",
    "\n",
    "A **vectorized** implementation uses NumPy to process **entire layers at once** using matrix operations. Instead of 784 individual multiply-and-add operations per neuron, we do a single matrix multiplication:\n",
    "\n",
    "```\n",
    "Scalar (slow):                    Vectorized (fast):\n",
    "for each neuron:                  Z = W @ A + b      ← one operation!\n",
    "    for each input:               A = relu(Z)        ← one operation!\n",
    "        act += w[i] * x[i]\n",
    "    act = relu(act)\n",
    "```\n",
    "\n",
    "NumPy's matrix operations run in optimized C/Fortran code and can leverage CPU vectorization (SIMD instructions). This makes the vectorized version **100-1000x faster**.\n",
    "\n",
    "### What Changes?\n",
    "\n",
    "| Aspect | Scalar Version | Vectorized Version |\n",
    "|--------|---------------|-------------------|\n",
    "| `data` type | `float` | `np.ndarray` |\n",
    "| Weights | List of Value scalars | Single Value matrix |\n",
    "| Forward pass | Loop over neurons & inputs | `Z = W @ A + b` |\n",
    "| Backward pass | Chain rule on scalars | Matrix calculus |\n",
    "| Batch processing | One sample at a time | All samples at once |\n",
    "\n",
    "### Key Insight: Matrix Calculus for Backprop\n",
    "\n",
    "For the linear transformation $Z = W \\cdot A + b$:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial W} = \\frac{\\partial L}{\\partial Z} \\cdot A^T, \\quad \\frac{\\partial L}{\\partial A} = W^T \\cdot \\frac{\\partial L}{\\partial Z}, \\quad \\frac{\\partial L}{\\partial b} = \\text{sum over samples of } \\frac{\\partial L}{\\partial Z}$$\n",
    "\n",
    "This is the matrix version of our scalar chain rule!\n",
    "\n",
    "### Task 7.1: Implement vectorized `create_value` (3 points)\n",
    "\n",
    "Same concept as the scalar version, but `data` and `grad` are now numpy arrays instead of floats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcacc6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_value_vec(data, _children=None, _op=''):\n",
    "    \"\"\"\n",
    "    Create a vectorized Value node.\n",
    "    \n",
    "    Same idea as scalar version, but data and grad are numpy arrays.\n",
    "    \"\"\"\n",
    "    if _children is None:\n",
    "        _children = ()\n",
    "    \n",
    "    # ============================================================\n",
    "    # TODO: Same as scalar create_value, but:\n",
    "    #   - Convert data to np.array with dtype=np.float64\n",
    "    #   - Initialize grad as np.zeros_like(data)\n",
    "    #   - Add '_id': id(data) for cycle detection in backward\n",
    "    # ============================================================\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 7.2: Implement vectorized operations (7 points)\n",
    "\n",
    "The key operations for the vectorized version. The main challenge is handling **broadcasting** in the backward pass — when shapes don't match during addition (like adding a bias vector to a matrix), NumPy broadcasts automatically. But during backprop, we need to sum over the broadcasted dimensions to get the correct gradient shape.\n",
    "\n",
    "**Matrix multiply (`matmul`):**\n",
    "- Forward: `out = A @ B`\n",
    "- Backward: `dA = dout @ B^T`, `dB = A^T @ dout`\n",
    "\n",
    "**Add with broadcasting:**\n",
    "- Forward: `out = a + b` (NumPy handles broadcasting)\n",
    "- Backward: Sum over any dimensions that were broadcasted\n",
    "\n",
    "**ReLU (element-wise):**\n",
    "- Forward: `out = np.maximum(0, a)`\n",
    "- Backward: `da = (out > 0) * dout` (element-wise mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783cce7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def matmul_vec(a, b):\n",
    "    \"\"\"Matrix multiplication: out = a @ b\"\"\"\n",
    "    out = create_value_vec(np.dot(a['data'], b['data']), (a, b), '@')\n",
    "    \n",
    "    def _backward():\n",
    "        # ============================================================\n",
    "        # TODO: Implement backward for matrix multiply\n",
    "        # a.grad += dout @ b.T\n",
    "        # b.grad += a.T @ dout\n",
    "        # ============================================================\n",
    "        pass\n",
    "    \n",
    "    out['_backward'] = _backward\n",
    "    return out\n",
    "\n",
    "def add_vec(a, b):\n",
    "    \"\"\"Addition with broadcasting support.\"\"\"\n",
    "    out = create_value_vec(a['data'] + b['data'], (a, b), '+')\n",
    "    \n",
    "    def _backward():\n",
    "        # ============================================================\n",
    "        # TODO: Implement backward for add\n",
    "        # Handle broadcasting: sum over dimensions that were broadcasted\n",
    "        # \n",
    "        # For each input (a and b):\n",
    "        #   1. Start with grad = out['grad']\n",
    "        #   2. If grad has more dims than the input, sum over extra leading dims\n",
    "        #   3. If any dim of input is 1, sum over that dim (keepdims=True)\n",
    "        # ============================================================\n",
    "        pass\n",
    "    \n",
    "    out['_backward'] = _backward\n",
    "    return out\n",
    "\n",
    "def relu_vec(a):\n",
    "    \"\"\"ReLU: out = np.maximum(0, a)\"\"\"\n",
    "    out = create_value_vec(np.maximum(0, a['data']), (a,), 'ReLU')\n",
    "    \n",
    "    def _backward():\n",
    "        # ============================================================\n",
    "        # TODO: Gradient flows where input was positive\n",
    "        # a['grad'] += (out['data'] > 0) * out['grad']\n",
    "        # ============================================================\n",
    "        pass\n",
    "    \n",
    "    out['_backward'] = _backward\n",
    "    return out\n",
    "\n",
    "def sigmoid_vec(a):\n",
    "    \"\"\"Sigmoid activation for arrays.\"\"\"\n",
    "    sigmoid_data = 1 / (1 + np.exp(-np.clip(a['data'], -500, 500)))\n",
    "    out = create_value_vec(sigmoid_data, (a,), 'sigmoid')\n",
    "    \n",
    "    def _backward():\n",
    "        # ============================================================\n",
    "        # TODO: a['grad'] += out['data'] * (1 - out['data']) * out['grad']\n",
    "        # ============================================================\n",
    "        pass\n",
    "    \n",
    "    out['_backward'] = _backward\n",
    "    return out\n",
    "\n",
    "def softmax_vec(a):\n",
    "    \"\"\"Softmax for arrays (column-wise).\"\"\"\n",
    "    shifted = a['data'] - np.max(a['data'], axis=0, keepdims=True)\n",
    "    exp_data = np.exp(shifted)\n",
    "    softmax_data = exp_data / np.sum(exp_data, axis=0, keepdims=True)\n",
    "    out = create_value_vec(softmax_data, (a,), 'softmax')\n",
    "    \n",
    "    def _backward():\n",
    "        # Simplified: pass gradient through (combined with cross-entropy)\n",
    "        a['grad'] += out['grad']\n",
    "    \n",
    "    out['_backward'] = _backward\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 7.3: Implement vectorized model and training (10 points)\n",
    "\n",
    "Now we assemble everything into a vectorized model. The key differences from the scalar version:\n",
    "\n",
    "- **Parameters are matrices:** Instead of each neuron having a list of weight Values, we have one weight *matrix* Value per layer\n",
    "- **`forward_vec` processes all samples at once:** `Z = W @ A + b` computes the output for every sample in the batch simultaneously\n",
    "- **Fused softmax + cross-entropy:** The gradient of softmax combined with cross-entropy simplifies beautifully to `(predictions - true_labels) / m`. This avoids computing the complex Jacobian of softmax individually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797d0897",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_vec(loss):\n",
    "    \"\"\"Backpropagation for vectorized graph.\"\"\"\n",
    "    # ============================================================\n",
    "    # TODO: Same as scalar backward, but use np.ones_like for \n",
    "    #       initializing loss gradient\n",
    "    # ============================================================\n",
    "    pass\n",
    "\n",
    "def zero_grad_vec(values):\n",
    "    \"\"\"Zero all gradients.\"\"\"\n",
    "    for v in values:\n",
    "        v['grad'] = np.zeros_like(v['data'])\n",
    "\n",
    "def initialize_model_vec(layer_sizes, activations=None, learning_rate=0.01):\n",
    "    \"\"\"\n",
    "    Create a vectorized MLP.\n",
    "    \n",
    "    Parameters are stored as full matrices (not individual scalars).\n",
    "    \n",
    "    Args:\n",
    "        layer_sizes: [input_size, hidden1, hidden2, ..., output_size]\n",
    "        activations: Activation for each layer transition\n",
    "        learning_rate: Learning rate\n",
    "    \"\"\"\n",
    "    if activations is None:\n",
    "        activations = ['relu'] * (len(layer_sizes) - 2) + ['sigmoid']\n",
    "    \n",
    "    mlp = {\n",
    "        'layer_sizes': layer_sizes,\n",
    "        'activations': activations,\n",
    "        'learning_rate': learning_rate,\n",
    "        'num_layers': len(layer_sizes),\n",
    "        'parameters': []\n",
    "    }\n",
    "    \n",
    "    for i in range(1, len(layer_sizes)):\n",
    "        fan_in, fan_out = layer_sizes[i-1], layer_sizes[i]\n",
    "        std_dev = np.sqrt(2.0 / (fan_in + fan_out))\n",
    "        \n",
    "        # ============================================================\n",
    "        # TODO: Create weight matrix W and bias vector b as Value nodes\n",
    "        #   W = create_value_vec(np.random.randn(fan_out, fan_in) * std_dev)\n",
    "        #   b = create_value_vec(np.zeros((fan_out, 1)))\n",
    "        #   Append both to mlp['parameters']\n",
    "        # ============================================================\n",
    "        pass\n",
    "    \n",
    "    return mlp\n",
    "\n",
    "def forward_vec(mlp, X):\n",
    "    \"\"\"Vectorized forward pass: processes all samples at once!\"\"\"\n",
    "    A = create_value_vec(X)\n",
    "    cache = [A]\n",
    "    params = mlp['parameters']\n",
    "    \n",
    "    for i in range(mlp['num_layers'] - 1):\n",
    "        W = params[2*i]\n",
    "        b = params[2*i + 1]\n",
    "        \n",
    "        # ============================================================\n",
    "        # TODO: Z = W @ A + b, then apply activation\n",
    "        # ============================================================\n",
    "        pass\n",
    "        \n",
    "        cache.append(A)\n",
    "    \n",
    "    return A, cache\n",
    "\n",
    "def categorical_crossentropy_loss_vec(pred, Y):\n",
    "    \"\"\"\n",
    "    Fused softmax + cross-entropy loss (vectorized).\n",
    "    \n",
    "    Uses the mathematical shortcut: gradient of (softmax + CE) w.r.t. \n",
    "    logits Z is simply (predictions - true_labels) / m\n",
    "    \"\"\"\n",
    "    Z = pred['_prev'][0]  # Input to softmax\n",
    "    m = Y.shape[1]\n",
    "    \n",
    "    cost = -1/m * np.sum(Y * np.log(pred['data'] + 1e-8))\n",
    "    loss = create_value_vec(cost, (pred,), 'fused_loss')\n",
    "    \n",
    "    def _backward():\n",
    "        Z['grad'] += (pred['data'] - Y) / m\n",
    "    \n",
    "    loss['_backward'] = _backward\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 7.4: Train the vectorized model\n",
    "\n",
    "Now we can use a **much bigger** network and **much more** data! The architecture below (`784 → 128 → 64 → 10`) has over 100,000 parameters and trains on 10,000 samples — this would take days with the scalar version but completes in seconds here.\n",
    "\n",
    "**This should be dramatically faster than the scalar version** — expect the entire training run to finish in under a minute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365187e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_vec(mlp, X_train, Y_train, X_test, Y_test,\n",
    "              epochs=10, batch_size=32, print_every=1):\n",
    "    \"\"\"\n",
    "    Train vectorized MLP with mini-batch gradient descent.\n",
    "    \"\"\"\n",
    "    params = mlp['parameters']\n",
    "    lr = mlp['learning_rate']\n",
    "    num_samples = X_train.shape[1]\n",
    "    loss_history = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Shuffle\n",
    "        perm = np.random.permutation(num_samples)\n",
    "        X_shuffled = X_train[:, perm]\n",
    "        Y_shuffled = Y_train[:, perm]\n",
    "        \n",
    "        epoch_loss = 0.0\n",
    "        num_batches = num_samples // batch_size\n",
    "        \n",
    "        for batch_idx in range(num_batches):\n",
    "            start = batch_idx * batch_size\n",
    "            end = start + batch_size\n",
    "            X_batch = X_shuffled[:, start:end]\n",
    "            Y_batch = Y_shuffled[:, start:end]\n",
    "            \n",
    "            # ============================================================\n",
    "            # TODO:\n",
    "            # 1. Zero gradients: zero_grad_vec(params)\n",
    "            # 2. Forward: pred, cache = forward_vec(mlp, X_batch)\n",
    "            # 3. Loss: loss = categorical_crossentropy_loss_vec(pred, Y_batch)\n",
    "            # 4. Backward: backward_vec(loss)\n",
    "            # 5. Update: for p in params: p['data'] -= lr * p['grad']\n",
    "            # ============================================================\n",
    "            pass\n",
    "            \n",
    "            # epoch_loss += loss['data']\n",
    "        \n",
    "        avg_loss = epoch_loss / max(num_batches, 1)\n",
    "        loss_history.append(avg_loss)\n",
    "        \n",
    "        if (epoch + 1) % print_every == 0:\n",
    "            # Compute accuracy\n",
    "            pred_test, _ = forward_vec(mlp, X_test)\n",
    "            pred_labels = np.argmax(pred_test['data'], axis=0)\n",
    "            true_labels = np.argmax(Y_test, axis=0)\n",
    "            acc = np.mean(pred_labels == true_labels) * 100\n",
    "            print(f\"Epoch {epoch+1}/{epochs} — Loss: {avg_loss:.4f}, \"\n",
    "                  f\"Test Acc: {acc:.1f}%\")\n",
    "    \n",
    "    return loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9576944",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train vectorized model\n",
    "# Now we can use a bigger network and more data!\n",
    "num_train_vec = 10000\n",
    "num_test_vec = 2000\n",
    "\n",
    "X_train_v = X_all[:num_train_vec].T\n",
    "Y_train_v = to_one_hot(Y_all[:num_train_vec])\n",
    "X_test_v = X_all[60000:60000+num_test_vec].T\n",
    "Y_test_v = to_one_hot(Y_all[60000:60000+num_test_vec])\n",
    "\n",
    "mlp_vec = initialize_model_vec(\n",
    "    [784, 128, 64, 10], \n",
    "    activations=['relu', 'relu', 'softmax'],\n",
    "    learning_rate=0.1\n",
    ")\n",
    "\n",
    "print(f\"Vectorized Model: 784 -> 128 -> 64 -> 10\")\n",
    "print(f\"Training on {num_train_vec} samples...\\n\")\n",
    "\n",
    "loss_history_vec = train_vec(\n",
    "    mlp_vec, X_train_v, Y_train_v, X_test_v, Y_test_v,\n",
    "    epochs=20, batch_size=64, print_every=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85052247",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot vectorized training\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(loss_history_vec, 'r-o')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Vectorized MLP Training Loss')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Submission Checklist\n",
    "\n",
    "Before submitting, make sure:\n",
    "\n",
    "- [ ] **Part 1:** `create_value` creates proper Value nodes *(provided — just make sure you understand it!)*\n",
    "- [ ] **Part 2:** All operations (`add`, `multiply`, `power`, `relu`, `sigmoid`, `log_op`, `exp_op`) work correctly — forward AND backward\n",
    "- [ ] **Part 3:** `initialize_model` and `forward` are implemented correctly\n",
    "- [ ] **Part 4:** `cross_entropy_loss` computes the correct loss\n",
    "- [ ] **Part 5:** `backward` correctly computes gradients via backpropagation *(provided)*\n",
    "- [ ] **Part 6:** \n",
    "  - `get_accuracy` correctly computes classification accuracy\n",
    "  - Training loop is implemented and runs successfully\n",
    "  - Base model achieves >75% test accuracy\n",
    "  - Improved model achieves ~82% test accuracy with training curves plotted\n",
    "- [ ] **(Extra Credit) Part 7:** Vectorized implementation trains successfully\n",
    "\n",
    "**Grading:**\n",
    "| Part | Points |\n",
    "|------|--------|\n",
    "| Part 1: Value Nodes | 0 (provided) |\n",
    "| Part 2: Operations (forward + backward) | 15 |\n",
    "| Part 3: Softmax, Model Init, Forward | 30 |\n",
    "| Part 4: Cross-Entropy Loss | 5 |\n",
    "| Part 5: Backpropagation | 0 (provided) |\n",
    "| Part 6: MNIST Training & Improvement | 50 |\n",
    "| **Total** | **100** |\n",
    "| Part 7: Vectorized (Extra Credit) | +20 |\n",
    "\n",
    "**Remember:** No classes allowed! Only dictionaries and lists."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
