{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc8332aa",
   "metadata": {},
   "source": [
    "# Programming Assignment 3: N-gram Language Models\n",
    "## Building Language Models from Scratch on Tiny Shakespeare\n",
    "\n",
    "**Objectives:**\n",
    "- Understand text preprocessing and tokenization for language modeling\n",
    "- Implement a **Unigram** model (frequency-based word generation)\n",
    "- Implement a **Bigram** model (next-word prediction with one word of context)\n",
    "- Generalize to an **N-gram** model with Laplace smoothing\n",
    "- Evaluate language models using **perplexity**\n",
    "- Generate Shakespeare-like text from your trained models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2178d028",
   "metadata": {},
   "source": [
    "---\n",
    "## Background: What is a Language Model?\n",
    "\n",
    "### The Big Picture\n",
    "\n",
    "A **language model** assigns probabilities to sequences of words. Given a sentence, a language model tells you how likely that sentence is. Given the start of a sentence, it can predict what comes next.\n",
    "\n",
    "This task is foundational for most modern Natural Langauge Processing (NLP) tasks: from phone autocomplete to ChatGPT. Before neural networks took over, **N-gram models** were the dominant approach, and they remain an excellent way to understand the core concepts.\n",
    "\n",
    "### From Counting to Predicting\n",
    "\n",
    "The simplest idea: **estimate word probabilities by counting**. If you read all of Shakespeare and count how often each word appears, you get a basic language model. The word \"the\" appears very frequently, while \"zephyr\" is rare. This is a **unigram model** which as the probability distribution:\n",
    "\n",
    "$$P(w_i) = \\frac{\\text{count}(w_i)}{\\sum_{w}\\text{count}(w)}$$\n",
    "\n",
    "In english, the probability of predicting the next word to be $w_i$ is the number of occurances of word $w_i$ divided by the total number of words in our corpus/vocabulary. \n",
    "\n",
    "But language has *structure*. Justing counting the occurances of each word completely removes the context that is embedded in language. For example, after \"to\", the word \"be\" is much more likely than \"the\". A **bigram model** captures this by looking at pairs of consecutive words:\n",
    "\n",
    "$$P(\\text{be} | \\text{to}) = \\frac{\\text{count}(\\text{\"to be\"})}{\\text{count}(\\text{\"to\"})}$$\n",
    "\n",
    "In this bigram model example, the probability that the next word is \"be\" given that the previous word is \"to\" is the number of occurances in which we found the \"be\" to be after \"to\" in the corpus divided by the number of occurnaces of the word \"to\".\n",
    "\n",
    "We can generalize to **N-grams** — conditioning on the previous $(N-1)$ words:\n",
    "\n",
    "$$P(w_i | w_{i-n+1}, \\ldots, w_{i-1}) = \\frac{\\text{count}(w_{i-n+1}, \\ldots, w_i)}{\\text{count}(w_{i-n+1}, \\ldots, w_{i-1})}$$\n",
    "\n",
    "Where the model predicts the next word to be $w_i$ given the previous N-1 words to be the sum of the observing the N length sequence normalized by observing the N-1 sequence (excluding $w_i$).\n",
    "\n",
    "### The Sparsity Problem\n",
    "\n",
    "While we can create Here's the catch: as $N$ grows, most possible N-grams **never appear** in the training data. Shakespeare never wrote \"thou doth yeet\" but should our model assign it probability *zero*? That seems too harsh. One solution is smoothing, which effectively redistributes a tiny probability to unseen events.\n",
    "\n",
    "### Evaluating Language Models: Perplexity\n",
    "\n",
    "How do we measure if one language model is better than another? We use **perplexity**:\n",
    "\n",
    "$$\\text{Perplexity} = \\exp\\left(-\\frac{1}{N} \\sum_{i=1}^{N} \\log P(w_i | \\text{context})\\right)$$\n",
    "\n",
    "**Note:** Context may vary depending on which n-gram model used. For unigram, there is no context and the term decomposes to $\\log P(w_i)$. For bigrams, the context is simply $w_{i-1}$ leading to the term becoming  $\\log P(w_i | w_{i-1})$. For n-grams where $n > 2$, the context is the n-1 words which came before it.\n",
    "\n",
    "**Intuition:** Perplexity measures how \"surprised\" the model is by the test data. Lower perplexity = better model. A perplexity of $k$ roughly means the model is as uncertain as if it were choosing uniformly among $k$ words at each step.\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d4df30",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Part 1: Text Preprocessing and Tokenization](#part1)\n",
    "2. [Part 2: Unigram Language Model](#part2)\n",
    "3. [Part 3: Bigram Language Model](#part3)\n",
    "4. [Part 4 (Extra Credit): General N-gram Model](#part4)\n",
    "\n",
    "**Note:** Some parts may be already implemented which will have **(Done for you)** and would be worth **0 points**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689ca466",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Run the cells below to install dependencies and import libraries. You only need to run the `pip install` cell once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f5c3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (run once and then comment out)\n",
    "!pip install numpy matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8291478c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import random\n",
    "import math\n",
    "from collections import Counter, defaultdict\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "random.seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba7d50e",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"part1\"></a>\n",
    "## Part 1: Text Preprocessing and Tokenization (10 points)\n",
    "\n",
    "### The Big Picture\n",
    "\n",
    "Before building any model, we need to transform raw text into a clean sequence of tokens. Shakespeare's text contains stage directions (`[Enter ROMEO]`), character name prefixes (`ROMEO:`), and mixed punctuation — none of which help our language model learn word patterns.\n",
    "\n",
    "Our preprocessing pipeline will:\n",
    "1. Normalize the text (lowercase)\n",
    "2. Remove non-speech content (stage directions, character names)\n",
    "3. Split into sentences\n",
    "4. Tokenize each sentence with special boundary markers\n",
    "\n",
    "### 1.0: Load the Dataset\n",
    "\n",
    "The Tiny Shakespeare dataset contains ~1.1 million characters from various Shakespeare plays. We download it from Andrej Karpathy's GitHub repository. (Done for you)\n",
    "\n",
    "**Note:** This creates a text file called tiny_shakespeare.txt and so you don't need to call this cell again after the first time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad7dd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import os\n",
    "\n",
    "data_path = \"tiny_shakespeare.txt\"\n",
    "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "\n",
    "# 1. Check if the local file is available\n",
    "if os.path.exists(data_path):\n",
    "    print(f\"Local copy found at '{data_path}'. Loading from disk...\")\n",
    "    with open(data_path, 'r', encoding='utf-8') as f:\n",
    "        raw_text = f.read()\n",
    "\n",
    "# 2. If not, load via URL\n",
    "else:\n",
    "    print(f\"Local copy not found. Loading via URL...\")\n",
    "    try:\n",
    "        # Fetch the data straight from the web\n",
    "        with urllib.request.urlopen(url) as response:\n",
    "            raw_text = response.read().decode('utf-8')\n",
    "        \n",
    "        # Save a local copy for the next time the script runs\n",
    "        with open(data_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(raw_text)\n",
    "        print(f\"Successfully loaded and saved a fallback copy to '{data_path}'.\")\n",
    "        \n",
    "    except urllib.error.URLError as e:\n",
    "        raise Exception(\n",
    "            f\"Failed to load dataset from URL. Please check your connection or the URL. Error: {e}\"\n",
    "        )\n",
    "\n",
    "# 3. Print out the stats to verify\n",
    "print(f\"\\nDataset size: {len(raw_text):,} characters\")\n",
    "print(f\"\\nFirst 500 characters:\\n{'='*50}\")\n",
    "print(raw_text[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e872afa",
   "metadata": {},
   "source": [
    "### Task 1.1: Implement `preprocess_text` (Done for you)\n",
    "\n",
    "This function transforms raw Shakespeare text into a list of clean sentences. Each sentence should contain only lowercase alphabetic words.\n",
    "\n",
    "**The preprocessing steps:**\n",
    "\n",
    "```\n",
    "Raw text:\n",
    "    \"ROMEO: O, she doth teach the torches to burn bright!\n",
    "     [Aside] It seems she hangs upon the cheek of night.\"\n",
    "\n",
    "After preprocessing:\n",
    "    [\"she doth teach the torches to burn bright\",\n",
    "     \"it seems she hangs upon the cheek of night\"]\n",
    "```\n",
    "\n",
    "**Step-by-step:**\n",
    "\n",
    "| Step | Operation | Tool |\n",
    "|------|-----------|------|\n",
    "| 1 | Convert to lowercase | `text.lower()` |\n",
    "| 2 | Remove stage directions `[...]` | `re.sub(r'\\[.*?\\]', ' ', text)` |\n",
    "| 3 | Remove character prefixes (e.g., `romeo:`) | `re.sub()` with `re.MULTILINE` |\n",
    "| 4 | Split into sentences on `.`, `!`, `?` | `re.split(r'[.!?]+', text)` |\n",
    "| 5 | Extract only alphabetic words | `re.findall(r'\\b[a-z]+\\b', sent)` |\n",
    "| 6 | Filter out sentences with < 2 words | `if len(words) >= 2` |\n",
    "\n",
    "**Why remove character names?** Lines like `ROMEO:` are not part of the spoken dialogue and including them would teach our model that \"romeo\" frequently starts sentences, which isn't useful.\n",
    "\n",
    "**Why filter short sentences?** Single-word \"sentences\" (or empty ones) don't provide useful bigram/trigram data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df00a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text: str) -> List[str]:\n",
    "    \"\"\"Preprocess raw Shakespeare text into a list of cleaned sentences.\"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\[.*?\\]', ' ', text)\n",
    "    text = re.sub(r'^[a-z ]+:', '', text, flags=re.MULTILINE)\n",
    "    raw_sentences = re.split(r'[.!?]+', text)\n",
    "    sentences = []\n",
    "    for sent in raw_sentences:\n",
    "        words = re.findall(r'\\b[a-z]+\\b', sent)\n",
    "        if len(words) >= 2:\n",
    "            sentences.append(' '.join(words))\n",
    "    return sentences\n",
    "\n",
    "sentences = preprocess_text(raw_text)\n",
    "\n",
    "print(f\"Preprocessed {len(sentences):,} sentences\") # 12,198 sentences\n",
    "for i, s in enumerate(sentences[:5]):\n",
    "    print(f\"  {i}: {s}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ccd4310",
   "metadata": {},
   "source": [
    "### Task 1.2: Implement `tokenize_sentences` (10 points)\n",
    "\n",
    "Language models need to know where sentences **begin** and **end**. We add special boundary tokens:\n",
    "- `<s>` — Start-of-sentence token\n",
    "- `</s>` — End-of-sentence token\n",
    "\n",
    "**Example:**\n",
    "```\n",
    "Input sentence:  \"to be or not to be\"\n",
    "Tokenized:       ['<s>', 'to', 'be', 'or', 'not', 'to', 'be', '</s>']\n",
    "```\n",
    "\n",
    "**Why boundary tokens?** They let the model learn:\n",
    "- Which words typically **start** sentences (e.g., after `<s>`, \"the\" and \"i\" are common)\n",
    "- Which words typically **end** sentences (e.g., before `</s>`, \"so\" and \"me\" are common)\n",
    "\n",
    "You also need to build a **vocabulary** — mappings between words and integer indices. This is standard practice in NLP and will be useful for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9d6887",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sentences(sentences: List[str]) -> Tuple[List[List[str]], dict, dict]:\n",
    "    \"\"\"\n",
    "    Tokenize sentences and build vocabulary mappings.\n",
    "    \n",
    "    For each sentence string like \"the torches burn bright\":\n",
    "        1. Split into words\n",
    "        2. Add <s> at the beginning and </s> at the end\n",
    "    \n",
    "    Also build:\n",
    "        word_to_idx: dict mapping each unique word to an integer index\n",
    "        idx_to_word: dict mapping each index back to the word\n",
    "    \n",
    "    Args:\n",
    "        sentences: List of preprocessed sentence strings\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (tokenized_sentences, word_to_idx, idx_to_word)\n",
    "    \"\"\"\n",
    "    # ============================================================\n",
    "    # TODO: Implement tokenization\n",
    "    #\n",
    "    # 1. For each sentence, split on spaces and add <s> / </s>\n",
    "    # 2. Collect all unique tokens into a vocabulary set\n",
    "    # 3. Build word_to_idx: sorted vocab -> enumerate\n",
    "    # 4. Build idx_to_word: reverse of word_to_idx\n",
    "    # ============================================================\n",
    "    pass\n",
    "\n",
    "\n",
    "# Test\n",
    "tokenized, word_to_idx, idx_to_word = tokenize_sentences(sentences)\n",
    "\n",
    "assert tokenized[0][0] == '<s>', \"First token should be <s>\"\n",
    "assert tokenized[0][-1] == '</s>', \"Last token should be </s>\"\n",
    "assert '<s>' in word_to_idx, \"Vocabulary should contain <s>\"\n",
    "assert '</s>' in word_to_idx, \"Vocabulary should contain </s>\"\n",
    "\n",
    "print(f\"Vocabulary size: {len(word_to_idx):,}\")\n",
    "print(f\"Total tokens: {sum(len(s) for s in tokenized):,}\")\n",
    "print(f\"\\nSample tokenized sentence: {tokenized[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e1e678",
   "metadata": {},
   "source": [
    "### Task 1.3: Train/Test Split and Exploratory Analysis\n",
    "\n",
    "We split the data into training (80%), validation (10%), and test (10%) sets. This is done for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc13447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data: 80% train, 10% validation, 10% test\n",
    "random.shuffle(tokenized)\n",
    "n = len(tokenized)\n",
    "train_data = tokenized[:int(0.8 * n)]\n",
    "val_data = tokenized[int(0.8 * n):int(0.9 * n)]\n",
    "test_data = tokenized[int(0.9 * n):]\n",
    "\n",
    "print(f\"Train: {len(train_data):,} sentences\")\n",
    "print(f\"Val:   {len(val_data):,} sentences\")\n",
    "print(f\"Test:  {len(test_data):,} sentences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b282709e",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"part2\"></a>\n",
    "## Part 2: Unigram Language Model (40 points)\n",
    "\n",
    "### How the Unigram Model Works\n",
    "\n",
    "A **unigram model** is the simplest possible language model. It assumes every word is generated independently, according to its frequency in the training data:\n",
    "\n",
    "$$P(w_i) = \\frac{\\text{count}(w_i)}{\\sum_{w} \\text{count}(w)}$$\n",
    "\n",
    "**Strengths:** Simple, fast, captures which words are common in Shakespeare.\n",
    "\n",
    "**Weaknesses:** Completely ignores word order. \"To be or not to be\" and \"be not or to to be\" have the same probability. Generated text is just random common words.\n",
    "\n",
    "### What You Need to Implement\n",
    "\n",
    "You will implement the unigram model as a set of **standalone functions** that operate on a shared dictionary:\n",
    "\n",
    "| Function | What It Does |\n",
    "|----------|-------------|\n",
    "| `unigram_train(data)` | Count words, compute probabilities, return model dict |\n",
    "| `unigram_probability(model, word)` | Return P(word) for any word |\n",
    "| `unigram_sentence_log_prob(model, sentence)` | Compute log P(sentence) as sum of log P(word) |\n",
    "| `unigram_perplexity(model, data)` | Evaluate the model on a dataset (Done for you) |\n",
    "| `unigram_generate(model, max_length)` | Sample random sentences from the model |\n",
    "\n",
    "**Key detail:** We count all words *except* `<s>` (since we never generate the start token — it's always given). We *do* count `</s>` since the model needs to learn when to stop.\n",
    "\n",
    "### Task 2.1: Implement `unigram_train` (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac03e92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unigram_train(data: List[List[str]]) -> dict:\n",
    "    \"\"\"\n",
    "    Train a unigram model on tokenized sentences.\n",
    "    \n",
    "    Compute the probability of each word as:\n",
    "        P(w) = count(w) / total_word_count\n",
    "    \n",
    "    Include </s> but exclude <s> from counts.\n",
    "    \n",
    "    Args:\n",
    "        data: List of tokenized sentences (each is a list of strings)\n",
    "    \n",
    "    Returns:\n",
    "        A dictionary with keys:\n",
    "            'word_probs': dict mapping word -> probability\n",
    "            'vocab': set of all words in the vocabulary\n",
    "    \"\"\"\n",
    "    # ============================================================\n",
    "    # TODO: Implement training\n",
    "    # ============================================================\n",
    "    pass\n",
    "\n",
    "\n",
    "# Test\n",
    "unigram_model = unigram_train(train_data)\n",
    "\n",
    "assert isinstance(unigram_model, dict), \"Should return a dict\"\n",
    "assert 'word_probs' in unigram_model, \"Should contain 'word_probs'\"\n",
    "assert 'vocab' in unigram_model, \"Should contain 'vocab'\"\n",
    "assert '</s>' in unigram_model['word_probs'], \"Should contain </s>\"\n",
    "assert '<s>' not in unigram_model['word_probs'], \"Should NOT contain <s>\"\n",
    "\n",
    "print(f\"Vocabulary size: {len(unigram_model['vocab']):,}\")\n",
    "print(f\"\\nTop 10 unigram probabilities:\")\n",
    "top_words = sorted(unigram_model['word_probs'].items(), key=lambda x: -x[1])[:10]\n",
    "for word, prob in top_words:\n",
    "    print(f\"  P({word:10s}) = {prob:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426447f2",
   "metadata": {},
   "source": [
    "### Task 2.2: Implement `unigram_probability` (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de680863",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unigram_probability(model: dict, word: str) -> float:\n",
    "    \"\"\"\n",
    "    Return P(word) from the unigram model.\n",
    "    Return 1e-10 for unknown words.\n",
    "    \n",
    "    Args:\n",
    "        model: The unigram model dictionary from unigram_train()\n",
    "        word: A word string\n",
    "    \n",
    "    Returns:\n",
    "        The probability P(word)\n",
    "    \"\"\"\n",
    "    # ============================================================\n",
    "    # TODO: Implement (~1 line)\n",
    "    # ============================================================\n",
    "    pass\n",
    "\n",
    "\n",
    "# Test\n",
    "assert unigram_probability(unigram_model, 'the') > 0.01, \"Common word should have high prob\"\n",
    "assert unigram_probability(unigram_model, 'xyzzy') == 1e-10, \"Unknown word should return 1e-10\"\n",
    "print(f\"P('the') = {unigram_probability(unigram_model, 'the'):.5f}\")\n",
    "print(f\"P('xyzzy') = {unigram_probability(unigram_model, 'xyzzy')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3533315d",
   "metadata": {},
   "source": [
    "### Task 2.3: Implement `unigram_sentence_log_prob` (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2194a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unigram_sentence_log_prob(model: dict, sentence: List[str]) -> float:\n",
    "    \"\"\"\n",
    "    Compute log P(sentence) = sum of log P(w_i) for each word.\n",
    "    Exclude <s> from the computation.\n",
    "    \n",
    "    Args:\n",
    "        model: The unigram model dictionary\n",
    "        sentence: A tokenized sentence (list of strings)\n",
    "    \n",
    "    Returns:\n",
    "        The log probability of the sentence\n",
    "    \"\"\"\n",
    "    # ============================================================\n",
    "    # TODO: Implement (~2 lines)\n",
    "    # ============================================================\n",
    "    pass\n",
    "\n",
    "\n",
    "# Test\n",
    "test_sent = train_data[0]\n",
    "log_prob = unigram_sentence_log_prob(unigram_model, test_sent)\n",
    "assert log_prob < 0, \"Log probability should be negative\"\n",
    "print(f\"Log P('{' '.join(test_sent)}') = {log_prob:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404b02d8",
   "metadata": {},
   "source": [
    "### `unigram_perplexity` (Done for you)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e739c80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unigram_perplexity(model: dict, data: List[List[str]]) -> float:\n",
    "    total_log_prob = 0\n",
    "    total_tokens = 0\n",
    "    for sentence in data:\n",
    "        total_log_prob += unigram_sentence_log_prob(model, sentence)\n",
    "        total_tokens += len(sentence) - 1\n",
    "    return math.exp(-total_log_prob / total_tokens)\n",
    "\n",
    "train_ppl = unigram_perplexity(unigram_model, train_data)\n",
    "val_ppl = unigram_perplexity(unigram_model, val_data)\n",
    "assert train_ppl > 0, \"Perplexity should be positive\"\n",
    "print(f\"Train perplexity: {train_ppl:.2f}\")\n",
    "print(f\"Val perplexity:   {val_ppl:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b78436a",
   "metadata": {},
   "source": [
    "### Task 2.5: Implement `unigram_generate` (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563d4a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unigram_generate(model: dict, max_length: int = 20) -> str:\n",
    "    \"\"\"\n",
    "    Generate a sentence by sampling words independently from the unigram distribution.\n",
    "    Stop when </s> is generated or max_length is reached.\n",
    "    \n",
    "    Args:\n",
    "        model: The unigram model dictionary\n",
    "        max_length: Maximum number of words to generate\n",
    "    \n",
    "    Returns:\n",
    "        Generated sentence as a string (without <s> and </s>)\n",
    "    \"\"\"\n",
    "    # ============================================================\n",
    "    # TODO: Implement\n",
    "    # 1. Build lists of words and their probabilities from model['word_probs']\n",
    "    # 2. Use np.random.choice(words, p=probs) to sample each word\n",
    "    # 3. Stop at </s> or max_length\n",
    "    # 4. Return ' '.join(generated_words)\n",
    "    # ============================================================\n",
    "    pass\n",
    "\n",
    "\n",
    "# Test\n",
    "print(\"Generated sentences (unigram):\")\n",
    "for i in range(5):\n",
    "    print(f\"  {i+1}. {unigram_generate(unigram_model)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94c6bda",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"part3\"></a>\n",
    "## Part 3: Bigram Language Model (50 points)\n",
    "\n",
    "### How the Bigram Model Works\n",
    "\n",
    "A **bigram model** conditions each word on the immediately preceding word:\n",
    "\n",
    "$$P(w_i | w_{i-1}) = \\frac{\\text{count}(w_{i-1}, w_i)}{\\text{count}(w_{i-1})}$$\n",
    "\n",
    "This captures basic word-to-word transitions. In Shakespeare, after \"thou\", words like \"art\", \"hast\", and \"shalt\" are far more likely than \"computer\" or \"pizza\".\n",
    "\n",
    "**How it builds the graph of transitions:**\n",
    "\n",
    "```\n",
    "Training sentence: ['<s>', 'to', 'be', 'or', 'not', 'to', 'be', '</s>']\n",
    "\n",
    "Bigrams extracted:\n",
    "    (<s>, to)     → count(<s>, to) += 1\n",
    "    (to, be)      → count(to, be) += 1     (counted TWICE)\n",
    "    (be, or)      → count(be, or) += 1\n",
    "    (or, not)     → count(or, not) += 1\n",
    "    (not, to)     → count(not, to) += 1\n",
    "    (to, be)      → count(to, be) += 1     (second occurrence)\n",
    "    (be, </s>)    → count(be, </s>) += 1\n",
    "```\n",
    "\n",
    "### The Zero-Probability Problem\n",
    "\n",
    "Without smoothing, if a bigram like (\"thou\", \"computer\") never appeared in training, we get:\n",
    "\n",
    "$$P(\\text{computer} | \\text{thou}) = \\frac{0}{\\text{count}(\\text{thou})} = 0$$\n",
    "\n",
    "A single zero makes the *entire sentence* probability zero! This is catastrophic for evaluation on new data where unseen bigrams are guaranteed. **Laplace (add-α) smoothing** fixes this:\n",
    "\n",
    "$$P_{\\text{smooth}}(w_i | w_{i-1}) = \\frac{\\text{count}(w_{i-1}, w_i) + \\alpha}{\\text{count}(w_{i-1}) + \\alpha \\cdot |V|}$$\n",
    "\n",
    "This redistributes a small amount of probability mass to every possible bigram, ensuring nothing gets probability zero.\n",
    "\n",
    "### Task 3.1: Implement `bigram_train` (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6d42c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigram_train(data: List[List[str]], smoothing: float = 0.0) -> dict:\n",
    "    \"\"\"\n",
    "    Train a bigram model on tokenized sentences.\n",
    "    \n",
    "    For each consecutive pair (w_{i-1}, w_i) in each sentence:\n",
    "        - Increment bigram_counts[w_{i-1}][w_i]\n",
    "        - Increment unigram_counts[w_{i-1}]\n",
    "    Also collect the full vocabulary.\n",
    "    \n",
    "    Args:\n",
    "        data: List of tokenized sentences\n",
    "        smoothing: Laplace smoothing parameter (alpha). If 0, no smoothing.\n",
    "    \n",
    "    Returns:\n",
    "        A dictionary with keys:\n",
    "            'bigram_counts': defaultdict(Counter) — bigram_counts[context][word] = count\n",
    "            'unigram_counts': Counter — count of each word used as context\n",
    "            'vocab': set of all unique tokens\n",
    "            'smoothing': the smoothing parameter\n",
    "    \"\"\"\n",
    "    # ============================================================\n",
    "    # TODO: Implement training\n",
    "    # ============================================================\n",
    "    pass\n",
    "\n",
    "\n",
    "# Test\n",
    "bigram_model = bigram_train(train_data, smoothing=0.0)\n",
    "\n",
    "assert isinstance(bigram_model, dict), \"Should return a dict\"\n",
    "assert 'bigram_counts' in bigram_model\n",
    "assert 'thou' in bigram_model['bigram_counts'], \"'thou' should appear as a context\"\n",
    "\n",
    "print(f\"Vocabulary size: {len(bigram_model['vocab']):,}\")\n",
    "print(f\"Unique contexts: {len(bigram_model['unigram_counts']):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64309dda",
   "metadata": {},
   "source": [
    "### Task 3.2: Implement `bigram_probability` (20 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2704f1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigram_probability(model: dict, word: str, context: str) -> float:\n",
    "    \"\"\"\n",
    "    Compute P(word | context) with optional Laplace smoothing.\n",
    "    \n",
    "    Without smoothing (alpha=0):\n",
    "        P(w | c) = count(c, w) / count(c)\n",
    "        Return 1e-10 if count is 0.\n",
    "    \n",
    "    With Laplace smoothing:\n",
    "        P(w | c) = (count(c, w) + alpha) / (count(c) + alpha * |V|)\n",
    "    \n",
    "    Args:\n",
    "        model: The bigram model dictionary\n",
    "        word: The word to compute probability for\n",
    "        context: The preceding word\n",
    "    \n",
    "    Returns:\n",
    "        P(word | context)\n",
    "    \"\"\"\n",
    "    # ============================================================\n",
    "    # TODO: Implement probability computation\n",
    "    # Handle both smoothed and unsmoothed cases\n",
    "    # ============================================================\n",
    "    pass\n",
    "\n",
    "\n",
    "# Test\n",
    "print(\"Words most likely to follow 'thou':\")\n",
    "thou_probs = {w: bigram_probability(bigram_model, w, 'thou') \n",
    "              for w in bigram_model['bigram_counts']['thou']}\n",
    "for word, prob in sorted(thou_probs.items(), key=lambda x: -x[1])[:10]:\n",
    "    print(f\"  P({word:12s} | thou) = {prob:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066f2f25",
   "metadata": {},
   "source": [
    "### Task 3.3: Implement `bigram_sentence_log_prob` and `bigram_perplexity` (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee907e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigram_sentence_log_prob(model: dict, sentence: List[str]) -> float:\n",
    "    \"\"\"\n",
    "    Compute log P(sentence) = sum of log P(w_i | w_{i-1}).\n",
    "    \n",
    "    Args:\n",
    "        model: The bigram model dictionary\n",
    "        sentence: A tokenized sentence\n",
    "    \n",
    "    Returns:\n",
    "        Log probability of the sentence\n",
    "    \"\"\"\n",
    "    # ============================================================\n",
    "    # TODO: Implement (~3 lines)\n",
    "    # ============================================================\n",
    "    pass\n",
    "\n",
    "\n",
    "def bigram_perplexity(model: dict, data: List[List[str]]) -> float:\n",
    "    total_log_prob = 0\n",
    "    total_tokens = 0\n",
    "    for sentence in data:\n",
    "        total_log_prob += bigram_sentence_log_prob(model, sentence)\n",
    "        total_tokens += len(sentence) - 1\n",
    "    return math.exp(-total_log_prob / total_tokens)\n",
    "\n",
    "# Test\n",
    "train_ppl = bigram_perplexity(bigram_model, train_data)\n",
    "print(f\"Train perplexity (no smoothing): {train_ppl:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee2e859",
   "metadata": {},
   "source": [
    "### Task 3.4: Implement `bigram_generate` (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb7a474",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigram_generate(model: dict, max_length: int = 30) -> str:\n",
    "    \"\"\"\n",
    "    Generate a sentence word by word, starting from <s>.\n",
    "    \n",
    "    At each step, sample the next word from P(w | previous_word).\n",
    "    Stop at </s> or max_length.\n",
    "    \n",
    "    Args:\n",
    "        model: The bigram model dictionary\n",
    "        max_length: Maximum number of words to generate\n",
    "    \n",
    "    Returns:\n",
    "        Generated sentence as a string (without <s> and </s>)\n",
    "    \"\"\"\n",
    "    # ============================================================\n",
    "    # TODO: Implement\n",
    "    # 1. Start with context = '<s>'\n",
    "    # 2. At each step, get candidates from bigram_counts[context]\n",
    "    # 3. Sample proportional to counts using np.random.choice\n",
    "    # 4. Update context to the sampled word\n",
    "    # 5. Stop at '</s>' or max_length\n",
    "    # ============================================================\n",
    "    pass\n",
    "\n",
    "\n",
    "# Test\n",
    "print(\"Generated sentences (bigram):\")\n",
    "for i in range(5):\n",
    "    print(f\"  {i+1}. {bigram_generate(bigram_model)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5645b8e",
   "metadata": {},
   "source": [
    "### Task 3.5: Smoothing Experiment (Done for you)\n",
    "\n",
    "The following code will hyperparameter tune for laplace smoothing. We will use choose the $\\alpha$ with the lowest perplexity for the heatmap visualization in the next part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbcde9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smoothing experiment\n",
    "alphas = [0.001, 0.01, 0.1, 0.5, 1.0, 2.0]\n",
    "val_perplexities = []\n",
    "\n",
    "for alpha in alphas:\n",
    "    m = bigram_train(train_data, smoothing=alpha)\n",
    "    ppl = bigram_perplexity(m, val_data)\n",
    "    val_perplexities.append(ppl)\n",
    "    print(f\"  alpha={alpha:.3f}  val_ppl={ppl:.2f}\")\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.semilogx(alphas, val_perplexities, 'bo-', linewidth=2, markersize=8)\n",
    "plt.xlabel('Smoothing Parameter (α)', fontsize=12)\n",
    "plt.ylabel('Validation Perplexity', fontsize=12)\n",
    "plt.title('Bigram Model: Smoothing vs. Perplexity', fontsize=14)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "best_idx = np.argmin(val_perplexities)\n",
    "print(f\"\\n Best alpha: {alphas[best_idx]}, Val PPL: {val_perplexities[best_idx]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8585dc4c",
   "metadata": {},
   "source": [
    "### Task 3.6: Visualize Bigram Probabilities (Done for you)\n",
    "\n",
    "This heatmap shows transition probabilities between common words. Study the patterns — you should see that `<s>` transitions mostly to common sentence-starting words, while function words like \"the\" and \"of\" have broad distributions.\n",
    "\n",
    "What are some of the most common bigrams? Do they make sense to you?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e18203b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize bigram probabilities as a heatmap\n",
    "best_bigram = bigram_train(train_data, smoothing=alphas[np.argmin(val_perplexities)])\n",
    "\n",
    "focus_words = ['<s>', 'the', 'thou', 'my', 'and', 'to', 'of', 'is', 'in', 'a',\n",
    "               'not', 'i', 'love', 'shall', 'what', 'his', 'her', '</s>']\n",
    "\n",
    "prob_matrix = np.zeros((len(focus_words), len(focus_words)))\n",
    "for i, ctx in enumerate(focus_words):\n",
    "    for j, word in enumerate(focus_words):\n",
    "        prob_matrix[i, j] = bigram_probability(best_bigram, word, ctx)\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "plt.imshow(prob_matrix, cmap='YlOrRd', aspect='auto')\n",
    "plt.colorbar(label='P(column | row)')\n",
    "plt.xticks(range(len(focus_words)), focus_words, rotation=45, ha='right')\n",
    "plt.yticks(range(len(focus_words)), focus_words)\n",
    "plt.xlabel('Next Word', fontsize=12)\n",
    "plt.ylabel('Context Word', fontsize=12)\n",
    "plt.title('Bigram Transition Probabilities', fontsize=14)\n",
    "\n",
    "for i in range(len(focus_words)):\n",
    "    for j in range(len(focus_words)):\n",
    "        val = prob_matrix[i, j]\n",
    "        if val > 0.01:\n",
    "            plt.text(j, i, f'{val:.2f}', ha='center', va='center', fontsize=7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248535e1",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"part4\"></a>\n",
    "## Part 4 (Extra Credit): General N-gram Language Model (20 points)\n",
    "\n",
    "### Generalizing Beyond Bigrams\n",
    "\n",
    "Now we extend to **N-grams**, where each word is conditioned on the previous $(N-1)$ words:\n",
    "\n",
    "$$P(w_i | w_{i-n+1}, \\ldots, w_{i-1}) = \\frac{\\text{count}(w_{i-n+1}, \\ldots, w_i)}{\\text{count}(w_{i-n+1}, \\ldots, w_{i-1})}$$\n",
    "\n",
    "**The context is now a tuple** rather than a single word. For a trigram model ($N=3$):\n",
    "\n",
    "```\n",
    "Sentence: ['<s>', '<s>', 'to', 'be', 'or', 'not', '</s>']\n",
    "                  ^^^^ padded with extra <s>\n",
    "\n",
    "Trigrams:\n",
    "    context=('<s>', '<s>')  → word='to'\n",
    "    context=('<s>', 'to')   → word='be'\n",
    "    context=('to', 'be')    → word='or'\n",
    "    context=('be', 'or')    → word='not'\n",
    "    context=('or', 'not')   → word='</s>'\n",
    "```\n",
    "\n",
    "**Important:** We pad the beginning of each sentence with $(N-1)$ `<s>` tokens so that even the first word has a full context window.\n",
    "\n",
    "### The Context-Length Trade-off\n",
    "\n",
    "| N | Context | Captures | Problem |\n",
    "|---|---------|----------|---------|\n",
    "| 1 (unigram) | None | Word frequencies | No word order |\n",
    "| 2 (bigram) | 1 word | Local pairs | Limited context |\n",
    "| 3 (trigram) | 2 words | Short phrases | Data sparsity grows |\n",
    "| 4+ | 3+ words | Longer patterns | Most contexts never seen |\n",
    "\n",
    "As $N$ increases, the model captures more context but faces exponentially worse data sparsity: there are $|V|^N$ possible N-grams, and most of them never appear in training.\n",
    "\n",
    "### Task 4.1: Implement `get_ngrams` (4 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4144137",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ngrams(sentence: List[str], n: int) -> List[Tuple[tuple, str]]:\n",
    "    \"\"\"\n",
    "    Extract (context, word) pairs from a sentence for an N-gram model.\n",
    "    \n",
    "    For a trigram model (n=3) and sentence ['<s>', 'to', 'be', 'or', '</s>']:\n",
    "        - Pad beginning: ['<s>', '<s>', 'to', 'be', 'or', '</s>']\n",
    "        - context = tuple of previous (n-1) = 2 words\n",
    "        - Pairs: (('<s>', '<s>'), 'to'), (('<s>', 'to'), 'be'), ...\n",
    "    \n",
    "    For unigram (n=1): context is always empty tuple ()\n",
    "    For bigram (n=2): context is 1-tuple of previous word\n",
    "    \n",
    "    Args:\n",
    "        sentence: A tokenized sentence (list of strings)\n",
    "        n: The order of the N-gram model\n",
    "    \n",
    "    Returns:\n",
    "        List of (context_tuple, next_word) pairs\n",
    "    \"\"\"\n",
    "    # ============================================================\n",
    "    # TODO: Implement N-gram extraction\n",
    "    # 1. Pad beginning: pad = ['<s>'] * (n - 1)\n",
    "    #    padded = pad + sentence[1:]  (avoid double-counting existing <s>)\n",
    "    # 2. For each position i from (n-1) to len(padded):\n",
    "    #    - context = tuple of padded[i-n+1 : i] if n > 1 else ()\n",
    "    #    - word = padded[i]\n",
    "    #    - Append (context, word) to results\n",
    "    # ============================================================\n",
    "    pass\n",
    "\n",
    "\n",
    "# Test\n",
    "test_sent = ['<s>', 'to', 'be', 'or', '</s>']\n",
    "bigrams = get_ngrams(test_sent, n=2)\n",
    "trigrams = get_ngrams(test_sent, n=3)\n",
    "unigrams = get_ngrams(test_sent, n=1)\n",
    "\n",
    "print(\"Bigrams:\", bigrams)\n",
    "print(\"Trigrams:\", trigrams)\n",
    "print(\"Unigrams:\", unigrams)\n",
    "\n",
    "assert bigrams[0] == (('<s>',), 'to'), f\"First bigram wrong: {bigrams[0]}\"\n",
    "assert trigrams[0] == (('<s>', '<s>'), 'to'), f\"First trigram wrong: {trigrams[0]}\"\n",
    "assert unigrams[0] == ((), 'to'), f\"First unigram wrong: {unigrams[0]}\"\n",
    "print(\"\\n✓ All ngram extraction tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06768979",
   "metadata": {},
   "source": [
    "### Task 4.2: Implement `ngram_train` (4 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ab8d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram_train(data: List[List[str]], n: int, smoothing: float = 0.1) -> dict:\n",
    "    \"\"\"\n",
    "    Train a general N-gram model.\n",
    "    \n",
    "    Args:\n",
    "        data: List of tokenized sentences\n",
    "        n: The N-gram order (1=unigram, 2=bigram, 3=trigram, etc.)\n",
    "        smoothing: Laplace smoothing parameter\n",
    "    \n",
    "    Returns:\n",
    "        A dictionary with keys:\n",
    "            'n': the N-gram order\n",
    "            'smoothing': the smoothing parameter\n",
    "            'ngram_counts': defaultdict(Counter) — ngram_counts[context_tuple][word]\n",
    "            'context_counts': Counter — total count for each context tuple\n",
    "            'vocab': set of all unique tokens\n",
    "    \"\"\"\n",
    "    # ============================================================\n",
    "    # TODO: Implement training\n",
    "    # For each sentence, extract ngrams using get_ngrams() and count them:\n",
    "    #   for context, word in get_ngrams(sentence, n):\n",
    "    #       ngram_counts[context][word] += 1\n",
    "    #       context_counts[context] += 1\n",
    "    #       vocab.add(word) + add all words in context\n",
    "    # ============================================================\n",
    "    pass\n",
    "\n",
    "\n",
    "# Test\n",
    "trigram_model = ngram_train(train_data, n=3, smoothing=0.1)\n",
    "assert isinstance(trigram_model, dict)\n",
    "assert trigram_model['n'] == 3\n",
    "print(f\"Trigram model: {len(trigram_model['context_counts']):,} unique contexts, \"\n",
    "      f\"{len(trigram_model['vocab']):,} vocab size\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f2cfaf",
   "metadata": {},
   "source": [
    "### Task 4.3: Implement `ngram_probability` (4 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f139771",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram_probability(model: dict, word: str, context: tuple) -> float:\n",
    "    \"\"\"\n",
    "    Compute P(word | context) with Laplace smoothing.\n",
    "    \n",
    "    P(w | c) = (count(c, w) + alpha) / (count(c) + alpha * |V|)\n",
    "    \n",
    "    Args:\n",
    "        model: The N-gram model dictionary\n",
    "        word: The word to compute probability for\n",
    "        context: A tuple of preceding words\n",
    "    \n",
    "    Returns:\n",
    "        P(word | context)\n",
    "    \"\"\"\n",
    "    # ============================================================\n",
    "    # TODO: Implement\n",
    "    # ============================================================\n",
    "    pass\n",
    "\n",
    "\n",
    "# Test\n",
    "p = ngram_probability(trigram_model, 'be', ('<s>', 'to'))\n",
    "print(f\"P(be | '<s>', 'to') = {p:.4f}\")\n",
    "assert p > 0, \"Probability should be positive\"\n",
    "# Unknown context should still give nonzero probability (due to smoothing)\n",
    "p_unk = ngram_probability(trigram_model, 'be', ('xyzzy', 'abcde'))\n",
    "assert p_unk > 0, \"Smoothed probability of unseen context should be > 0\"\n",
    "print(f\"P(be | 'xyzzy', 'abcde') = {p_unk:.6f}  (smoothed)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037dfa34",
   "metadata": {},
   "source": [
    "### Task 4.4: Implement `ngram_perplexity` (4 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b5515c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram_perplexity(model: dict, data: List[List[str]]) -> float:\n",
    "    \"\"\"\n",
    "    Compute perplexity of the N-gram model on a dataset.\n",
    "    \n",
    "    Args:\n",
    "        model: The N-gram model dictionary\n",
    "        data: List of tokenized sentences\n",
    "    \n",
    "    Returns:\n",
    "        Perplexity (positive float; lower is better)\n",
    "    \"\"\"\n",
    "    # ============================================================\n",
    "    # TODO: Implement\n",
    "    # For each sentence, use get_ngrams(sentence, model['n']) to\n",
    "    # extract (context, word) pairs, then compute log probabilities\n",
    "    # ============================================================\n",
    "    pass\n",
    "\n",
    "\n",
    "# Test\n",
    "train_ppl = ngram_perplexity(trigram_model, train_data)\n",
    "val_ppl = ngram_perplexity(trigram_model, val_data)\n",
    "print(f\"Trigram — Train perplexity: {train_ppl:.2f}\")\n",
    "print(f\"Trigram — Val perplexity:   {val_ppl:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c176e4",
   "metadata": {},
   "source": [
    "### Task 4.5: Implement `ngram_generate` (4 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a488eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram_generate(model: dict, max_length: int = 30) -> str:\n",
    "    \"\"\"\n",
    "    Generate a sentence from the N-gram model.\n",
    "    \n",
    "    Start with context of (n-1) <s> tokens.\n",
    "    At each step, sample next word given current context.\n",
    "    Slide context window forward after each word.\n",
    "    \n",
    "    Args:\n",
    "        model: The N-gram model dictionary\n",
    "        max_length: Maximum number of words to generate\n",
    "    \n",
    "    Returns:\n",
    "        Generated sentence as a string (without <s> and </s>)\n",
    "    \"\"\"\n",
    "    # ============================================================\n",
    "    # TODO: Implement\n",
    "    # 1. n = model['n']\n",
    "    # 2. context = tuple(['<s>'] * (n-1)) if n > 1 else ()\n",
    "    # 3. Loop: sample word from ngram_counts[context]\n",
    "    # 4. Update context: context = tuple(list(context[1:]) + [word])\n",
    "    # 5. Stop at '</s>' or max_length\n",
    "    # ============================================================\n",
    "    pass\n",
    "\n",
    "\n",
    "# Test\n",
    "print(\"Generated sentences (trigram):\")\n",
    "for i in range(5):\n",
    "    print(f\"  {i+1}. {ngram_generate(trigram_model)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aadde57",
   "metadata": {},
   "source": [
    "### 4.6: N-gram Comparison (Done for you)\n",
    "\n",
    "Run the following code which compares n-gram models of different sizes. \n",
    "\n",
    "**Conceptual Questions (not graded)**\n",
    "\n",
    "- Does the text generated seem like something Shakespeare would wrtite?\n",
    "- How does the text quality change as we increase n? (unigram vs bigram vs trigram vs etc.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3986197e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare N-gram orders\n",
    "n_values = [1, 2, 3, 4, 5]\n",
    "train_ppls = []\n",
    "val_ppls = []\n",
    "models = {}\n",
    "\n",
    "for nv in n_values:\n",
    "    m = ngram_train(train_data, n=nv, smoothing=0.1)\n",
    "    models[nv] = m\n",
    "    t_ppl = ngram_perplexity(m, train_data)\n",
    "    v_ppl = ngram_perplexity(m, val_data)\n",
    "    train_ppls.append(t_ppl)\n",
    "    val_ppls.append(v_ppl)\n",
    "    print(f\"N={nv}: Train PPL={t_ppl:8.2f}  Val PPL={v_ppl:8.2f}\")\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(n_values, train_ppls, 'bo-', label='Train', linewidth=2, markersize=8)\n",
    "plt.plot(n_values, val_ppls, 'rs-', label='Validation', linewidth=2, markersize=8)\n",
    "plt.xlabel('N-gram Order (N)', fontsize=12)\n",
    "plt.ylabel('Perplexity', fontsize=12)\n",
    "plt.title('Perplexity vs. N-gram Order', fontsize=14)\n",
    "plt.legend(fontsize=11)\n",
    "plt.xticks(n_values)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"GENERATED TEXT COMPARISON\")\n",
    "print(\"=\" * 60)\n",
    "for nv in n_values:\n",
    "    print(f\"\\n--- {nv}-gram Model ---\")\n",
    "    for i in range(3):\n",
    "        print(f\"  {ngram_generate(models[nv])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a248186",
   "metadata": {},
   "source": [
    "---\n",
    "## Submission Checklist\n",
    "\n",
    "Before submitting, make sure:\n",
    "\n",
    "- [ ] **Part 1:** `tokenize_sentences` work correctly\n",
    "- [ ] **Part 2:** All 5 unigram functions implemented\n",
    "- [ ] **Part 3:** All bigram functions implemented\n",
    "- [ ] **Part 4 (Extra Credit):** All N-gram functions implemented\n",
    "\n",
    "**Grading:**\n",
    "\n",
    "| Part | Points |\n",
    "|------|--------|\n",
    "| Part 1: Preprocessing & Tokenization | 10 |\n",
    "| Part 2: Unigram Model | 40 |\n",
    "| Part 3: Bigram Model | 50 |\n",
    "| Part 4 (Extra Credit): N-gram Model | 20 |\n",
    "| **Total** | **100** |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d158258",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
