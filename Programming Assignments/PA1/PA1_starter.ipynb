{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d006243",
   "metadata": {},
   "source": [
    "# PA1 - Perceptron\n",
    "\n",
    "## Background: What is a perceptron?\n",
    "- A perceptron is a simple classifier: it computes a weighted sum of the input\n",
    "  features and predicts based on the sign (binary) or the largest score (multiclass).\n",
    "- Perceptrons work best when the classes can be separated by a single straight\n",
    "  boundary (a line in 2D, a flat plane in 3D, and so on). They are fast and easy\n",
    "  to understand, but they cannot capture complex curved boundaries on their own.\n",
    "- If the data are not separable by one straight boundary, the algorithm may keep\n",
    "  updating without fully reaching 100% accuracy. XOR in 2D is a classic example:\n",
    "  - Class A: (0, 0), (1, 1)\n",
    "  - Class B: (0, 1), (1, 0)\n",
    "  - No single line can separate A from B in 2D.\n",
    "\n",
    "## Assignment rules\n",
    "- Use only Python standard libraries for any training logic that you implement.\n",
    "- Do **not** use machine learning libraries for training (e.g., no scikit-learn models, no PyTorch, no TensorFlow).\n",
    "- Libraries to **load** data are allowed (e.g., `sklearn.datasets.load_digits`).\n",
    "- The provided imports of scikit-learn in this notebook are only for data loading / utilities, not for implementing your perceptron.\n",
    "\n",
    "References: [Perceptron](https://en.wikipedia.org/wiki/Perceptron), [Linear separability](https://en.wikipedia.org/wiki/Linear_separability)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04ca19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (run once and then comment out)\n",
    "!pip install scikit-learn matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0384bb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass\n",
    "import random\n",
    "from typing import List, Sequence, Tuple\n",
    "\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split as sk_train_test_split\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28a6d0d",
   "metadata": {},
   "source": [
    "## Question 0: Setup + Utils\n",
    "High-level goal: build small helper functions so the rest of the assignment\n",
    "focuses on perceptron logic rather than data plumbing. This allows you to\n",
    "spend your time on the learning algorithm itself.\n",
    "\n",
    "\n",
    "Parts (TODO):\n",
    "- 0.1 Load the dataset (TODO). [5 points]\n",
    "- 0.2 Normalize features (TODO). [5 points]\n",
    "\n",
    "Provided utility (not graded):\n",
    "- Train/test split wrapper (scikit-learn).\n",
    "\n",
    "Tip: keep these helpers simple and readable; you will reuse them throughout\n",
    "later questions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21dd384",
   "metadata": {},
   "source": [
    "### Part 0.1: Load the dataset. [5 points]\n",
    "We are doing this to get a standard, small image dataset quickly so\n",
    "we can focus on the perceptron implementation instead of file I/O.\n",
    "\n",
    "What to implement (TODO):\n",
    "- Import the loader at the top with the other imports (keeps dependencies centralized).\n",
    "- Load the dataset object, extract `.data` and `.target`, and return them\n",
    "  as plain Python lists (`List[List[float]]` and `List[int]`).\n",
    "\n",
    "\n",
    "Dataset context (scikit-learn digits):\n",
    "- 8x8 grayscale images, flattened to 64 features per example.\n",
    "- Labels are digits 0â€“9.\n",
    "- Pixel values are in the range [0, 16].\n",
    "- Total samples: 1,797.\n",
    "\n",
    "Docs: [digits dataset overview](https://scikit-learn.org/stable/datasets/toy_dataset.html#digits-dataset), [load_digits API](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html).\n",
    "Example pattern (different dataset):\n",
    "```python\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "X_example = iris.data.tolist()\n",
    "y_example = iris.target.tolist()\n",
    "```\n",
    "\n",
    "Your task: do the same pattern with `load_digits()` inside `load_digits_data()`.\n",
    "\n",
    "Reference: [scikit-learn load_digits](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html)\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/f/f7/MnistExamplesModified.png\" alt=\"MNIST handwritten digit examples\" width=\"420\" style=\"display:block;margin:0 auto;\" />\n",
    "\n",
    "*Image: [\"MnistExamplesModified.png\"](https://commons.wikimedia.org/wiki/File:MnistExamplesModified.png) by Suvanjanprasai, [CC BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0/).* \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21b28b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_digits_data() -> Tuple[List[List[float]], List[int]]:\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "        X: list of 64-length feature lists (each is 8x8 flattened).\n",
    "        y: list of integer labels in {0..9}.\n",
    "    \"\"\"\n",
    "    # TODO: Import load_digits with the other imports at the top.\n",
    "    # TODO: Load the dataset, extract features and labels, and convert them\n",
    "    # to plain Python lists before returning.\n",
    "    raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f20c64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Part 0.1 (load digits)\n",
    "X_demo, y_demo = load_digits_data()\n",
    "assert len(X_demo) == len(y_demo) and len(X_demo) > 0\n",
    "assert len(X_demo[0]) == 64\n",
    "assert min(y_demo) == 0 and max(y_demo) == 9\n",
    "print(\"OK: loaded\", len(X_demo), \"samples\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f108f707",
   "metadata": {},
   "source": [
    "### Train/test split (given, not graded).\n",
    "We are doing this to evaluate generalization, not just training accuracy,\n",
    "which is important because a model can memorize the training data.\n",
    "\n",
    "Conceptually, a train/test split should:\n",
    "- Use a random seed for reproducibility.\n",
    "- Shuffle data in unison so X and y stay aligned.\n",
    "- Split according to a test ratio (e.g., 0.2 for 80/20).\n",
    "\n",
    "Context: X is a list of input rows (a list of lists) and y is the list of labels.\n",
    "For example, if our dataset has two instances (rows) of whether you bring an umbrella\n",
    "depending on the weather conditions, X would have n rows for each day recorded with\n",
    "m columns for each weather condition noted (sunny/cloudy, hot/cold), and y would have\n",
    "n entries for the outcome of each day.\n",
    "\n",
    "Dataset:   \n",
    "| sunny/cloudy  | hot/cold  | umbrella | \n",
    "|---------------|-----------|-------------| \n",
    "| sunny         | hot       | not brought | \n",
    "| cloudy        | cold      | brought | \n",
    "\n",
    "X:   \n",
    "| sunny/cloudy | hot/cold | \n",
    "|--------|---------|  \n",
    "| sunny | hot |  \n",
    "| cloudy | cold |   \n",
    "\n",
    "y:   \n",
    "| umbrella |   \n",
    "|----------|  \n",
    "| not brought |  \n",
    "| brought |  \n",
    "    \n",
    "Reference: [train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/8/88/Machine_learning_nutshell_--_Split_into_train-test_set.svg\" alt=\"Train/test split diagram\" width=\"420\" style=\"display:block;margin:0 auto;\" />\n",
    "\n",
    "*Image: [\"Machine learning nutshell -- Split into train-test set.svg\"](https://commons.wikimedia.org/wiki/File:Machine_learning_nutshell_--_Split_into_train-test_set.svg) by EpochFail, [CC BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0/).* \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24e393a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(\n",
    "    X: Sequence[Sequence[float]],\n",
    "    y: Sequence[int],\n",
    "    test_ratio: float = 0.2,\n",
    "    seed: int = 0,\n",
    "    shuffle: bool = True,\n",
    ") -> Tuple[List[List[float]], List[List[float]], List[int], List[int]]:\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "        X_train, X_test, y_train, y_test\n",
    "    \"\"\"\n",
    "    stratify_y = list(y) if shuffle else None\n",
    "    X_train, X_test, y_train, y_test = sk_train_test_split(\n",
    "        list(X),\n",
    "        list(y),\n",
    "        test_size=test_ratio,\n",
    "        random_state=seed,\n",
    "        shuffle=shuffle,\n",
    "        stratify=stratify_y,\n",
    "    )\n",
    "    return (\n",
    "        [list(row) for row in X_train],\n",
    "        [list(row) for row in X_test],\n",
    "        [int(v) for v in y_train],\n",
    "        [int(v) for v in y_test],\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc7abbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example (not graded)\n",
    "X_tts_example = [[i] for i in range(10)]\n",
    "y_tts_example = list(range(10))\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_tts_example, y_tts_example, test_ratio=0.2, seed=0, shuffle=False\n",
    ")\n",
    "print(\"X_train:\", X_train)\n",
    "print(\"X_test:\", X_test)\n",
    "print(\"y_train:\", y_train)\n",
    "print(\"y_test:\", y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6673e31e",
   "metadata": {},
   "source": [
    "### Part 0.2: Normalize features. [5 points]\n",
    "We are doing this to keep feature magnitudes in a reasonable range.\n",
    "This helps the learning rule behave more consistently.\n",
    "\n",
    "What to implement (TODO):\n",
    "- Return a new list of lists with each feature divided by `scale`.\n",
    "- Do not mutate the input list.\n",
    "\n",
    "Reference: [Feature scaling / normalization](https://en.wikipedia.org/wiki/Feature_scaling)\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/7/77/The_effect_of_z-score_normalization_on_k-means_clustering.svg\" alt=\"Effect of z-score normalization\" width=\"420\" style=\"display:block;margin:0 auto;\" />\n",
    "\n",
    "*Image: [\"The effect of z-score normalization on k-means clustering.svg\"](https://commons.wikimedia.org/wiki/File:The_effect_of_z-score_normalization_on_k-means_clustering.svg) by Cosmia Nebula, [CC BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0/).* \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472084a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_features(\n",
    "    X: Sequence[Sequence[float]],\n",
    "    scale: float = 16.0,\n",
    ") -> List[List[float]]:\n",
    "    \"\"\"\n",
    "    Normalizes each feature by dividing by `scale`.\n",
    "    For digits, pixel values are in [0, 16].\n",
    "    \"\"\"\n",
    "    # TODO: Return a new list of lists with each feature scaled by `scale`.\n",
    "    raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6730997c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Part 0.2 (normalize)\n",
    "X_preview = [row[:] for row in X_demo[:3]]\n",
    "X_norm = normalize_features(X_demo)\n",
    "assert X_demo[:3] == X_preview  # input should not be mutated\n",
    "assert 0.0 <= min(X_norm[0]) and max(X_norm[0]) <= 1.0\n",
    "print(\"OK: normalized; first row in [0, 1]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d5e3d3",
   "metadata": {},
   "source": [
    "## Question 1: Binary Perceptron\n",
    "High-level goal: build a perceptron that separates two digits. This gives you\n",
    "the core learning rule in a simple setting and lets you see how a straight\n",
    "decision boundary behaves before going multiclass.\n",
    "\n",
    "Intuition: a perceptron scores an input by multiplying each input by its\n",
    "corresponding weight, adding the results, and then adding a bias.\n",
    "If the data can be separated by one straight boundary, repeated updates\n",
    "push that boundary to correctly classify all training points.\n",
    "\n",
    "Note: The autograder expects small classes below, but you do not need to\n",
    "understand classes to finish this assignment. Implement the helper functions\n",
    "(`binary_init`, `binary_score`, `binary_predict`, `binary_update`, `binary_fit`);\n",
    "the class methods call them for you.\n",
    "\n",
    "TODOs (TODO):\n",
    "- TODO (1.1) Implement `binary_init`: weights length = num_features, bias = 0.0. [5 points]\n",
    "- TODO (1.2) Implement `binary_score`: weighted sum + bias. [5 points]\n",
    "- TODO (1.3) Implement `binary_predict`: score >= 0 -> +1, else -1. [5 points]\n",
    "- TODO (1.4) Implement `binary_update`: adjust weights and bias on a mistake. [10 points]\n",
    "- TODO (1.5) Implement `binary_fit`: loop epochs, shuffle if requested, call update. [10 points]\n",
    "\n",
    "Reference: [Perceptron](https://en.wikipedia.org/wiki/Perceptron)\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/3/31/Perceptron.svg\" alt=\"Perceptron diagram\" width=\"420\" style=\"display:block;margin:0 auto;\" />\n",
    "\n",
    "*Image: [\"Perceptron.svg\"](https://commons.wikimedia.org/wiki/File:Perceptron.svg) by Matthew, [CC BY-SA 3.0](https://creativecommons.org/licenses/by-sa/3.0/).* \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90cc072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions (function-based approach).\n",
    "\n",
    "def binary_init(num_features: int) -> Tuple[List[float], float]:\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "        weights: list of length num_features (all zeros)\n",
    "        bias: 0.0\n",
    "    \"\"\"\n",
    "    # TODO (1.1): Initialize weights to all zeros and bias to 0.0.\n",
    "    raise NotImplementedError\n",
    "\n",
    "\n",
    "def binary_score(weights: Sequence[float], bias: float, x: Sequence[float]) -> float:\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "        score: weighted sum of inputs plus bias.\n",
    "    \"\"\"\n",
    "    # TODO (1.2): Compute the weighted sum:\n",
    "    #   multiply each input by its corresponding weight,\n",
    "    #   add the results, then add the bias.\n",
    "    #   (This maps directly to a for loop.)\n",
    "    raise NotImplementedError\n",
    "\n",
    "\n",
    "def binary_predict(weights: Sequence[float], bias: float, x: Sequence[float]) -> int:\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "        +1 if score >= 0 else -1\n",
    "    \"\"\"\n",
    "    # TODO (1.3): Use the sign of the score to map to +1 or -1.\n",
    "    raise NotImplementedError\n",
    "\n",
    "\n",
    "def binary_update(\n",
    "    weights: List[float],\n",
    "    bias: float,\n",
    "    x: Sequence[float],\n",
    "    y: int,\n",
    ") -> Tuple[List[float], float]:\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "        updated weights and bias after one example.\n",
    "    \"\"\"\n",
    "    # TODO (1.4): If the example is misclassified, update each weight and bias:\n",
    "    #   w_i = w_i + y * x_i\n",
    "    #   bias = bias + y\n",
    "    raise NotImplementedError\n",
    "\n",
    "\n",
    "def binary_fit(\n",
    "    weights: List[float],\n",
    "    bias: float,\n",
    "    X: Sequence[Sequence[float]],\n",
    "    y: Sequence[int],\n",
    "    epochs: int = 10,\n",
    "    seed: int = 0,\n",
    "    shuffle: bool = True,\n",
    ") -> Tuple[List[float], float]:\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "        weights and bias after training.\n",
    "    \"\"\"\n",
    "    # TODO (1.5) (pseudocode):\n",
    "    #   make an index list [0..len(X)-1]\n",
    "    #   if shuffle: set random.seed(seed) for reproducibility\n",
    "    #   repeat for each epoch:\n",
    "    #       if shuffle: random.shuffle(indices)\n",
    "    #       for idx in indices: update weights/bias on (X[idx], y[idx])\n",
    "    raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680a7461",
   "metadata": {},
   "source": [
    "### Python Classes\n",
    "The `BinaryPerceptron` class below uses concepts from object oriented programming. The class has variables for storing a perceptron's number of features/weights, weights, and bias (y-intercept). The methods `create`, `score`, `predict`, `update`, and `fit` are needed to edit the class's internal variables.\n",
    "\n",
    "Reference: [Python OOP](https://pythonnumericalmethods.studentorg.berkeley.edu/notebooks/chapter07.01-Introduction-to-OOP.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e575f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The class below is a thin wrapper used by the autograder.\n",
    "@dataclass\n",
    "class BinaryPerceptron:\n",
    "    \"\"\"\n",
    "    Binary perceptron for labels in {+1, -1}.\n",
    "    \"\"\"\n",
    "\n",
    "    num_features: int\n",
    "    weights: List[float]\n",
    "    bias: float\n",
    "\n",
    "    @classmethod\n",
    "    def create(cls, num_features: int) -> \"BinaryPerceptron\":\n",
    "        weights, bias = binary_init(num_features)\n",
    "        return cls(num_features=num_features, weights=weights, bias=bias)\n",
    "\n",
    "    def score(self, x: Sequence[float]) -> float:\n",
    "        return binary_score(self.weights, self.bias, x)\n",
    "\n",
    "    def predict(self, x: Sequence[float]) -> int:\n",
    "        return binary_predict(self.weights, self.bias, x)\n",
    "\n",
    "    def update(self, x: Sequence[float], y: int) -> None:\n",
    "        self.weights, self.bias = binary_update(self.weights, self.bias, x, y)\n",
    "\n",
    "    def fit(\n",
    "        self,\n",
    "        X: Sequence[Sequence[float]],\n",
    "        y: Sequence[int],\n",
    "        epochs: int = 10,\n",
    "        seed: int = 0,\n",
    "        shuffle: bool = True,\n",
    "    ) -> None:\n",
    "        self.weights, self.bias = binary_fit(\n",
    "            self.weights,\n",
    "            self.bias,\n",
    "            X,\n",
    "            y,\n",
    "            epochs=epochs,\n",
    "            seed=seed,\n",
    "            shuffle=shuffle,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b3b2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Question 1 (binary perceptron sanity check)\n",
    "X_or = [[0, 0], [0, 1], [1, 0], [1, 1]]\n",
    "y_or = [-1, 1, 1, 1]\n",
    "model_or = BinaryPerceptron.create(num_features=2)\n",
    "model_or.fit(X_or, y_or, epochs=10, shuffle=False)\n",
    "preds_or = [model_or.predict(x) for x in X_or]\n",
    "assert preds_or == y_or\n",
    "print(\"OK: OR learned\", preds_or)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9a4a01",
   "metadata": {},
   "source": [
    "## Question 2: Binary Classification Task\n",
    "High-level goal: use the binary perceptron to separate two digits\n",
    "(e.g., 0 vs 1). This exercises the full pipeline end-to-end and\n",
    "gives you a concrete, visual task to evaluate.\n",
    "\n",
    "Parts (TODO):\n",
    "- 2.1 Filter the dataset to two digits and relabel (TODO). [8 points]\n",
    "- 2.2 Evaluate binary accuracy (TODO). [6 points]\n",
    "- 2.3a Build the XOR dataset (TODO). [3 points]\n",
    "- 2.3b Map XOR features (TODO). [3 points]\n",
    "\n",
    "Tip: pick digit pairs that are visually distinct first (e.g., 0 vs 1) and then\n",
    "try harder pairs (e.g., 3 vs 5) to see how a straight boundary affects accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1697ea",
   "metadata": {},
   "source": [
    "### Part 2.1: Filter the dataset to two digits and relabel. [8 points]\n",
    "We are doing this so the binary perceptron sees labels in {+1, -1}.\n",
    "\n",
    "What to implement (TODO):\n",
    "- Keep only examples where the label is either `positive_digit` or `negative_digit`.\n",
    "- Map `positive_digit` -> +1 and `negative_digit` -> -1.\n",
    "- Preserve the original order of examples.\n",
    "\n",
    "Why this matters: the update rule assumes a symmetric label set {+1, -1},\n",
    "so relabeling makes the math and the code consistent.\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/a/a2/Linearly_separable.JPG\" alt=\"Straight-line separable example\" width=\"420\" style=\"display:block;margin:0 auto;\" />\n",
    "\n",
    "*Image: example from Wikimedia Commons (public domain).* \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0120d99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_binary_dataset(\n",
    "    X: Sequence[Sequence[float]],\n",
    "    y: Sequence[int],\n",
    "    positive_digit: int,\n",
    "    negative_digit: int,\n",
    ") -> Tuple[List[List[float]], List[int]]:\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "        X_bin, y_bin where y_bin is +1 for positive_digit and -1 for negative_digit\n",
    "    \"\"\"\n",
    "    # TODO: Filter to the two digits and map them to +1/-1 labels.\n",
    "    # Return the new feature and label lists.\n",
    "    # if the digit is positive_digit, label +1\n",
    "    # if the digit is negative_digit, label -1\n",
    "    raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a7cd54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Part 2.1 (make binary dataset)\n",
    "X_bin, y_bin = make_binary_dataset(X_norm, y_demo, positive_digit=0, negative_digit=1)\n",
    "expected = sum(1 for v in y_demo if v in (0, 1))\n",
    "assert len(X_bin) == expected and len(y_bin) == expected\n",
    "assert set(y_bin).issubset({1, -1})\n",
    "print(\"OK: binary dataset size\", len(X_bin))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064213f1",
   "metadata": {},
   "source": [
    "### Part 2.2: Evaluate binary accuracy. [6 points]\n",
    "Binary accuracy is the proportion of predictions that are correct, when there are only two possible classes.\n",
    "\n",
    "What to implement (TODO):\n",
    "- Use `model.predict` for each example.\n",
    "- Return correct / total as a float.\n",
    "- If X is empty, return 0.0.\n",
    "\n",
    "Reference: [accuracy_score definition](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccdf2a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy(\n",
    "    model: BinaryPerceptron,\n",
    "    X: Sequence[Sequence[float]],\n",
    "    y: Sequence[int],\n",
    ") -> float:\n",
    "    # TODO: Compute accuracy as correct predictions divided by total.\n",
    "    raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cab690f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Part 2.2 (binary accuracy)\n",
    "acc_or = binary_accuracy(model_or, X_or, y_or)\n",
    "assert acc_or == 1.0\n",
    "print(\"OK: OR accuracy\", acc_or)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9652fc98",
   "metadata": {},
   "source": [
    "### Part 2.3: XOR feature mapping (2D -> 3D). [6 points]\n",
    "We are doing this to show how adding a new feature can make the XOR\n",
    "points separable by a single straight boundary. XOR is not separable\n",
    "by a line in 2D, but it becomes separable after adding the feature x1 * x2.\n",
    "\n",
    "What to implement (TODO):\n",
    "- Part 2.3a: Create the 4 XOR points and labels (+1 for XOR-true, -1 otherwise). [3 points]\n",
    "- Part 2.3b: Map each [x1, x2] -> [x1, x2, x1 * x2]. [3 points]\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/7/7f/XOR_truth_table.svg\" alt=\"XOR truth table\" width=\"420\" style=\"display:block;margin:0 auto;\" />\n",
    "\n",
    "*Image: [\"XOR truth table.svg\"](https://commons.wikimedia.org/wiki/File:XOR_truth_table.svg) by Lionel Allorge, [CC BY-SA 3.0](https://creativecommons.org/licenses/by-sa/3.0/).* \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3080fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xor_dataset() -> Tuple[List[List[float]], List[int]]:\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "        X: 2D XOR inputs\n",
    "        y: labels in {+1, -1} where +1 is XOR-true\n",
    "    \"\"\"\n",
    "    # TODO: Return the four XOR points with labels in {+1, -1}.\n",
    "    # +1 for (0,1) and (1,0); -1 for (0,0) and (1,1).\n",
    "\n",
    "    raise NotImplementedError\n",
    "\n",
    "\n",
    "def map_xor_features(X: Sequence[Sequence[float]]) -> List[List[float]]:\n",
    "    \"\"\"\n",
    "    Maps 2D inputs [x1, x2] -> [x1, x2, x1*x2].\n",
    "    \"\"\"\n",
    "    # TODO: Return a new list with the mapped features.\n",
    "    # For each input [x1, x2], create [x1, x2, x1*x2].\n",
    "    # return the new list of lists.\n",
    "    raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0460ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Part 2.3 (XOR feature mapping)\n",
    "X_xor, y_xor = xor_dataset()\n",
    "mapped = map_xor_features(X_xor)\n",
    "assert len(mapped) == 4 and all(len(row) == 3 for row in mapped)\n",
    "for (x1, x2), m in zip(X_xor, mapped):\n",
    "    assert m[2] == x1 * x2\n",
    "assert sorted(y_xor) == [-1, -1, 1, 1]\n",
    "print(\"OK: XOR mapping\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21dfe352",
   "metadata": {},
   "source": [
    "## Question 3: Multiclass Perceptron\n",
    "High-level goal: extend the perceptron to handle all 10 digits.\n",
    "This allows us to make a single model that predicts 0-9 directly by\n",
    "scoring each class and choosing the highest. Note that digit images are\n",
    "not perfectly separable by a single boundary in 64D, so we should expect good but\n",
    "not perfect accuracy.\n",
    "\n",
    "Intuition: instead of one weight list, you maintain one list per class.\n",
    "Given an input, compute 10 scores and predict the class with the\n",
    "largest score (a \"winner-take-all\" rule).\n",
    "\n",
    "Note: As in Question 1, you can ignore the class details. Implement the\n",
    "helper functions (`multiclass_init`, `multiclass_scores`, `multiclass_predict`,\n",
    "`multiclass_update`, `multiclass_fit`); the class methods call them for you.\n",
    "\n",
    "TODOs (TODO):\n",
    "- TODO (3.1) Implement `multiclass_init`: one weight list per class, all zeros. [5 points]\n",
    "- TODO (3.2) Implement `multiclass_scores`: one score per class. [5 points]\n",
    "- TODO (3.3) Implement `multiclass_predict`: return class with highest score. [5 points]\n",
    "- TODO (3.4) Implement `multiclass_update`: push true class up, predicted class down. [5 points]\n",
    "- TODO (3.5) Implement `multiclass_fit`: loop epochs, shuffle if requested, call update. [5 points]\n",
    "\n",
    "Reference: [Multiclass classification overview](https://en.wikipedia.org/wiki/Multiclass_classification)\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/7/71/Multiclass_classification.png\" alt=\"Multiclass classification example\" width=\"420\" style=\"display:block;margin:0 auto;\" />\n",
    "\n",
    "*Image: [\"Multiclass classification.png\"](https://commons.wikimedia.org/wiki/File:Multiclass_classification.png) by SoroushJahanzad, [CC BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0/).* \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df85f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions (function-based approach).\n",
    "\n",
    "def multiclass_init(\n",
    "    num_features: int,\n",
    "    num_classes: int,\n",
    ") -> Tuple[List[List[float]], List[float]]:\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "        weights: list of weight lists (one per class)\n",
    "        bias: list of class biases\n",
    "    \"\"\"\n",
    "    # TODO (3.1): Initialize all weights to zeros and all biases to 0.0.\n",
    "    #   weights should be a list of length num_classes, each a list of length num_features.\n",
    "    #   bias should be a list of length num_classes.\n",
    "    raise NotImplementedError\n",
    "\n",
    "\n",
    "def multiclass_scores(weights: Sequence[Sequence[float]], bias: Sequence[float], x: Sequence[float]) -> List[float]:\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "        list of scores, one per class.\n",
    "    \"\"\"\n",
    "    # TODO (3.2): For each class, compute a weighted sum of inputs plus its bias.\n",
    "    #   (Multiply each input by its corresponding weight, add them up, then add bias.)\n",
    "    raise NotImplementedError\n",
    "\n",
    "\n",
    "def multiclass_predict(weights: Sequence[Sequence[float]], bias: Sequence[float], x: Sequence[float]) -> int:\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "        index of the class with the largest score.\n",
    "    \"\"\"\n",
    "    # TODO (3.3): Return the class index with the largest score.\n",
    "    raise NotImplementedError\n",
    "\n",
    "\n",
    "def multiclass_update(\n",
    "    weights: List[List[float]],\n",
    "    bias: List[float],\n",
    "    x: Sequence[float],\n",
    "    y_true: int,\n",
    ") -> Tuple[List[List[float]], List[float]]:\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "        updated weights and bias after one example.\n",
    "    \"\"\"\n",
    "    # TODO (3.4):\n",
    "    #   y_pred = multiclass_predict(...)\n",
    "    #   if y_pred is not equal to y_true:\n",
    "    #       for each feature i:\n",
    "    #           add x[i] to true class weights\n",
    "    #           subtract x[i] from predicted class weights\n",
    "    #       increase true class bias, decrease predicted class bias\n",
    "    raise NotImplementedError\n",
    "\n",
    "\n",
    "def multiclass_fit(\n",
    "    weights: List[List[float]],\n",
    "    bias: List[float],\n",
    "    X: Sequence[Sequence[float]],\n",
    "    y: Sequence[int],\n",
    "    epochs: int = 10,\n",
    "    seed: int = 0,\n",
    "    shuffle: bool = True,\n",
    ") -> Tuple[List[List[float]], List[float]]:\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "        weights and bias after training.\n",
    "    \"\"\"\n",
    "    # TODO (3.5):\n",
    "    #   make an index list [0..len(X)-1]\n",
    "    #   if shuffle: set random.seed(seed) for reproducibility\n",
    "    #   repeat for each epoch:\n",
    "    #       if shuffle: random.shuffle(indices)\n",
    "    #       update on (X[idx], y[idx]) for each idx\n",
    "    raise NotImplementedError\n",
    "\n",
    "\n",
    "# The class below is a thin wrapper we will use for grading.\n",
    "@dataclass\n",
    "class MulticlassPerceptron:\n",
    "    \"\"\"\n",
    "    Multiclass perceptron using one weight list per class.\n",
    "    \"\"\"\n",
    "\n",
    "    num_features: int\n",
    "    num_classes: int\n",
    "    weights: List[List[float]]\n",
    "    bias: List[float]\n",
    "\n",
    "    @classmethod\n",
    "    def create(cls, num_features: int, num_classes: int) -> \"MulticlassPerceptron\":\n",
    "        weights, bias = multiclass_init(num_features, num_classes)\n",
    "        return cls(num_features=num_features, num_classes=num_classes, weights=weights, bias=bias)\n",
    "\n",
    "    def scores(self, x: Sequence[float]) -> List[float]:\n",
    "        return multiclass_scores(self.weights, self.bias, x)\n",
    "\n",
    "    def predict(self, x: Sequence[float]) -> int:\n",
    "        return multiclass_predict(self.weights, self.bias, x)\n",
    "\n",
    "    def update(self, x: Sequence[float], y_true: int) -> None:\n",
    "        self.weights, self.bias = multiclass_update(self.weights, self.bias, x, y_true)\n",
    "\n",
    "    def fit(\n",
    "        self,\n",
    "        X: Sequence[Sequence[float]],\n",
    "        y: Sequence[int],\n",
    "        epochs: int = 10,\n",
    "        seed: int = 0,\n",
    "        shuffle: bool = True,\n",
    "    ) -> None:\n",
    "        self.weights, self.bias = multiclass_fit(\n",
    "            self.weights,\n",
    "            self.bias,\n",
    "            X,\n",
    "            y,\n",
    "            epochs=epochs,\n",
    "            seed=seed,\n",
    "            shuffle=shuffle,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d5c13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Question 3 (multiclass perceptron quick check)\n",
    "X_mc = [[1, 0, 0], [0, 1, 0], [0, 0, 1]]\n",
    "y_mc = [0, 1, 2]\n",
    "mc = MulticlassPerceptron.create(num_features=3, num_classes=3)\n",
    "mc.fit(X_mc, y_mc, epochs=3, shuffle=False)\n",
    "preds_mc = [mc.predict(x) for x in X_mc]\n",
    "assert preds_mc == y_mc\n",
    "print(\"OK: multiclass one-hot fit\", preds_mc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30a85d2",
   "metadata": {},
   "source": [
    "## Question 4: Multiclass Classification Task\n",
    "High-level goal: train and evaluate the multiclass perceptron on all digits.\n",
    "This shows how well a simple model can do on a real dataset.\n",
    "\n",
    "Parts (TODO):\n",
    "- 4.1 Evaluate multiclass accuracy (TODO). [2 points]\n",
    "- 4.2 Track mistakes per epoch for a binary perceptron (TODO). [3 points]\n",
    "- 4.3 Plot a learning curve (provided, not graded).\n",
    "\n",
    "Tip: accuracy summarizes overall performance, while the learning curve tells\n",
    "you how fast the model improves over training epochs.\n",
    "\n",
    "References: [accuracy_score definition](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html), [Learning curve](https://en.wikipedia.org/wiki/Learning_curve_%28machine_learning%29)\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/1/11/Confusion_Matrix_Metrics.png\" alt=\"Confusion matrix metrics\" width=\"420\" style=\"display:block;margin:0 auto;\" />\n",
    "\n",
    "*Image: [\"Confusion Matrix Metrics.png\"](https://commons.wikimedia.org/wiki/File:Confusion_Matrix_Metrics.png) by Hssiqueira, [CC0 1.0](https://creativecommons.org/publicdomain/zero/1.0/).* \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9ecd07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 4.1: Evaluate multiclass accuracy. [2 points]\n",
    "def multiclass_accuracy(\n",
    "    model: MulticlassPerceptron,\n",
    "    X: Sequence[Sequence[float]],\n",
    "    y: Sequence[int],\n",
    ") -> float:\n",
    "    # TODO: Compute accuracy as correct predictions divided by total.\n",
    "    # If X is empty, return 0.0.\n",
    "    raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd09934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Part 4.1 (multiclass accuracy)\n",
    "Xm_train, Xm_test, ym_train, ym_test = train_test_split(X_norm, y_demo)\n",
    "mc_digits = MulticlassPerceptron.create(num_features=len(Xm_train[0]), num_classes=10)\n",
    "mc_digits.fit(Xm_train, ym_train, epochs=3)\n",
    "acc_mc = multiclass_accuracy(mc_digits, Xm_test, ym_test)\n",
    "assert 0.0 <= acc_mc <= 1.0\n",
    "print(\"Multiclass acc (quick):\", acc_mc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25b966b",
   "metadata": {},
   "source": [
    "### Part 4.2: Track mistakes per epoch (binary). [3 points]\n",
    "If the model predicts a label incorrectly, we can track these mistakes in order to evaluate the model so that we know where it can be improved.\n",
    "We are doing this to visualize training dynamics and compare digit pairs.\n",
    "\n",
    "What to implement (TODO):\n",
    "- For each epoch, count how many times predict(x) != y.\n",
    "- Return a list of mistake counts, one per epoch.\n",
    "\n",
    "Reference: [Learning curve](https://en.wikipedia.org/wiki/Learning_curve_%28machine_learning%29)\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/2/24/Learning_Curves_%28Naive_Bayes%29.png\" alt=\"Learning curve example\" width=\"420\" style=\"display:block;margin:0 auto;\" />\n",
    "\n",
    "*Image: [\"Learning Curves (Naive Bayes).png\"](https://commons.wikimedia.org/wiki/File:Learning_Curves_(Naive_Bayes).png) by Justin Ormont, [BSD 3-Clause](https://opensource.org/licenses/BSD-3-Clause).* \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf43b517",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_binary_with_mistakes(\n",
    "    model: BinaryPerceptron,\n",
    "    X: Sequence[Sequence[float]],\n",
    "    y: Sequence[int],\n",
    "    epochs: int = 10,\n",
    "    seed: int = 0,\n",
    "    shuffle: bool = True,\n",
    ") -> List[int]:\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "        mistakes_per_epoch: list of mistake counts per epoch.\n",
    "    \"\"\"\n",
    "    # TODO: Train the model for `epochs`, counting misclassifications each epoch.\n",
    "    #   make an index list [0..len(X)-1]\n",
    "    #   if shuffle: set random.seed(seed) for reproducibility\n",
    "    #   for each epoch:\n",
    "    #       if shuffle: random.shuffle(indices)\n",
    "    #       for each index\n",
    "    #           if predict(X[idx]) is not equal to y[idx], increment the number of mistakes \n",
    "    #           update(X[idx], y[idx])\n",
    "    #       append mistakes to list\n",
    "    \n",
    "    raise NotImplementedError\n",
    "\n",
    "\n",
    "# Part 4.3: Plot a learning curve. [not graded]\n",
    "def plot_learning_curve(mistakes_per_epoch: Sequence[int], title: str) -> None:\n",
    "    \"\"\"\n",
    "    Plots mistakes per epoch using matplotlib.\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    epochs = list(range(1, len(mistakes_per_epoch) + 1))\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.plot(epochs, mistakes_per_epoch, marker=\"o\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Mistakes\")\n",
    "    plt.title(title)\n",
    "    plt.grid(True, linestyle=\"--\", alpha=0.4)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5699035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Part 4.2 (mistakes per epoch)\n",
    "model_bin = BinaryPerceptron.create(num_features=len(X_bin[0]))\n",
    "mistakes = train_binary_with_mistakes(model_bin, X_bin, y_bin, epochs=5)\n",
    "assert len(mistakes) == 5 and all(m >= 0 for m in mistakes)\n",
    "print(\"Mistakes per epoch:\", mistakes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7673c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Part 4.3 (plot learning curve)\n",
    "# Run Part 4.2 first to create `mistakes`.\n",
    "plot_learning_curve(mistakes, title=\"Perceptron Mistakes per Epoch\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1545acdf",
   "metadata": {},
   "source": [
    "## Question 5: Main Experiment [5 points]\n",
    "High-level goal: wire everything together so you can run the assignment\n",
    "end-to-end from a single notebook.\n",
    "\n",
    "Parts (not individually graded; this section is an integration check):\n",
    "- 5.1 Load data.\n",
    "- 5.2 Normalize features.\n",
    "- 5.3 Create a binary dataset (e.g., 0 vs 1).\n",
    "- 5.4 Train and evaluate the binary perceptron.\n",
    "- 5.5 Train and evaluate the multiclass perceptron.\n",
    "- 5.6 Track mistakes and plot a learning curve for a digit pair.\n",
    "\n",
    "Suggested workflow:\n",
    "1. Start with default settings and verify you can run end-to-end.\n",
    "2. Try a few digit pairs and compare accuracies.\n",
    "3. Inspect learning curves to see whether training plateaus or keeps improving.\n",
    "\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/a/a4/Machine_learning_workflow_diagram.png\" alt=\"Machine learning workflow diagram\" width=\"420\" style=\"display:block;margin:0 auto;\" />\n",
    "\n",
    "*Image: [\"Machine learning workflow diagram.png\"](https://commons.wikimedia.org/wiki/File:Machine_learning_workflow_diagram.png) by Brylie Christopher Oxley, [CC0 1.0](https://creativecommons.org/publicdomain/zero/1.0/).* \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ecf78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main() -> None:\n",
    "    # Part 5.1: Load data.\n",
    "    X, y = load_digits_data()\n",
    "\n",
    "    # Part 5.2: Normalize features (optional but recommended).\n",
    "    X = normalize_features(X)\n",
    "\n",
    "    # Part 5.3: Create a binary dataset (e.g., 0 vs 1).\n",
    "    X_bin, y_bin = make_binary_dataset(X, y, positive_digit=0, negative_digit=1)\n",
    "    Xb_train, Xb_test, yb_train, yb_test = train_test_split(X_bin, y_bin)\n",
    "\n",
    "    # Part 5.4: Train + evaluate binary perceptron.\n",
    "    if not Xb_train:\n",
    "        print(\"Binary dataset is empty; skipping binary perceptron.\")\n",
    "    else:\n",
    "        bin_model = BinaryPerceptron.create(num_features=len(Xb_train[0]))\n",
    "        bin_model.fit(Xb_train, yb_train, epochs=10)\n",
    "        bin_acc = binary_accuracy(bin_model, Xb_test, yb_test)\n",
    "        print(\"Binary accuracy:\", bin_acc)\n",
    "\n",
    "    # Part 5.5: Train + evaluate multiclass perceptron.\n",
    "    Xm_train, Xm_test, ym_train, ym_test = train_test_split(X, y)\n",
    "    if not Xm_train:\n",
    "        print(\"Multiclass dataset is empty; skipping multiclass perceptron.\")\n",
    "    else:\n",
    "        multi_model = MulticlassPerceptron.create(\n",
    "            num_features=len(Xm_train[0]), num_classes=10\n",
    "        )\n",
    "        multi_model.fit(Xm_train, ym_train, epochs=10)\n",
    "        multi_acc = multiclass_accuracy(multi_model, Xm_test, ym_test)\n",
    "        print(\"Multiclass accuracy:\", multi_acc)\n",
    "\n",
    "    # Part 5.6: Track mistakes and plot a learning curve for a digit pair.\n",
    "    # Example pair: 3 vs 5 (often harder than 0 vs 1).\n",
    "    X_bin_35, y_bin_35 = make_binary_dataset(X, y, positive_digit=3, negative_digit=5)\n",
    "    if not X_bin_35:\n",
    "        print(\"Digit pair 3 vs 5 not present; skipping learning curve.\")\n",
    "        return\n",
    "    X35_train, X35_test, y35_train, y35_test = train_test_split(X_bin_35, y_bin_35)\n",
    "    if not X35_train:\n",
    "        print(\"Not enough data for learning curve; skipping.\")\n",
    "        return\n",
    "    model_35 = BinaryPerceptron.create(num_features=len(X35_train[0]))\n",
    "    mistakes = train_binary_with_mistakes(model_35, X35_train, y35_train, epochs=15)\n",
    "    # Uncomment to display the plot.\n",
    "    # plot_learning_curve(mistakes, title=\"Perceptron Mistakes per Epoch (3 vs 5)\")\n",
    "\n",
    "# Uncomment to run:\n",
    "# main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
