{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Captioning with MS-COCO Dataset\n",
    "\n",
    "**Problem Statement:** Automatically describing the content of an image using natural language bridges Computer Vision and NLP. In this project, we use the MS-COCO dataset to train an encoder-decoder architecture for image captioning.\n",
    "\n",
    "The following code is provided to help you get started. We recommend using GPUs train the models as image cpationing can be very intensive even using relatively simple models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install and Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install additional packages if needed\n",
    "# !pip install pycocotools nltk\n",
    "\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "from torchvision import models\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('punkt_tab', quiet=True)\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checks which device to use (Datahub GPU is either RTX 2070 or RTX 2080)\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Download the MS-COCO Dataset\n",
    "\n",
    "We download the 2017 training images and annotations. This is will create a data folder with MSCOCO inside. You only have to run this code once. **Note:** The full training set is ~18GB. So for this project, we will just use the validation set ~1GB. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = './data/coco'\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "# Download annotations (relatively small ~241MB)\n",
    "ANNOTATIONS_URL = 'http://images.cocodataset.org/annotations/annotations_trainval2017.zip'\n",
    "\n",
    "if not os.path.exists(os.path.join(DATA_DIR, 'annotations')):\n",
    "    print(\"Downloading COCO annotations...\")\n",
    "    !wget -q {ANNOTATIONS_URL} -O {DATA_DIR}/annotations.zip\n",
    "    !unzip -q {DATA_DIR}/annotations.zip -d {DATA_DIR}\n",
    "    !rm {DATA_DIR}/annotations.zip\n",
    "    print(\"Annotations downloaded!\")\n",
    "else:\n",
    "    print(\"Annotations already exist.\")\n",
    "\n",
    "# Download validation images (smaller set, ~1GB - good for development)\n",
    "VAL_IMAGES_URL = 'http://images.cocodataset.org/zips/val2017.zip'\n",
    "\n",
    "if not os.path.exists(os.path.join(DATA_DIR, 'val2017')):\n",
    "    print(\"Downloading COCO val2017 images...\")\n",
    "    !wget -q {VAL_IMAGES_URL} -O {DATA_DIR}/val2017.zip\n",
    "    !unzip -q {DATA_DIR}/val2017.zip -d {DATA_DIR}\n",
    "    !rm {DATA_DIR}/val2017.zip\n",
    "    print(\"Validation images downloaded!\")\n",
    "else:\n",
    "    print(\"Validation images already exist.\")\n",
    "\n",
    "print(f\"\\nDataset contents: {os.listdir(DATA_DIR)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Explore the Captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load captions annotation file\n",
    "# Using val set captions for development; switch to train for full training\n",
    "captions_file = os.path.join(DATA_DIR, 'annotations', 'captions_val2017.json')\n",
    "\n",
    "with open(captions_file, 'r') as f:\n",
    "    captions_data = json.load(f)\n",
    "\n",
    "print(f\"Keys: {captions_data.keys()}\")\n",
    "print(f\"Number of images: {len(captions_data['images'])}\")\n",
    "print(f\"Number of captions: {len(captions_data['annotations'])}\")\n",
    "print(f\"Average captions per image: {len(captions_data['annotations']) / len(captions_data['images']):.1f}\")\n",
    "\n",
    "# Build a mapping from image_id to captions and filenames\n",
    "img_id_to_filename = {img['id']: img['file_name'] for img in captions_data['images']}\n",
    "img_id_to_captions = {}\n",
    "for ann in captions_data['annotations']:\n",
    "    img_id = ann['image_id']\n",
    "    if img_id not in img_id_to_captions:\n",
    "        img_id_to_captions[img_id] = []\n",
    "    img_id_to_captions[img_id].append(ann['caption'])\n",
    "\n",
    "# Show a few examples\n",
    "sample_ids = random.sample(list(img_id_to_captions.keys()), 3)\n",
    "for img_id in sample_ids:\n",
    "    print(f\"\\nImage: {img_id_to_filename[img_id]}\")\n",
    "    for i, cap in enumerate(img_id_to_captions[img_id]):\n",
    "        print(f\"  Caption {i+1}: {cap}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sample images with their captions\n",
    "img_dir = os.path.join(DATA_DIR, 'val2017')\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "for ax, img_id in zip(axes, sample_ids):\n",
    "    img_path = os.path.join(img_dir, img_id_to_filename[img_id])\n",
    "    img = Image.open(img_path)\n",
    "    ax.imshow(img)\n",
    "    ax.set_title('\\n'.join(img_id_to_captions[img_id][:2]), fontsize=9, wrap=True)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle('Sample COCO Images with Captions', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Build Vocabulary\n",
    "\n",
    "Tokenize all captions and build a word-to-index vocabulary, including special tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    \"\"\"\n",
    "    A vocabulary that maps words to indices and vice versa.\n",
    "    \n",
    "    Special tokens:\n",
    "        <pad>   : padding token (index 0)\n",
    "        <start> : start-of-sequence token (index 1)\n",
    "        <end>   : end-of-sequence token (index 2)\n",
    "        <unk>   : unknown word token (index 3)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, freq_threshold=5):\n",
    "        self.freq_threshold = freq_threshold\n",
    "        self.word2idx = {'<pad>': 0, '<start>': 1, '<end>': 2, '<unk>': 3}\n",
    "        self.idx2word = {0: '<pad>', 1: '<start>', 2: '<end>', 3: '<unk>'}\n",
    "        self.word_count = Counter()\n",
    "        self.idx = 4  # Start after special tokens\n",
    "    \n",
    "    def build_vocabulary(self, captions_list):\n",
    "        \"\"\"Build vocabulary from a list of caption strings.\"\"\"\n",
    "        # Count word frequencies\n",
    "        for caption in captions_list:\n",
    "            tokens = word_tokenize(caption.lower())\n",
    "            self.word_count.update(tokens)\n",
    "        \n",
    "        # Add words that meet the frequency threshold\n",
    "        for word, count in self.word_count.items():\n",
    "            if count >= self.freq_threshold:\n",
    "                self.word2idx[word] = self.idx\n",
    "                self.idx2word[self.idx] = word\n",
    "                self.idx += 1\n",
    "        \n",
    "        print(f\"Vocabulary built: {len(self.word2idx)} words \"\n",
    "              f\"(threshold={self.freq_threshold})\")\n",
    "    \n",
    "    def numericalize(self, caption):\n",
    "        \"\"\"\n",
    "        Convert a caption string to a list of token indices.\n",
    "        Wraps with <start> and <end> tokens.\n",
    "        \"\"\"\n",
    "        tokens = word_tokenize(caption.lower())\n",
    "        indices = [self.word2idx['<start>']]\n",
    "        indices += [self.word2idx.get(tok, self.word2idx['<unk>']) for tok in tokens]\n",
    "        indices.append(self.word2idx['<end>'])\n",
    "        return indices\n",
    "    \n",
    "    def decode(self, indices):\n",
    "        \"\"\"Convert a list of indices back to a string.\"\"\"\n",
    "        words = []\n",
    "        for idx in indices:\n",
    "            word = self.idx2word.get(idx, '<unk>')\n",
    "            if word == '<end>':\n",
    "                break\n",
    "            if word not in ('<start>', '<pad>'):\n",
    "                words.append(word)\n",
    "        return ' '.join(words)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.word2idx)\n",
    "\n",
    "# Build vocabulary from all captions\n",
    "all_captions = [ann['caption'] for ann in captions_data['annotations']]\n",
    "\n",
    "vocab = Vocabulary(freq_threshold=5)\n",
    "vocab.build_vocabulary(all_captions)\n",
    "\n",
    "# Test\n",
    "sample_caption = all_captions[0]\n",
    "print(f\"\\nOriginal: {sample_caption}\")\n",
    "indices = vocab.numericalize(sample_caption)\n",
    "print(f\"Numericalized: {indices}\")\n",
    "print(f\"Decoded: {vocab.decode(indices)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create Custom Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CocoImageCaptionDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for MS-COCO image captioning.\n",
    "    \n",
    "    Each sample returns:\n",
    "        - image: Transformed image tensor [3, 224, 224]\n",
    "        - caption: Numericalized caption as a tensor\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, img_dir, annotations_file, vocab, transform=None, max_len=50):\n",
    "        self.img_dir = img_dir\n",
    "        self.vocab = vocab\n",
    "        self.transform = transform\n",
    "        self.max_len = max_len\n",
    "        \n",
    "        with open(annotations_file, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        self.img_id_to_filename = {img['id']: img['file_name'] for img in data['images']}\n",
    "        \n",
    "        # Each annotation is one (image, caption) pair\n",
    "        self.annotations = data['annotations']\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        ann = self.annotations[idx]\n",
    "        img_id = ann['image_id']\n",
    "        caption = ann['caption']\n",
    "        \n",
    "        # Load image\n",
    "        img_path = os.path.join(self.img_dir, self.img_id_to_filename[img_id])\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        # Numericalize caption\n",
    "        caption_indices = self.vocab.numericalize(caption)\n",
    "        \n",
    "        # Truncate or pad to max_len\n",
    "        if len(caption_indices) > self.max_len:\n",
    "            caption_indices = caption_indices[:self.max_len]\n",
    "        \n",
    "        caption_tensor = torch.tensor(caption_indices, dtype=torch.long)\n",
    "        \n",
    "        return img, caption_tensor\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "\n",
    "def caption_collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Custom collate function to pad captions to the same length within a batch.\n",
    "    \"\"\"\n",
    "    images, captions = zip(*batch)\n",
    "    images = torch.stack(images, dim=0)\n",
    "    \n",
    "    # Pad captions to the length of the longest caption in the batch\n",
    "    lengths = [len(cap) for cap in captions]\n",
    "    max_len = max(lengths)\n",
    "    padded_captions = torch.zeros(len(captions), max_len, dtype=torch.long)\n",
    "    for i, cap in enumerate(captions):\n",
    "        padded_captions[i, :len(cap)] = cap\n",
    "    \n",
    "    lengths = torch.tensor(lengths, dtype=torch.long)\n",
    "    return images, padded_captions, lengths\n",
    "\n",
    "\n",
    "# Define image transforms\n",
    "transform = T.Compose([\n",
    "    T.Resize((256, 256)),\n",
    "    T.CenterCrop(224),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Create dataset\n",
    "dataset = CocoImageCaptionDataset(\n",
    "    img_dir=os.path.join(DATA_DIR, 'val2017'),\n",
    "    annotations_file=os.path.join(DATA_DIR, 'annotations', 'captions_val2017.json'),\n",
    "    vocab=vocab,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "print(f\"Dataset size: {len(dataset)}\")\n",
    "\n",
    "# Split into train/val (using val2017 images for development)\n",
    "torch.manual_seed(42)\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "print(f\"Train size: {len(train_dataset)}, Val size: {len(val_dataset)}\")\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    collate_fn=caption_collate_fn\n",
    ")\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    collate_fn=caption_collate_fn\n",
    ")\n",
    "\n",
    "# Test a batch\n",
    "images, captions, lengths = next(iter(train_loader))\n",
    "print(f\"\\nBatch check:\")\n",
    "print(f\"  Images shape: {images.shape}\")\n",
    "print(f\"  Captions shape: {captions.shape}\")\n",
    "print(f\"  Lengths: {lengths[:5]}\")\n",
    "print(f\"  Sample decoded: {vocab.decode(captions[0].tolist())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Encoder-Decoder Architecture\n",
    "\n",
    "The **encoder** is a pre-trained CNN (ResNet-50) that extracts image features. We have provided the encoder for you (resnet50).The **decoder** is an RNN (LSTM) that generates captions word by word. Your task is to implement and experiment with different decoders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    CNN Encoder using a pre-trained ResNet-50.\n",
    "    Extracts a fixed-size feature vector from an input image.\n",
    "    \n",
    "    The final fully connected layer of ResNet is replaced with\n",
    "    a linear projection to the desired embedding dimension.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, embed_size):\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        resnet = models.resnet50(pretrained=True)\n",
    "        \n",
    "        # Remove the final FC layer\n",
    "        modules = list(resnet.children())[:-1]\n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "        \n",
    "        # Project to embedding space\n",
    "        self.linear = nn.Linear(resnet.fc.in_features, embed_size)\n",
    "        self.bn = nn.BatchNorm1d(embed_size)\n",
    "    \n",
    "    def forward(self, images):\n",
    "        # Freeze CNN weights (or fine-tune later)\n",
    "        with torch.no_grad():\n",
    "            features = self.resnet(images)\n",
    "        \n",
    "        features = features.view(features.size(0), -1)\n",
    "        features = self.bn(self.linear(features))\n",
    "        return features\n",
    "\n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "    \"\"\"\n",
    "    RNN Decoder using LSTM.\n",
    "    Takes image features and generates captions word by word.\n",
    "    \n",
    "    TODO: This is a basic implementation. You should experiment with:\n",
    "    - Attention mechanisms\n",
    "    - Transformer-based decoders\n",
    "    - Beam search for inference\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers=1):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        pass\n",
    "    \n",
    "    def forward(self, features, captions):\n",
    "        \"\"\"\n",
    "        Forward pass for training.\n",
    "        \n",
    "        Args:\n",
    "            features: Image features from encoder [batch, embed_size]\n",
    "            captions: Target captions [batch, max_len]\n",
    "        \n",
    "        Returns:\n",
    "            outputs: Predicted word scores [batch, max_len, vocab_size]\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def generate(self, features, max_len=20, vocab=None):\n",
    "        \"\"\"\n",
    "        Generate a caption for a single image (greedy decoding).\n",
    "        \n",
    "        Args:\n",
    "            features: Image features [1, embed_size]\n",
    "            max_len: Maximum caption length\n",
    "            vocab: Vocabulary object for decoding\n",
    "        \n",
    "        Returns:\n",
    "            generated_ids: List of generated token indices\n",
    "        \"\"\"\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
