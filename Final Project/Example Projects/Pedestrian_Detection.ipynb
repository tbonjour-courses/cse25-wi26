{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pedestrian Detection in Penn-Fudan\n",
    "\n",
    "**Problem Statement:** Object detection is a fundamental challenge in Computer Vision that involves identifying the presence, location, and type of objects within an image. In this project, we focus on pedestrian detection using the Penn-Fudan Database for Pedestrian Detection.\n",
    "\n",
    "The following code is provided to help you get started. We recommend using GPUs train the models as object detection can be very intensive even using relatively simple vision models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install and Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torchvision matplotlib pillow\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torchvision\n",
    "from torchvision import transforms as T\n",
    "\n",
    "# Feel free to add additional packages here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checks which device to use (Datahub GPU is either RTX 2070 or RTX 2080)\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Torchvision version: {torchvision.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Download the Penn-Fudan Dataset\n",
    "\n",
    "The following code reads in the Penn-Fudan dataset from the website and saves it as a data folder in the root directory. If you are having trouble download the dataset using the URL, try manually downloading the zip file directly from their website.\n",
    "\n",
    " **You only need to run this code once.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and extract the Penn-Fudan dataset\n",
    "DATA_DIR = './data'\n",
    "DATASET_URL = 'https://www.cis.upenn.edu/~jshi/ped_html/PennFudanPed.zip'\n",
    "\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "if not os.path.exists(os.path.join(DATA_DIR, 'PennFudanPed')):\n",
    "    print(\"Downloading Penn-Fudan dataset...\")\n",
    "    !wget -q {DATASET_URL} -O {DATA_DIR}/PennFudanPed.zip\n",
    "    !unzip -q {DATA_DIR}/PennFudanPed.zip -d {DATA_DIR}\n",
    "    !rm {DATA_DIR}/PennFudanPed.zip\n",
    "    print(\"Download complete!\")\n",
    "else:\n",
    "    print(\"Dataset already exists.\")\n",
    "\n",
    "# List the dataset structure\n",
    "dataset_root = os.path.join(DATA_DIR, 'PennFudanPed')\n",
    "print(f\"\\nDataset contents: {os.listdir(dataset_root)}\")\n",
    "print(f\"Number of images: {len(os.listdir(os.path.join(dataset_root, 'PNGImages')))}\")\n",
    "print(f\"Number of masks: {len(os.listdir(os.path.join(dataset_root, 'PedMasks')))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualize the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at a sample image and its corresponding mask\n",
    "img_dir = os.path.join(dataset_root, 'PNGImages')\n",
    "mask_dir = os.path.join(dataset_root, 'PedMasks')\n",
    "\n",
    "# Get sorted file lists\n",
    "img_files = sorted(os.listdir(img_dir))\n",
    "mask_files = sorted(os.listdir(mask_dir))\n",
    "\n",
    "# Display a few samples\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "for i in range(4):\n",
    "    # Load image\n",
    "    img = Image.open(os.path.join(img_dir, img_files[i]))\n",
    "    mask = Image.open(os.path.join(mask_dir, mask_files[i]))\n",
    "    \n",
    "    axes[0, i].imshow(img)\n",
    "    axes[0, i].set_title(img_files[i])\n",
    "    axes[0, i].axis('off')\n",
    "    \n",
    "    axes[1, i].imshow(mask)\n",
    "    axes[1, i].set_title(f'Mask: {mask_files[i]}')\n",
    "    axes[1, i].axis('off')\n",
    "\n",
    "plt.suptitle('Sample Images (top) and Segmentation Masks (bottom)', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Parse Annotations: Extract Bounding Boxes from Masks\n",
    "\n",
    "The Penn-Fudan dataset provides segmentation masks where each pedestrian is labeled with a unique pixel value. We extract bounding boxes from these masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bboxes_from_mask(mask_path):\n",
    "    \"\"\"\n",
    "    Extract bounding boxes from a segmentation mask.\n",
    "    Each unique non-zero value in the mask corresponds to a different pedestrian.\n",
    "    \n",
    "    Args:\n",
    "        mask_path (str): Path to the mask image.\n",
    "    \n",
    "    Returns:\n",
    "        boxes (list of [x_min, y_min, x_max, y_max]): Bounding boxes for each pedestrian.\n",
    "        num_objects (int): Number of pedestrians detected.\n",
    "    \"\"\"\n",
    "    mask = Image.open(mask_path)\n",
    "    mask_np = np.array(mask)\n",
    "    \n",
    "    # Get unique object IDs (0 is background)\n",
    "    obj_ids = np.unique(mask_np)\n",
    "    obj_ids = obj_ids[obj_ids != 0]  # Remove background\n",
    "    \n",
    "    boxes = []\n",
    "    for obj_id in obj_ids:\n",
    "        pos = np.where(mask_np == obj_id)\n",
    "        x_min = int(np.min(pos[1]))\n",
    "        x_max = int(np.max(pos[1]))\n",
    "        y_min = int(np.min(pos[0]))\n",
    "        y_max = int(np.max(pos[0]))\n",
    "        boxes.append([x_min, y_min, x_max, y_max])\n",
    "    \n",
    "    return boxes, len(obj_ids)\n",
    "\n",
    "# Test on a sample\n",
    "sample_mask_path = os.path.join(mask_dir, mask_files[0])\n",
    "boxes, num_peds = get_bboxes_from_mask(sample_mask_path)\n",
    "print(f\"Sample: {mask_files[0]} -> {num_peds} pedestrian(s)\")\n",
    "print(f\"Bounding boxes: {boxes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize bounding boxes on an image\n",
    "def visualize_bboxes(img_path, mask_path):\n",
    "    \"\"\"Display image with bounding boxes drawn from the mask.\"\"\"\n",
    "    img = Image.open(img_path)\n",
    "    boxes, num_peds = get_bboxes_from_mask(mask_path)\n",
    "    \n",
    "    fig, ax = plt.subplots(1, figsize=(8, 6))\n",
    "    ax.imshow(img)\n",
    "    \n",
    "    colors = plt.cm.Set1(np.linspace(0, 1, max(num_peds, 1)))\n",
    "    for i, box in enumerate(boxes):\n",
    "        x_min, y_min, x_max, y_max = box\n",
    "        rect = patches.Rectangle(\n",
    "            (x_min, y_min), x_max - x_min, y_max - y_min,\n",
    "            linewidth=2, edgecolor=colors[i], facecolor='none'\n",
    "        )\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(x_min, y_min - 5, f'Person {i+1}', color=colors[i],\n",
    "                fontsize=10, fontweight='bold')\n",
    "    \n",
    "    ax.set_title(f'{os.path.basename(img_path)} - {num_peds} pedestrian(s)')\n",
    "    ax.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Visualize first 3 samples\n",
    "for i in range(3):\n",
    "    visualize_bboxes(\n",
    "        os.path.join(img_dir, img_files[i]),\n",
    "        os.path.join(mask_dir, mask_files[i])\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create a Custom PyTorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PennFudanDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Custom dataset for the Penn-Fudan Pedestrian Detection dataset.\n",
    "    \n",
    "    Each sample returns:\n",
    "        - image: a PIL Image converted to a tensor\n",
    "        - target: a dict containing:\n",
    "            - boxes (FloatTensor[N, 4]): bounding boxes in [x_min, y_min, x_max, y_max] format\n",
    "            - labels (Int64Tensor[N]): class labels (1 = pedestrian)\n",
    "            - masks (UInt8Tensor[N, H, W]): segmentation masks for each instance\n",
    "            - image_id (Int64Tensor[1]): unique image identifier\n",
    "            - area (FloatTensor[N]): area of each bounding box\n",
    "            - iscrowd (Int64Tensor[N]): whether each instance is a crowd region\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, root, transforms=None):\n",
    "        self.root = root\n",
    "        self.transforms = transforms\n",
    "        \n",
    "        # Load all image and mask file paths, sorted to ensure alignment\n",
    "        self.imgs = sorted(os.listdir(os.path.join(root, 'PNGImages')))\n",
    "        self.masks = sorted(os.listdir(os.path.join(root, 'PedMasks')))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Load image and mask\n",
    "        img_path = os.path.join(self.root, 'PNGImages', self.imgs[idx])\n",
    "        mask_path = os.path.join(self.root, 'PedMasks', self.masks[idx])\n",
    "        \n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        mask = Image.open(mask_path)\n",
    "        mask_np = np.array(mask)\n",
    "        \n",
    "        # Get unique object instances (0 is background)\n",
    "        obj_ids = np.unique(mask_np)\n",
    "        obj_ids = obj_ids[obj_ids != 0]\n",
    "        \n",
    "        # Create binary masks for each instance\n",
    "        masks = (mask_np == obj_ids[:, None, None]).astype(np.uint8)\n",
    "        \n",
    "        # Compute bounding boxes from masks\n",
    "        num_objs = len(obj_ids)\n",
    "        boxes = []\n",
    "        for i in range(num_objs):\n",
    "            pos = np.where(masks[i])\n",
    "            x_min = int(np.min(pos[1]))\n",
    "            x_max = int(np.max(pos[1]))\n",
    "            y_min = int(np.min(pos[0]))\n",
    "            y_max = int(np.max(pos[0]))\n",
    "            boxes.append([x_min, y_min, x_max, y_max])\n",
    "        \n",
    "        # Convert to tensors\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.ones((num_objs,), dtype=torch.int64)  # All are pedestrians (class 1)\n",
    "        masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
    "        image_id = torch.tensor([idx])\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
    "        \n",
    "        target = {\n",
    "            'boxes': boxes,\n",
    "            'labels': labels,\n",
    "            'masks': masks,\n",
    "            'image_id': image_id,\n",
    "            'area': area,\n",
    "            'iscrowd': iscrowd\n",
    "        }\n",
    "        \n",
    "        if self.transforms is not None:\n",
    "            img = self.transforms(img)\n",
    "        else:\n",
    "            img = T.ToTensor()(img)\n",
    "        \n",
    "        return img, target\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "\n",
    "# Test the dataset\n",
    "dataset = PennFudanDataset(dataset_root)\n",
    "print(f\"Dataset size: {len(dataset)}\")\n",
    "\n",
    "img, target = dataset[0]\n",
    "print(f\"Image shape: {img.shape}\")\n",
    "print(f\"Number of pedestrians: {len(target['boxes'])}\")\n",
    "print(f\"Bounding boxes: {target['boxes']}\")\n",
    "print(f\"Labels: {target['labels']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Create Train/Validation Split and Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_transform(train):\n",
    "    \"\"\"Define transforms for training and validation.\"\"\"\n",
    "    transforms = [T.ToTensor()]\n",
    "    if train:\n",
    "        # You can add data augmentation here for training\n",
    "        # e.g., T.RandomHorizontalFlip(0.5)\n",
    "        pass\n",
    "    return T.Compose(transforms)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Custom collate function for variable-size targets.\"\"\"\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "# Create full dataset\n",
    "dataset_full = PennFudanDataset(dataset_root, transforms=get_transform(train=True))\n",
    "\n",
    "# Split into train and validation sets (80/20 split)\n",
    "torch.manual_seed(42)\n",
    "total_size = len(dataset_full)\n",
    "train_size = int(0.8 * total_size)\n",
    "val_size = total_size - train_size\n",
    "\n",
    "indices = torch.randperm(total_size).tolist()\n",
    "train_dataset = torch.utils.data.Subset(dataset_full, indices[:train_size])\n",
    "val_dataset = torch.utils.data.Subset(dataset_full, indices[train_size:])\n",
    "\n",
    "print(f\"Total samples: {total_size}\")\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=2,\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=2,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "# Verify a batch\n",
    "images, targets = next(iter(train_loader))\n",
    "print(f\"\\nBatch check:\")\n",
    "print(f\"  Number of images in batch: {len(images)}\")\n",
    "print(f\"  First image shape: {images[0].shape}\")\n",
    "print(f\"  First target keys: {targets[0].keys()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
