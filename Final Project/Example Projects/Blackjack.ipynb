{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning to Play Blackjack with Reinforcement Learning\n",
    "\n",
    "**Problem Statement:** Reinforcement Learning (RL) trains an agent to make decisions by interacting with an environment to maximize rewards. In this project, we apply RL to Blackjack using the OpenAI Gymnasium environment.\n",
    "\n",
    "The following code is provided to help you get started. For neural network approaches, we recommend using GPUs train the models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install and Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install gymnasium if needed\n",
    "# !pip install gymnasium matplotlib numpy seaborn\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(f\"Gymnasium version: {gym.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Set Up and Explore the Blackjack Environment\n",
    "\n",
    "The Blackjack-v1 environment simulates the card game with the following:\n",
    "- **State**: A tuple of (player_sum, dealer_showing_card, usable_ace)\n",
    "  - `player_sum` (int): Current sum of player's hand (4–21)\n",
    "  - `dealer_showing_card` (int): Dealer's face-up card (1–10, where 1 = Ace)\n",
    "  - `usable_ace` (bool): Whether the player has a usable ace\n",
    "- **Actions**: 0 = Stand (stop), 1 = Hit (draw another card)\n",
    "- **Rewards**: +1 (win), 0 (draw), -1 (lose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the environment\n",
    "env = gym.make('Blackjack-v1', sab=True)  # sab=True uses Sutton & Barto rules\n",
    "\n",
    "print(f\"Observation space: {env.observation_space}\")\n",
    "print(f\"Action space: {env.action_space}\")\n",
    "print(f\"Actions: 0=Stand, 1=Hit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Play a few sample episodes manually to understand the environment\n",
    "print(\"=== Sample Episodes ===\")\n",
    "\n",
    "for episode in range(5):\n",
    "    state, info = env.reset()\n",
    "    print(f\"\\nEpisode {episode + 1}:\")\n",
    "    print(f\"  Initial state: player_sum={state[0]}, dealer_card={state[1]}, usable_ace={state[2]}\")\n",
    "    \n",
    "    done = False\n",
    "    step = 0\n",
    "    while not done:\n",
    "        # Random action for demonstration\n",
    "        action = env.action_space.sample()\n",
    "        action_name = 'Hit' if action == 1 else 'Stand'\n",
    "        \n",
    "        next_state, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        step += 1\n",
    "        \n",
    "        print(f\"  Step {step}: Action={action_name}, \"\n",
    "              f\"New state={next_state}, Reward={reward}, Done={done}\")\n",
    "        \n",
    "        state = next_state\n",
    "    \n",
    "    result = 'Win' if reward > 0 else ('Draw' if reward == 0 else 'Lose')\n",
    "    print(f\"  Result: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Random Policy Baseline\n",
    "\n",
    "We've provided two simple baselines:\n",
    "\n",
    "1. Random policy agent: This agent will choose to hit or stand randomly.\n",
    "\n",
    "2. Stand on 17+: This agent will hit until it has reached 17+ in which it will stand.\n",
    "\n",
    "**Note:** Your job is to implement better RL algorithms to beat these baselines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy(env, policy_fn, n_episodes=100000):\n",
    "    \"\"\"\n",
    "    Evaluate a policy over many episodes.\n",
    "    \n",
    "    Args:\n",
    "        env: Gymnasium environment\n",
    "        policy_fn: Function that takes a state and returns an action\n",
    "        n_episodes: Number of episodes to simulate\n",
    "    \n",
    "    Returns:\n",
    "        win_rate, draw_rate, lose_rate\n",
    "    \"\"\"\n",
    "    wins, draws, losses = 0, 0, 0\n",
    "    \n",
    "    for _ in range(n_episodes):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            action = policy_fn(state)\n",
    "            state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "        \n",
    "        if reward > 0:\n",
    "            wins += 1\n",
    "        elif reward == 0:\n",
    "            draws += 1\n",
    "        else:\n",
    "            losses += 1\n",
    "    \n",
    "    return wins / n_episodes, draws / n_episodes, losses / n_episodes\n",
    "\n",
    "\n",
    "# Random policy\n",
    "random_policy = lambda state: env.action_space.sample()\n",
    "win_rate, draw_rate, lose_rate = evaluate_policy(env, random_policy)\n",
    "print(f\"Random Policy: Win={win_rate:.3%}, Draw={draw_rate:.3%}, Lose={lose_rate:.3%}\")\n",
    "\n",
    "# Simple threshold policy (stand on 17+)\n",
    "threshold_policy = lambda state: 1 if state[0] < 17 else 0\n",
    "win_rate, draw_rate, lose_rate = evaluate_policy(env, threshold_policy)\n",
    "print(f\"Threshold (17) Policy: Win={win_rate:.3%}, Draw={draw_rate:.3%}, Lose={lose_rate:.3%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Your code goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
